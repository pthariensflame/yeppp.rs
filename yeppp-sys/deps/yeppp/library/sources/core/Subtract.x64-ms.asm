;                       Yeppp! library implementation
;                   This file is auto-generated by Peach-Py,
;        Portable Efficient Assembly Code-generator in Higher-level Python,
;                  part of the Yeppp! library infrastructure
; This file is part of Yeppp! library and licensed under the New BSD license.
; See LICENSE.txt for the full text of the license.

section .text$e code align=16
global _yepCore_Subtract_V8sV8s_V8s_Nehalem
_yepCore_Subtract_V8sV8s_V8s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], al
	ADD r8, 1
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 128
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm15, [rcx]
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PSUBB xmm15, xmm14
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PSUBB xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PSUBB xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm15, [rcx]
	ADD rdx, 128
	PSUBB xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	PSUBB xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	PSUBB xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	PSUBB xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	PSUBB xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQA [byte r8 + 112], xmm13
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PSUBB xmm15, xmm14
	ADD r8, 128
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PSUBB xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PSUBB xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	PSUBB xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PSUBB xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PSUBB xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PSUBB xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PSUBB xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], al
	ADD r8, 1
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return

section .text$f code align=16
global _yepCore_Subtract_V8sV8s_V8s_SandyBridge
_yepCore_Subtract_V8sV8s_V8s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], al
	ADD r8, 1
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm15, [rcx]
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPSUBB xmm15, xmm15, xmm14
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPSUBB xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPSUBB xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm15, [rcx]
	ADD rdx, 128
	VPSUBB xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VPSUBB xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VPSUBB xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VPSUBB xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VPSUBB xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQA [byte r8 + 112], xmm13
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPSUBB xmm15, xmm15, xmm14
	ADD r8, 128
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPSUBB xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPSUBB xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPSUBB xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPSUBB xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPSUBB xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPSUBB xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPSUBB xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], al
	ADD r8, 1
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return

section .text$h code align=16
global _yepCore_Subtract_V8sV8s_V8s_Haswell
_yepCore_Subtract_V8sV8s_V8s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], al
	ADD r8, 1
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 256
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm15, [rcx]
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPSUBB ymm15, ymm15, ymm8
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPSUBB ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPSUBB ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 256
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm15, [rcx]
	ADD rdx, 256
	VPSUBB ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VPSUBB ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VPSUBB ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VPSUBB ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VPSUBB ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQA [dword r8 + 224], ymm0
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPSUBB ymm15, ymm15, ymm8
	ADD r8, 256
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPSUBB ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPSUBB ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 256
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 256
	VPSUBB ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPSUBB ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPSUBB ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPSUBB ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPSUBB ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 256
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], al
	ADD r8, 1
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return

section .text$m code align=16
global _yepCore_Subtract_V8sV8s_V16s_K10
_yepCore_Subtract_V8sV8s_V16s_K10:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm12
	MOVAPS [byte rsp + 16], xmm13
	MOVAPS [byte rsp + 32], xmm14
	MOVAPS [byte rsp + 48], xmm15
	MOVAPS [byte rsp + 64], xmm6
	MOVAPS [byte rsp + 80], xmm8
	MOVAPS [byte rsp + 96], xmm9
	MOVAPS [byte rsp + 112], xmm10
	MOVAPS [dword rsp + 128], xmm11
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm12, [rcx]
	MOVQ xmm13, [rdx]
	PXOR xmm14, xmm14
	PXOR xmm15, xmm15
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PXOR xmm8, xmm8
	PXOR xmm2, xmm2
	PCMPGTB xmm14, xmm12
	PCMPGTB xmm15, xmm13
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PXOR xmm1, xmm1
	PXOR xmm9, xmm9
	PCMPGTB xmm8, xmm4
	PCMPGTB xmm2, xmm6
	PUNPCKLBW xmm12, xmm14
	PUNPCKLBW xmm13, xmm15
	PSUBW xmm12, xmm13
	MOVQ xmm0, [byte rcx + 24]
	MOVQ xmm10, [byte rdx + 24]
	PXOR xmm11, xmm11
	PXOR xmm7, xmm7
	PCMPGTB xmm1, xmm5
	PCMPGTB xmm9, xmm3
	PUNPCKLBW xmm4, xmm8
	PUNPCKLBW xmm6, xmm2
	PSUBW xmm4, xmm6
	MOVDQA [r8], xmm12
	ADD rcx, 32
	ADD rdx, 32
	PCMPGTB xmm11, xmm0
	PCMPGTB xmm7, xmm10
	PUNPCKLBW xmm5, xmm1
	PUNPCKLBW xmm3, xmm9
	PSUBW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm12, [rcx]
	MOVQ xmm13, [rdx]
	PXOR xmm14, xmm14
	PXOR xmm15, xmm15
	PUNPCKLBW xmm0, xmm11
	PUNPCKLBW xmm10, xmm7
	PSUBW xmm0, xmm10
	MOVDQA [byte r8 + 32], xmm5
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PXOR xmm8, xmm8
	PXOR xmm2, xmm2
	PCMPGTB xmm14, xmm12
	PCMPGTB xmm15, xmm13
	MOVDQA [byte r8 + 48], xmm0
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PXOR xmm1, xmm1
	PXOR xmm9, xmm9
	PCMPGTB xmm8, xmm4
	PCMPGTB xmm2, xmm6
	PUNPCKLBW xmm12, xmm14
	PUNPCKLBW xmm13, xmm15
	PSUBW xmm12, xmm13
	ADD r8, 64
	MOVQ xmm0, [byte rcx + 24]
	MOVQ xmm10, [byte rdx + 24]
	PXOR xmm11, xmm11
	PXOR xmm7, xmm7
	PCMPGTB xmm1, xmm5
	PCMPGTB xmm9, xmm3
	PUNPCKLBW xmm4, xmm8
	PUNPCKLBW xmm6, xmm2
	PSUBW xmm4, xmm6
	MOVDQA [r8], xmm12
	ADD rcx, 32
	ADD rdx, 32
	PCMPGTB xmm11, xmm0
	PCMPGTB xmm7, xmm10
	PUNPCKLBW xmm5, xmm1
	PUNPCKLBW xmm3, xmm9
	PSUBW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLBW xmm0, xmm11
	PUNPCKLBW xmm10, xmm7
	PSUBW xmm0, xmm10
	MOVDQA [byte r8 + 32], xmm5
	MOVDQA [byte r8 + 48], xmm0
	ADD r8, 64
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm12, [rsp]
	MOVAPS xmm13, [byte rsp + 16]
	MOVAPS xmm14, [byte rsp + 32]
	MOVAPS xmm15, [byte rsp + 48]
	MOVAPS xmm6, [byte rsp + 64]
	MOVAPS xmm8, [byte rsp + 80]
	MOVAPS xmm9, [byte rsp + 96]
	MOVAPS xmm10, [byte rsp + 112]
	MOVAPS xmm11, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Subtract_V8sV8s_V16s_Nehalem
_yepCore_Subtract_V8sV8s_V16s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVSXBW xmm15, [rcx]
	PMOVSXBW xmm4, [byte rcx + 8]
	PMOVSXBW xmm14, [rdx]
	PMOVSXBW xmm5, [byte rcx + 16]
	PMOVSXBW xmm0, [byte rdx + 8]
	PMOVSXBW xmm1, [byte rcx + 24]
	PMOVSXBW xmm3, [byte rdx + 16]
	PMOVSXBW xmm6, [byte rcx + 32]
	PMOVSXBW xmm12, [byte rdx + 24]
	PMOVSXBW xmm2, [byte rcx + 40]
	PMOVSXBW xmm9, [byte rdx + 32]
	PMOVSXBW xmm11, [byte rcx + 48]
	PMOVSXBW xmm8, [byte rdx + 40]
	PSUBW xmm15, xmm14
	PMOVSXBW xmm13, [byte rcx + 56]
	PMOVSXBW xmm10, [byte rdx + 48]
	PSUBW xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVSXBW xmm7, [byte rdx + 56]
	PSUBW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVSXBW xmm15, [rcx]
	ADD rdx, 64
	PSUBW xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PMOVSXBW xmm4, [byte rcx + 8]
	PMOVSXBW xmm14, [rdx]
	PSUBW xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PMOVSXBW xmm5, [byte rcx + 16]
	PMOVSXBW xmm0, [byte rdx + 8]
	PSUBW xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PMOVSXBW xmm1, [byte rcx + 24]
	PMOVSXBW xmm3, [byte rdx + 16]
	PSUBW xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PMOVSXBW xmm6, [byte rcx + 32]
	PMOVSXBW xmm12, [byte rdx + 24]
	PSUBW xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	PMOVSXBW xmm2, [byte rcx + 40]
	PMOVSXBW xmm9, [byte rdx + 32]
	MOVDQA [byte r8 + 112], xmm13
	PMOVSXBW xmm11, [byte rcx + 48]
	PMOVSXBW xmm8, [byte rdx + 40]
	PSUBW xmm15, xmm14
	ADD r8, 128
	PMOVSXBW xmm13, [byte rcx + 56]
	PMOVSXBW xmm10, [byte rdx + 48]
	PSUBW xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVSXBW xmm7, [byte rdx + 56]
	PSUBW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	PSUBW xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PSUBW xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PSUBW xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PSUBW xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PSUBW xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Subtract_V8sV8s_V16s_SandyBridge
_yepCore_Subtract_V8sV8s_V16s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXBW xmm15, [rcx]
	VPMOVSXBW xmm4, [byte rcx + 8]
	VPMOVSXBW xmm14, [rdx]
	VPMOVSXBW xmm5, [byte rcx + 16]
	VPMOVSXBW xmm0, [byte rdx + 8]
	VPMOVSXBW xmm1, [byte rcx + 24]
	VPMOVSXBW xmm3, [byte rdx + 16]
	VPMOVSXBW xmm6, [byte rcx + 32]
	VPMOVSXBW xmm12, [byte rdx + 24]
	VPMOVSXBW xmm2, [byte rcx + 40]
	VPMOVSXBW xmm9, [byte rdx + 32]
	VPMOVSXBW xmm11, [byte rcx + 48]
	VPMOVSXBW xmm8, [byte rdx + 40]
	VPSUBW xmm15, xmm15, xmm14
	VPMOVSXBW xmm13, [byte rcx + 56]
	VPMOVSXBW xmm10, [byte rdx + 48]
	VPSUBW xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVSXBW xmm7, [byte rdx + 56]
	VPSUBW xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXBW xmm15, [rcx]
	ADD rdx, 64
	VPSUBW xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPMOVSXBW xmm4, [byte rcx + 8]
	VPMOVSXBW xmm14, [rdx]
	VPSUBW xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMOVSXBW xmm5, [byte rcx + 16]
	VPMOVSXBW xmm0, [byte rdx + 8]
	VPSUBW xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPMOVSXBW xmm1, [byte rcx + 24]
	VPMOVSXBW xmm3, [byte rdx + 16]
	VPSUBW xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPMOVSXBW xmm6, [byte rcx + 32]
	VPMOVSXBW xmm12, [byte rdx + 24]
	VPSUBW xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VPMOVSXBW xmm2, [byte rcx + 40]
	VPMOVSXBW xmm9, [byte rdx + 32]
	VMOVDQA [byte r8 + 112], xmm13
	VPMOVSXBW xmm11, [byte rcx + 48]
	VPMOVSXBW xmm8, [byte rdx + 40]
	VPSUBW xmm15, xmm15, xmm14
	ADD r8, 128
	VPMOVSXBW xmm13, [byte rcx + 56]
	VPMOVSXBW xmm10, [byte rdx + 48]
	VPSUBW xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVSXBW xmm7, [byte rdx + 56]
	VPSUBW xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	VPSUBW xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPSUBW xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPSUBW xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPSUBW xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPSUBW xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Subtract_V8sV8s_V16s_Haswell
_yepCore_Subtract_V8sV8s_V16s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXBW ymm15, [rcx]
	VPMOVSXBW ymm4, [byte rcx + 16]
	VPMOVSXBW ymm8, [rdx]
	VPMOVSXBW ymm5, [byte rcx + 32]
	VPMOVSXBW ymm10, [byte rdx + 16]
	VPMOVSXBW ymm14, [byte rcx + 48]
	VPMOVSXBW ymm12, [byte rdx + 32]
	VPMOVSXBW ymm6, [byte rcx + 64]
	VPMOVSXBW ymm13, [byte rdx + 48]
	VPMOVSXBW ymm1, [byte rcx + 80]
	VPMOVSXBW ymm7, [byte rdx + 64]
	VPMOVSXBW ymm2, [byte rcx + 96]
	VPMOVSXBW ymm3, [byte rdx + 80]
	VPSUBW ymm15, ymm15, ymm8
	VPMOVSXBW ymm0, [byte rcx + 112]
	VPMOVSXBW ymm11, [byte rdx + 96]
	VPSUBW ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVSXBW ymm9, [byte rdx + 112]
	VPSUBW ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXBW ymm15, [rcx]
	ADD rdx, 128
	VPSUBW ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPMOVSXBW ymm4, [byte rcx + 16]
	VPMOVSXBW ymm8, [rdx]
	VPSUBW ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPMOVSXBW ymm5, [byte rcx + 32]
	VPMOVSXBW ymm10, [byte rdx + 16]
	VPSUBW ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPMOVSXBW ymm14, [byte rcx + 48]
	VPMOVSXBW ymm12, [byte rdx + 32]
	VPSUBW ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPMOVSXBW ymm6, [byte rcx + 64]
	VPMOVSXBW ymm13, [byte rdx + 48]
	VPSUBW ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVSXBW ymm1, [byte rcx + 80]
	VPMOVSXBW ymm7, [byte rdx + 64]
	VMOVDQA [dword r8 + 224], ymm0
	VPMOVSXBW ymm2, [byte rcx + 96]
	VPMOVSXBW ymm3, [byte rdx + 80]
	VPSUBW ymm15, ymm15, ymm8
	ADD r8, 256
	VPMOVSXBW ymm0, [byte rcx + 112]
	VPMOVSXBW ymm11, [byte rdx + 96]
	VPSUBW ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVSXBW ymm9, [byte rdx + 112]
	VPSUBW ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPSUBW ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPSUBW ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPSUBW ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPSUBW ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPSUBW ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Subtract_V8uV8u_V16u_K10
_yepCore_Subtract_V8uV8u_V16u_K10:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm12
	MOVAPS [byte rsp + 16], xmm13
	MOVAPS [byte rsp + 32], xmm14
	MOVAPS [byte rsp + 48], xmm6
	MOVAPS [byte rsp + 64], xmm7
	MOVAPS [byte rsp + 80], xmm8
	MOVAPS [byte rsp + 96], xmm11
	MOVAPS [byte rsp + 112], xmm10
	MOVAPS [dword rsp + 128], xmm9
	PXOR xmm12, xmm12
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 56
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm13, [rcx]
	MOVQ xmm14, [rdx]
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	MOVQ xmm7, [byte rcx + 24]
	MOVQ xmm0, [byte rdx + 24]
	PUNPCKLBW xmm13, xmm12
	MOVQ xmm8, [byte rcx + 32]
	MOVQ xmm11, [byte rdx + 32]
	PUNPCKLBW xmm4, xmm12
	PUNPCKLBW xmm14, xmm12
	MOVQ xmm1, [byte rcx + 40]
	MOVQ xmm2, [byte rdx + 40]
	PUNPCKLBW xmm5, xmm12
	PUNPCKLBW xmm6, xmm12
	PSUBW xmm13, xmm14
	MOVQ xmm10, [byte rcx + 48]
	MOVQ xmm9, [byte rdx + 48]
	PUNPCKLBW xmm7, xmm12
	PUNPCKLBW xmm3, xmm12
	PSUBW xmm4, xmm6
	MOVDQA [r8], xmm13
	ADD rcx, 56
	ADD rdx, 56
	PUNPCKLBW xmm8, xmm12
	PUNPCKLBW xmm0, xmm12
	PSUBW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 56
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm13, [rcx]
	MOVQ xmm14, [rdx]
	PUNPCKLBW xmm1, xmm12
	PUNPCKLBW xmm11, xmm12
	PSUBW xmm7, xmm0
	MOVDQA [byte r8 + 32], xmm5
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PUNPCKLBW xmm10, xmm12
	PUNPCKLBW xmm2, xmm12
	PSUBW xmm8, xmm11
	MOVDQA [byte r8 + 48], xmm7
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PUNPCKLBW xmm9, xmm12
	PSUBW xmm1, xmm2
	MOVDQA [byte r8 + 64], xmm8
	MOVQ xmm7, [byte rcx + 24]
	MOVQ xmm0, [byte rdx + 24]
	PUNPCKLBW xmm13, xmm12
	PSUBW xmm10, xmm9
	MOVDQA [byte r8 + 80], xmm1
	MOVQ xmm8, [byte rcx + 32]
	MOVQ xmm11, [byte rdx + 32]
	PUNPCKLBW xmm4, xmm12
	PUNPCKLBW xmm14, xmm12
	MOVDQA [byte r8 + 96], xmm10
	MOVQ xmm1, [byte rcx + 40]
	MOVQ xmm2, [byte rdx + 40]
	PUNPCKLBW xmm5, xmm12
	PUNPCKLBW xmm6, xmm12
	PSUBW xmm13, xmm14
	ADD r8, 112
	MOVQ xmm10, [byte rcx + 48]
	MOVQ xmm9, [byte rdx + 48]
	PUNPCKLBW xmm7, xmm12
	PUNPCKLBW xmm3, xmm12
	PSUBW xmm4, xmm6
	MOVDQA [r8], xmm13
	ADD rcx, 56
	ADD rdx, 56
	PUNPCKLBW xmm8, xmm12
	PUNPCKLBW xmm0, xmm12
	PSUBW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 56
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLBW xmm1, xmm12
	PUNPCKLBW xmm11, xmm12
	PSUBW xmm7, xmm0
	MOVDQA [byte r8 + 32], xmm5
	PUNPCKLBW xmm10, xmm12
	PUNPCKLBW xmm2, xmm12
	PSUBW xmm8, xmm11
	MOVDQA [byte r8 + 48], xmm7
	PUNPCKLBW xmm9, xmm12
	PSUBW xmm1, xmm2
	MOVDQA [byte r8 + 64], xmm8
	PSUBW xmm10, xmm9
	MOVDQA [byte r8 + 80], xmm1
	MOVDQA [byte r8 + 96], xmm10
	ADD r8, 112
	.batch_process_finish:
	ADD r9, 56
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm12, [rsp]
	MOVAPS xmm13, [byte rsp + 16]
	MOVAPS xmm14, [byte rsp + 32]
	MOVAPS xmm6, [byte rsp + 48]
	MOVAPS xmm7, [byte rsp + 64]
	MOVAPS xmm8, [byte rsp + 80]
	MOVAPS xmm11, [byte rsp + 96]
	MOVAPS xmm10, [byte rsp + 112]
	MOVAPS xmm9, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Subtract_V8uV8u_V16u_Nehalem
_yepCore_Subtract_V8uV8u_V16u_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXBW xmm15, [rcx]
	PMOVZXBW xmm4, [byte rcx + 8]
	PMOVZXBW xmm14, [rdx]
	PMOVZXBW xmm5, [byte rcx + 16]
	PMOVZXBW xmm0, [byte rdx + 8]
	PMOVZXBW xmm1, [byte rcx + 24]
	PMOVZXBW xmm3, [byte rdx + 16]
	PMOVZXBW xmm6, [byte rcx + 32]
	PMOVZXBW xmm12, [byte rdx + 24]
	PMOVZXBW xmm2, [byte rcx + 40]
	PMOVZXBW xmm9, [byte rdx + 32]
	PMOVZXBW xmm11, [byte rcx + 48]
	PMOVZXBW xmm8, [byte rdx + 40]
	PSUBW xmm15, xmm14
	PMOVZXBW xmm13, [byte rcx + 56]
	PMOVZXBW xmm10, [byte rdx + 48]
	PSUBW xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVZXBW xmm7, [byte rdx + 56]
	PSUBW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXBW xmm15, [rcx]
	ADD rdx, 64
	PSUBW xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PMOVZXBW xmm4, [byte rcx + 8]
	PMOVZXBW xmm14, [rdx]
	PSUBW xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PMOVZXBW xmm5, [byte rcx + 16]
	PMOVZXBW xmm0, [byte rdx + 8]
	PSUBW xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PMOVZXBW xmm1, [byte rcx + 24]
	PMOVZXBW xmm3, [byte rdx + 16]
	PSUBW xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PMOVZXBW xmm6, [byte rcx + 32]
	PMOVZXBW xmm12, [byte rdx + 24]
	PSUBW xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	PMOVZXBW xmm2, [byte rcx + 40]
	PMOVZXBW xmm9, [byte rdx + 32]
	MOVDQA [byte r8 + 112], xmm13
	PMOVZXBW xmm11, [byte rcx + 48]
	PMOVZXBW xmm8, [byte rdx + 40]
	PSUBW xmm15, xmm14
	ADD r8, 128
	PMOVZXBW xmm13, [byte rcx + 56]
	PMOVZXBW xmm10, [byte rdx + 48]
	PSUBW xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVZXBW xmm7, [byte rdx + 56]
	PSUBW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	PSUBW xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PSUBW xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PSUBW xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PSUBW xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PSUBW xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Subtract_V8uV8u_V16u_SandyBridge
_yepCore_Subtract_V8uV8u_V16u_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXBW xmm15, [rcx]
	VPMOVZXBW xmm4, [byte rcx + 8]
	VPMOVZXBW xmm14, [rdx]
	VPMOVZXBW xmm5, [byte rcx + 16]
	VPMOVZXBW xmm0, [byte rdx + 8]
	VPMOVZXBW xmm1, [byte rcx + 24]
	VPMOVZXBW xmm3, [byte rdx + 16]
	VPMOVZXBW xmm6, [byte rcx + 32]
	VPMOVZXBW xmm12, [byte rdx + 24]
	VPMOVZXBW xmm2, [byte rcx + 40]
	VPMOVZXBW xmm9, [byte rdx + 32]
	VPMOVZXBW xmm11, [byte rcx + 48]
	VPMOVZXBW xmm8, [byte rdx + 40]
	VPSUBW xmm15, xmm15, xmm14
	VPMOVZXBW xmm13, [byte rcx + 56]
	VPMOVZXBW xmm10, [byte rdx + 48]
	VPSUBW xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVZXBW xmm7, [byte rdx + 56]
	VPSUBW xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXBW xmm15, [rcx]
	ADD rdx, 64
	VPSUBW xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPMOVZXBW xmm4, [byte rcx + 8]
	VPMOVZXBW xmm14, [rdx]
	VPSUBW xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMOVZXBW xmm5, [byte rcx + 16]
	VPMOVZXBW xmm0, [byte rdx + 8]
	VPSUBW xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPMOVZXBW xmm1, [byte rcx + 24]
	VPMOVZXBW xmm3, [byte rdx + 16]
	VPSUBW xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPMOVZXBW xmm6, [byte rcx + 32]
	VPMOVZXBW xmm12, [byte rdx + 24]
	VPSUBW xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VPMOVZXBW xmm2, [byte rcx + 40]
	VPMOVZXBW xmm9, [byte rdx + 32]
	VMOVDQA [byte r8 + 112], xmm13
	VPMOVZXBW xmm11, [byte rcx + 48]
	VPMOVZXBW xmm8, [byte rdx + 40]
	VPSUBW xmm15, xmm15, xmm14
	ADD r8, 128
	VPMOVZXBW xmm13, [byte rcx + 56]
	VPMOVZXBW xmm10, [byte rdx + 48]
	VPSUBW xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVZXBW xmm7, [byte rdx + 56]
	VPSUBW xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	VPSUBW xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPSUBW xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPSUBW xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPSUBW xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPSUBW xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Subtract_V8uV8u_V16u_Haswell
_yepCore_Subtract_V8uV8u_V16u_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXBW ymm15, [rcx]
	VPMOVZXBW ymm4, [byte rcx + 16]
	VPMOVZXBW ymm8, [rdx]
	VPMOVZXBW ymm5, [byte rcx + 32]
	VPMOVZXBW ymm10, [byte rdx + 16]
	VPMOVZXBW ymm14, [byte rcx + 48]
	VPMOVZXBW ymm12, [byte rdx + 32]
	VPMOVZXBW ymm6, [byte rcx + 64]
	VPMOVZXBW ymm13, [byte rdx + 48]
	VPMOVZXBW ymm1, [byte rcx + 80]
	VPMOVZXBW ymm7, [byte rdx + 64]
	VPMOVZXBW ymm2, [byte rcx + 96]
	VPMOVZXBW ymm3, [byte rdx + 80]
	VPSUBW ymm15, ymm15, ymm8
	VPMOVZXBW ymm0, [byte rcx + 112]
	VPMOVZXBW ymm11, [byte rdx + 96]
	VPSUBW ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVZXBW ymm9, [byte rdx + 112]
	VPSUBW ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXBW ymm15, [rcx]
	ADD rdx, 128
	VPSUBW ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPMOVZXBW ymm4, [byte rcx + 16]
	VPMOVZXBW ymm8, [rdx]
	VPSUBW ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPMOVZXBW ymm5, [byte rcx + 32]
	VPMOVZXBW ymm10, [byte rdx + 16]
	VPSUBW ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPMOVZXBW ymm14, [byte rcx + 48]
	VPMOVZXBW ymm12, [byte rdx + 32]
	VPSUBW ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPMOVZXBW ymm6, [byte rcx + 64]
	VPMOVZXBW ymm13, [byte rdx + 48]
	VPSUBW ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVZXBW ymm1, [byte rcx + 80]
	VPMOVZXBW ymm7, [byte rdx + 64]
	VMOVDQA [dword r8 + 224], ymm0
	VPMOVZXBW ymm2, [byte rcx + 96]
	VPMOVZXBW ymm3, [byte rdx + 80]
	VPSUBW ymm15, ymm15, ymm8
	ADD r8, 256
	VPMOVZXBW ymm0, [byte rcx + 112]
	VPMOVZXBW ymm11, [byte rdx + 96]
	VPSUBW ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVZXBW ymm9, [byte rdx + 112]
	VPSUBW ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPSUBW ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPSUBW ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPSUBW ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPSUBW ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPSUBW ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 128
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Subtract_V16sV16s_V16s_Nehalem
_yepCore_Subtract_V16sV16s_V16s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm15, [rcx]
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PSUBW xmm15, xmm14
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PSUBW xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PSUBW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm15, [rcx]
	ADD rdx, 128
	PSUBW xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	PSUBW xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	PSUBW xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	PSUBW xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	PSUBW xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQA [byte r8 + 112], xmm13
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PSUBW xmm15, xmm14
	ADD r8, 128
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PSUBW xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PSUBW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	PSUBW xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PSUBW xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PSUBW xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PSUBW xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PSUBW xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Subtract_V16sV16s_V16s_SandyBridge
_yepCore_Subtract_V16sV16s_V16s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm15, [rcx]
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPSUBW xmm15, xmm15, xmm14
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPSUBW xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPSUBW xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm15, [rcx]
	ADD rdx, 128
	VPSUBW xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VPSUBW xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VPSUBW xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VPSUBW xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VPSUBW xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQA [byte r8 + 112], xmm13
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPSUBW xmm15, xmm15, xmm14
	ADD r8, 128
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPSUBW xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPSUBW xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPSUBW xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPSUBW xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPSUBW xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPSUBW xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPSUBW xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Subtract_V16sV16s_V16s_Haswell
_yepCore_Subtract_V16sV16s_V16s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm15, [rcx]
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPSUBW ymm15, ymm15, ymm8
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPSUBW ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPSUBW ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm15, [rcx]
	ADD rdx, 256
	VPSUBW ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VPSUBW ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VPSUBW ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VPSUBW ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VPSUBW ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQA [dword r8 + 224], ymm0
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPSUBW ymm15, ymm15, ymm8
	ADD r8, 256
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPSUBW ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPSUBW ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 256
	VPSUBW ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPSUBW ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPSUBW ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPSUBW ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPSUBW ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Subtract_V16sV16s_V32s_K10
_yepCore_Subtract_V16sV16s_V32s_K10:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm12
	MOVAPS [byte rsp + 16], xmm13
	MOVAPS [byte rsp + 32], xmm14
	MOVAPS [byte rsp + 48], xmm15
	MOVAPS [byte rsp + 64], xmm6
	MOVAPS [byte rsp + 80], xmm8
	MOVAPS [byte rsp + 96], xmm9
	MOVAPS [byte rsp + 112], xmm10
	MOVAPS [dword rsp + 128], xmm11
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm12, [rcx]
	MOVQ xmm13, [rdx]
	PXOR xmm14, xmm14
	PXOR xmm15, xmm15
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PXOR xmm8, xmm8
	PXOR xmm2, xmm2
	PCMPGTW xmm14, xmm12
	PCMPGTW xmm15, xmm13
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PXOR xmm1, xmm1
	PXOR xmm9, xmm9
	PCMPGTW xmm8, xmm4
	PCMPGTW xmm2, xmm6
	PUNPCKLWD xmm12, xmm14
	PUNPCKLWD xmm13, xmm15
	PSUBD xmm12, xmm13
	MOVQ xmm0, [byte rcx + 24]
	MOVQ xmm10, [byte rdx + 24]
	PXOR xmm11, xmm11
	PXOR xmm7, xmm7
	PCMPGTW xmm1, xmm5
	PCMPGTW xmm9, xmm3
	PUNPCKLWD xmm4, xmm8
	PUNPCKLWD xmm6, xmm2
	PSUBD xmm4, xmm6
	MOVDQA [r8], xmm12
	ADD rcx, 32
	ADD rdx, 32
	PCMPGTW xmm11, xmm0
	PCMPGTW xmm7, xmm10
	PUNPCKLWD xmm5, xmm1
	PUNPCKLWD xmm3, xmm9
	PSUBD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm12, [rcx]
	MOVQ xmm13, [rdx]
	PXOR xmm14, xmm14
	PXOR xmm15, xmm15
	PUNPCKLWD xmm0, xmm11
	PUNPCKLWD xmm10, xmm7
	PSUBD xmm0, xmm10
	MOVDQA [byte r8 + 32], xmm5
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PXOR xmm8, xmm8
	PXOR xmm2, xmm2
	PCMPGTW xmm14, xmm12
	PCMPGTW xmm15, xmm13
	MOVDQA [byte r8 + 48], xmm0
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PXOR xmm1, xmm1
	PXOR xmm9, xmm9
	PCMPGTW xmm8, xmm4
	PCMPGTW xmm2, xmm6
	PUNPCKLWD xmm12, xmm14
	PUNPCKLWD xmm13, xmm15
	PSUBD xmm12, xmm13
	ADD r8, 64
	MOVQ xmm0, [byte rcx + 24]
	MOVQ xmm10, [byte rdx + 24]
	PXOR xmm11, xmm11
	PXOR xmm7, xmm7
	PCMPGTW xmm1, xmm5
	PCMPGTW xmm9, xmm3
	PUNPCKLWD xmm4, xmm8
	PUNPCKLWD xmm6, xmm2
	PSUBD xmm4, xmm6
	MOVDQA [r8], xmm12
	ADD rcx, 32
	ADD rdx, 32
	PCMPGTW xmm11, xmm0
	PCMPGTW xmm7, xmm10
	PUNPCKLWD xmm5, xmm1
	PUNPCKLWD xmm3, xmm9
	PSUBD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLWD xmm0, xmm11
	PUNPCKLWD xmm10, xmm7
	PSUBD xmm0, xmm10
	MOVDQA [byte r8 + 32], xmm5
	MOVDQA [byte r8 + 48], xmm0
	ADD r8, 64
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm12, [rsp]
	MOVAPS xmm13, [byte rsp + 16]
	MOVAPS xmm14, [byte rsp + 32]
	MOVAPS xmm15, [byte rsp + 48]
	MOVAPS xmm6, [byte rsp + 64]
	MOVAPS xmm8, [byte rsp + 80]
	MOVAPS xmm9, [byte rsp + 96]
	MOVAPS xmm10, [byte rsp + 112]
	MOVAPS xmm11, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Subtract_V16sV16s_V32s_Nehalem
_yepCore_Subtract_V16sV16s_V32s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVSXWD xmm15, [rcx]
	PMOVSXWD xmm4, [byte rcx + 8]
	PMOVSXWD xmm14, [rdx]
	PMOVSXWD xmm5, [byte rcx + 16]
	PMOVSXWD xmm0, [byte rdx + 8]
	PMOVSXWD xmm1, [byte rcx + 24]
	PMOVSXWD xmm3, [byte rdx + 16]
	PMOVSXWD xmm6, [byte rcx + 32]
	PMOVSXWD xmm12, [byte rdx + 24]
	PMOVSXWD xmm2, [byte rcx + 40]
	PMOVSXWD xmm9, [byte rdx + 32]
	PMOVSXWD xmm11, [byte rcx + 48]
	PMOVSXWD xmm8, [byte rdx + 40]
	PSUBD xmm15, xmm14
	PMOVSXWD xmm13, [byte rcx + 56]
	PMOVSXWD xmm10, [byte rdx + 48]
	PSUBD xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVSXWD xmm7, [byte rdx + 56]
	PSUBD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVSXWD xmm15, [rcx]
	ADD rdx, 64
	PSUBD xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PMOVSXWD xmm4, [byte rcx + 8]
	PMOVSXWD xmm14, [rdx]
	PSUBD xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PMOVSXWD xmm5, [byte rcx + 16]
	PMOVSXWD xmm0, [byte rdx + 8]
	PSUBD xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PMOVSXWD xmm1, [byte rcx + 24]
	PMOVSXWD xmm3, [byte rdx + 16]
	PSUBD xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PMOVSXWD xmm6, [byte rcx + 32]
	PMOVSXWD xmm12, [byte rdx + 24]
	PSUBD xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	PMOVSXWD xmm2, [byte rcx + 40]
	PMOVSXWD xmm9, [byte rdx + 32]
	MOVDQA [byte r8 + 112], xmm13
	PMOVSXWD xmm11, [byte rcx + 48]
	PMOVSXWD xmm8, [byte rdx + 40]
	PSUBD xmm15, xmm14
	ADD r8, 128
	PMOVSXWD xmm13, [byte rcx + 56]
	PMOVSXWD xmm10, [byte rdx + 48]
	PSUBD xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVSXWD xmm7, [byte rdx + 56]
	PSUBD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	PSUBD xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PSUBD xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PSUBD xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PSUBD xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PSUBD xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Subtract_V16sV16s_V32s_SandyBridge
_yepCore_Subtract_V16sV16s_V32s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXWD xmm15, [rcx]
	VPMOVSXWD xmm4, [byte rcx + 8]
	VPMOVSXWD xmm14, [rdx]
	VPMOVSXWD xmm5, [byte rcx + 16]
	VPMOVSXWD xmm0, [byte rdx + 8]
	VPMOVSXWD xmm1, [byte rcx + 24]
	VPMOVSXWD xmm3, [byte rdx + 16]
	VPMOVSXWD xmm6, [byte rcx + 32]
	VPMOVSXWD xmm12, [byte rdx + 24]
	VPMOVSXWD xmm2, [byte rcx + 40]
	VPMOVSXWD xmm9, [byte rdx + 32]
	VPMOVSXWD xmm11, [byte rcx + 48]
	VPMOVSXWD xmm8, [byte rdx + 40]
	VPSUBD xmm15, xmm15, xmm14
	VPMOVSXWD xmm13, [byte rcx + 56]
	VPMOVSXWD xmm10, [byte rdx + 48]
	VPSUBD xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVSXWD xmm7, [byte rdx + 56]
	VPSUBD xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXWD xmm15, [rcx]
	ADD rdx, 64
	VPSUBD xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPMOVSXWD xmm4, [byte rcx + 8]
	VPMOVSXWD xmm14, [rdx]
	VPSUBD xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMOVSXWD xmm5, [byte rcx + 16]
	VPMOVSXWD xmm0, [byte rdx + 8]
	VPSUBD xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPMOVSXWD xmm1, [byte rcx + 24]
	VPMOVSXWD xmm3, [byte rdx + 16]
	VPSUBD xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPMOVSXWD xmm6, [byte rcx + 32]
	VPMOVSXWD xmm12, [byte rdx + 24]
	VPSUBD xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VPMOVSXWD xmm2, [byte rcx + 40]
	VPMOVSXWD xmm9, [byte rdx + 32]
	VMOVDQA [byte r8 + 112], xmm13
	VPMOVSXWD xmm11, [byte rcx + 48]
	VPMOVSXWD xmm8, [byte rdx + 40]
	VPSUBD xmm15, xmm15, xmm14
	ADD r8, 128
	VPMOVSXWD xmm13, [byte rcx + 56]
	VPMOVSXWD xmm10, [byte rdx + 48]
	VPSUBD xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVSXWD xmm7, [byte rdx + 56]
	VPSUBD xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	VPSUBD xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPSUBD xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPSUBD xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPSUBD xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPSUBD xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Subtract_V16sV16s_V32s_Haswell
_yepCore_Subtract_V16sV16s_V32s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXWD ymm15, [rcx]
	VPMOVSXWD ymm4, [byte rcx + 16]
	VPMOVSXWD ymm8, [rdx]
	VPMOVSXWD ymm5, [byte rcx + 32]
	VPMOVSXWD ymm10, [byte rdx + 16]
	VPMOVSXWD ymm14, [byte rcx + 48]
	VPMOVSXWD ymm12, [byte rdx + 32]
	VPMOVSXWD ymm6, [byte rcx + 64]
	VPMOVSXWD ymm13, [byte rdx + 48]
	VPMOVSXWD ymm1, [byte rcx + 80]
	VPMOVSXWD ymm7, [byte rdx + 64]
	VPMOVSXWD ymm2, [byte rcx + 96]
	VPMOVSXWD ymm3, [byte rdx + 80]
	VPSUBD ymm15, ymm15, ymm8
	VPMOVSXWD ymm0, [byte rcx + 112]
	VPMOVSXWD ymm11, [byte rdx + 96]
	VPSUBD ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVSXWD ymm9, [byte rdx + 112]
	VPSUBD ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXWD ymm15, [rcx]
	ADD rdx, 128
	VPSUBD ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPMOVSXWD ymm4, [byte rcx + 16]
	VPMOVSXWD ymm8, [rdx]
	VPSUBD ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPMOVSXWD ymm5, [byte rcx + 32]
	VPMOVSXWD ymm10, [byte rdx + 16]
	VPSUBD ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPMOVSXWD ymm14, [byte rcx + 48]
	VPMOVSXWD ymm12, [byte rdx + 32]
	VPSUBD ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPMOVSXWD ymm6, [byte rcx + 64]
	VPMOVSXWD ymm13, [byte rdx + 48]
	VPSUBD ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVSXWD ymm1, [byte rcx + 80]
	VPMOVSXWD ymm7, [byte rdx + 64]
	VMOVDQA [dword r8 + 224], ymm0
	VPMOVSXWD ymm2, [byte rcx + 96]
	VPMOVSXWD ymm3, [byte rdx + 80]
	VPSUBD ymm15, ymm15, ymm8
	ADD r8, 256
	VPMOVSXWD ymm0, [byte rcx + 112]
	VPMOVSXWD ymm11, [byte rdx + 96]
	VPSUBD ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVSXWD ymm9, [byte rdx + 112]
	VPSUBD ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPSUBD ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPSUBD ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPSUBD ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPSUBD ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPSUBD ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Subtract_V16uV16u_V32u_K10
_yepCore_Subtract_V16uV16u_V32u_K10:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm12
	MOVAPS [byte rsp + 16], xmm13
	MOVAPS [byte rsp + 32], xmm14
	MOVAPS [byte rsp + 48], xmm6
	MOVAPS [byte rsp + 64], xmm7
	MOVAPS [byte rsp + 80], xmm8
	MOVAPS [byte rsp + 96], xmm11
	MOVAPS [byte rsp + 112], xmm10
	MOVAPS [dword rsp + 128], xmm9
	PXOR xmm12, xmm12
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 28
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm13, [rcx]
	MOVQ xmm14, [rdx]
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	MOVQ xmm7, [byte rcx + 24]
	MOVQ xmm0, [byte rdx + 24]
	PUNPCKLWD xmm13, xmm12
	MOVQ xmm8, [byte rcx + 32]
	MOVQ xmm11, [byte rdx + 32]
	PUNPCKLWD xmm4, xmm12
	PUNPCKLWD xmm14, xmm12
	MOVQ xmm1, [byte rcx + 40]
	MOVQ xmm2, [byte rdx + 40]
	PUNPCKLWD xmm5, xmm12
	PUNPCKLWD xmm6, xmm12
	PSUBD xmm13, xmm14
	MOVQ xmm10, [byte rcx + 48]
	MOVQ xmm9, [byte rdx + 48]
	PUNPCKLWD xmm7, xmm12
	PUNPCKLWD xmm3, xmm12
	PSUBD xmm4, xmm6
	MOVDQA [r8], xmm13
	ADD rcx, 56
	ADD rdx, 56
	PUNPCKLWD xmm8, xmm12
	PUNPCKLWD xmm0, xmm12
	PSUBD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm13, [rcx]
	MOVQ xmm14, [rdx]
	PUNPCKLWD xmm1, xmm12
	PUNPCKLWD xmm11, xmm12
	PSUBD xmm7, xmm0
	MOVDQA [byte r8 + 32], xmm5
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PUNPCKLWD xmm10, xmm12
	PUNPCKLWD xmm2, xmm12
	PSUBD xmm8, xmm11
	MOVDQA [byte r8 + 48], xmm7
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PUNPCKLWD xmm9, xmm12
	PSUBD xmm1, xmm2
	MOVDQA [byte r8 + 64], xmm8
	MOVQ xmm7, [byte rcx + 24]
	MOVQ xmm0, [byte rdx + 24]
	PUNPCKLWD xmm13, xmm12
	PSUBD xmm10, xmm9
	MOVDQA [byte r8 + 80], xmm1
	MOVQ xmm8, [byte rcx + 32]
	MOVQ xmm11, [byte rdx + 32]
	PUNPCKLWD xmm4, xmm12
	PUNPCKLWD xmm14, xmm12
	MOVDQA [byte r8 + 96], xmm10
	MOVQ xmm1, [byte rcx + 40]
	MOVQ xmm2, [byte rdx + 40]
	PUNPCKLWD xmm5, xmm12
	PUNPCKLWD xmm6, xmm12
	PSUBD xmm13, xmm14
	ADD r8, 112
	MOVQ xmm10, [byte rcx + 48]
	MOVQ xmm9, [byte rdx + 48]
	PUNPCKLWD xmm7, xmm12
	PUNPCKLWD xmm3, xmm12
	PSUBD xmm4, xmm6
	MOVDQA [r8], xmm13
	ADD rcx, 56
	ADD rdx, 56
	PUNPCKLWD xmm8, xmm12
	PUNPCKLWD xmm0, xmm12
	PSUBD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 28
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLWD xmm1, xmm12
	PUNPCKLWD xmm11, xmm12
	PSUBD xmm7, xmm0
	MOVDQA [byte r8 + 32], xmm5
	PUNPCKLWD xmm10, xmm12
	PUNPCKLWD xmm2, xmm12
	PSUBD xmm8, xmm11
	MOVDQA [byte r8 + 48], xmm7
	PUNPCKLWD xmm9, xmm12
	PSUBD xmm1, xmm2
	MOVDQA [byte r8 + 64], xmm8
	PSUBD xmm10, xmm9
	MOVDQA [byte r8 + 80], xmm1
	MOVDQA [byte r8 + 96], xmm10
	ADD r8, 112
	.batch_process_finish:
	ADD r9, 28
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm12, [rsp]
	MOVAPS xmm13, [byte rsp + 16]
	MOVAPS xmm14, [byte rsp + 32]
	MOVAPS xmm6, [byte rsp + 48]
	MOVAPS xmm7, [byte rsp + 64]
	MOVAPS xmm8, [byte rsp + 80]
	MOVAPS xmm11, [byte rsp + 96]
	MOVAPS xmm10, [byte rsp + 112]
	MOVAPS xmm9, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Subtract_V16uV16u_V32u_Nehalem
_yepCore_Subtract_V16uV16u_V32u_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXWD xmm15, [rcx]
	PMOVZXWD xmm4, [byte rcx + 8]
	PMOVZXWD xmm14, [rdx]
	PMOVZXWD xmm5, [byte rcx + 16]
	PMOVZXWD xmm0, [byte rdx + 8]
	PMOVZXWD xmm1, [byte rcx + 24]
	PMOVZXWD xmm3, [byte rdx + 16]
	PMOVZXWD xmm6, [byte rcx + 32]
	PMOVZXWD xmm12, [byte rdx + 24]
	PMOVZXWD xmm2, [byte rcx + 40]
	PMOVZXWD xmm9, [byte rdx + 32]
	PMOVZXWD xmm11, [byte rcx + 48]
	PMOVZXWD xmm8, [byte rdx + 40]
	PSUBD xmm15, xmm14
	PMOVZXWD xmm13, [byte rcx + 56]
	PMOVZXWD xmm10, [byte rdx + 48]
	PSUBD xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVZXWD xmm7, [byte rdx + 56]
	PSUBD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXWD xmm15, [rcx]
	ADD rdx, 64
	PSUBD xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PMOVZXWD xmm4, [byte rcx + 8]
	PMOVZXWD xmm14, [rdx]
	PSUBD xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PMOVZXWD xmm5, [byte rcx + 16]
	PMOVZXWD xmm0, [byte rdx + 8]
	PSUBD xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PMOVZXWD xmm1, [byte rcx + 24]
	PMOVZXWD xmm3, [byte rdx + 16]
	PSUBD xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PMOVZXWD xmm6, [byte rcx + 32]
	PMOVZXWD xmm12, [byte rdx + 24]
	PSUBD xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	PMOVZXWD xmm2, [byte rcx + 40]
	PMOVZXWD xmm9, [byte rdx + 32]
	MOVDQA [byte r8 + 112], xmm13
	PMOVZXWD xmm11, [byte rcx + 48]
	PMOVZXWD xmm8, [byte rdx + 40]
	PSUBD xmm15, xmm14
	ADD r8, 128
	PMOVZXWD xmm13, [byte rcx + 56]
	PMOVZXWD xmm10, [byte rdx + 48]
	PSUBD xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVZXWD xmm7, [byte rdx + 56]
	PSUBD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	PSUBD xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PSUBD xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PSUBD xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PSUBD xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PSUBD xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Subtract_V16uV16u_V32u_SandyBridge
_yepCore_Subtract_V16uV16u_V32u_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXWD xmm15, [rcx]
	VPMOVZXWD xmm4, [byte rcx + 8]
	VPMOVZXWD xmm14, [rdx]
	VPMOVZXWD xmm5, [byte rcx + 16]
	VPMOVZXWD xmm0, [byte rdx + 8]
	VPMOVZXWD xmm1, [byte rcx + 24]
	VPMOVZXWD xmm3, [byte rdx + 16]
	VPMOVZXWD xmm6, [byte rcx + 32]
	VPMOVZXWD xmm12, [byte rdx + 24]
	VPMOVZXWD xmm2, [byte rcx + 40]
	VPMOVZXWD xmm9, [byte rdx + 32]
	VPMOVZXWD xmm11, [byte rcx + 48]
	VPMOVZXWD xmm8, [byte rdx + 40]
	VPSUBD xmm15, xmm15, xmm14
	VPMOVZXWD xmm13, [byte rcx + 56]
	VPMOVZXWD xmm10, [byte rdx + 48]
	VPSUBD xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVZXWD xmm7, [byte rdx + 56]
	VPSUBD xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXWD xmm15, [rcx]
	ADD rdx, 64
	VPSUBD xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPMOVZXWD xmm4, [byte rcx + 8]
	VPMOVZXWD xmm14, [rdx]
	VPSUBD xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMOVZXWD xmm5, [byte rcx + 16]
	VPMOVZXWD xmm0, [byte rdx + 8]
	VPSUBD xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPMOVZXWD xmm1, [byte rcx + 24]
	VPMOVZXWD xmm3, [byte rdx + 16]
	VPSUBD xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPMOVZXWD xmm6, [byte rcx + 32]
	VPMOVZXWD xmm12, [byte rdx + 24]
	VPSUBD xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VPMOVZXWD xmm2, [byte rcx + 40]
	VPMOVZXWD xmm9, [byte rdx + 32]
	VMOVDQA [byte r8 + 112], xmm13
	VPMOVZXWD xmm11, [byte rcx + 48]
	VPMOVZXWD xmm8, [byte rdx + 40]
	VPSUBD xmm15, xmm15, xmm14
	ADD r8, 128
	VPMOVZXWD xmm13, [byte rcx + 56]
	VPMOVZXWD xmm10, [byte rdx + 48]
	VPSUBD xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVZXWD xmm7, [byte rdx + 56]
	VPSUBD xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	VPSUBD xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPSUBD xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPSUBD xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPSUBD xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPSUBD xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Subtract_V16uV16u_V32u_Haswell
_yepCore_Subtract_V16uV16u_V32u_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXWD ymm15, [rcx]
	VPMOVZXWD ymm4, [byte rcx + 16]
	VPMOVZXWD ymm8, [rdx]
	VPMOVZXWD ymm5, [byte rcx + 32]
	VPMOVZXWD ymm10, [byte rdx + 16]
	VPMOVZXWD ymm14, [byte rcx + 48]
	VPMOVZXWD ymm12, [byte rdx + 32]
	VPMOVZXWD ymm6, [byte rcx + 64]
	VPMOVZXWD ymm13, [byte rdx + 48]
	VPMOVZXWD ymm1, [byte rcx + 80]
	VPMOVZXWD ymm7, [byte rdx + 64]
	VPMOVZXWD ymm2, [byte rcx + 96]
	VPMOVZXWD ymm3, [byte rdx + 80]
	VPSUBD ymm15, ymm15, ymm8
	VPMOVZXWD ymm0, [byte rcx + 112]
	VPMOVZXWD ymm11, [byte rdx + 96]
	VPSUBD ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVZXWD ymm9, [byte rdx + 112]
	VPSUBD ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXWD ymm15, [rcx]
	ADD rdx, 128
	VPSUBD ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPMOVZXWD ymm4, [byte rcx + 16]
	VPMOVZXWD ymm8, [rdx]
	VPSUBD ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPMOVZXWD ymm5, [byte rcx + 32]
	VPMOVZXWD ymm10, [byte rdx + 16]
	VPSUBD ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPMOVZXWD ymm14, [byte rcx + 48]
	VPMOVZXWD ymm12, [byte rdx + 32]
	VPSUBD ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPMOVZXWD ymm6, [byte rcx + 64]
	VPMOVZXWD ymm13, [byte rdx + 48]
	VPSUBD ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVZXWD ymm1, [byte rcx + 80]
	VPMOVZXWD ymm7, [byte rdx + 64]
	VMOVDQA [dword r8 + 224], ymm0
	VPMOVZXWD ymm2, [byte rcx + 96]
	VPMOVZXWD ymm3, [byte rdx + 80]
	VPSUBD ymm15, ymm15, ymm8
	ADD r8, 256
	VPMOVZXWD ymm0, [byte rcx + 112]
	VPMOVZXWD ymm11, [byte rdx + 96]
	VPSUBD ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVZXWD ymm9, [byte rdx + 112]
	VPSUBD ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPSUBD ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPSUBD ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPSUBD ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPSUBD ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPSUBD ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Subtract_V32sV32s_V32s_Nehalem
_yepCore_Subtract_V32sV32s_V32s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm15, [rcx]
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PSUBD xmm15, xmm14
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PSUBD xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PSUBD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm15, [rcx]
	ADD rdx, 128
	PSUBD xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	PSUBD xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	PSUBD xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	PSUBD xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	PSUBD xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQA [byte r8 + 112], xmm13
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PSUBD xmm15, xmm14
	ADD r8, 128
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PSUBD xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PSUBD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	PSUBD xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PSUBD xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PSUBD xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PSUBD xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PSUBD xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Subtract_V32sV32s_V32s_SandyBridge
_yepCore_Subtract_V32sV32s_V32s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm15, [rcx]
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPSUBD xmm15, xmm15, xmm14
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPSUBD xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPSUBD xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm15, [rcx]
	ADD rdx, 128
	VPSUBD xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VPSUBD xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VPSUBD xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VPSUBD xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VPSUBD xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQA [byte r8 + 112], xmm13
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPSUBD xmm15, xmm15, xmm14
	ADD r8, 128
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPSUBD xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPSUBD xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPSUBD xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPSUBD xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPSUBD xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPSUBD xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPSUBD xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Subtract_V32sV32s_V32s_Haswell
_yepCore_Subtract_V32sV32s_V32s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm15, [rcx]
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPSUBD ymm15, ymm15, ymm8
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPSUBD ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPSUBD ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm15, [rcx]
	ADD rdx, 256
	VPSUBD ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VPSUBD ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VPSUBD ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VPSUBD ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VPSUBD ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQA [dword r8 + 224], ymm0
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPSUBD ymm15, ymm15, ymm8
	ADD r8, 256
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPSUBD ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPSUBD ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 256
	VPSUBD ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPSUBD ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPSUBD ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPSUBD ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPSUBD ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	SUB eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Subtract_V32sV32s_V64s_K10
_yepCore_Subtract_V32sV32s_V64s_K10:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm12
	MOVAPS [byte rsp + 16], xmm13
	MOVAPS [byte rsp + 32], xmm14
	MOVAPS [byte rsp + 48], xmm15
	MOVAPS [byte rsp + 64], xmm6
	MOVAPS [byte rsp + 80], xmm8
	MOVAPS [byte rsp + 96], xmm9
	MOVAPS [byte rsp + 112], xmm10
	MOVAPS [dword rsp + 128], xmm11
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 8
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm12, [rcx]
	MOVQ xmm13, [rdx]
	PXOR xmm14, xmm14
	PXOR xmm15, xmm15
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PXOR xmm8, xmm8
	PXOR xmm2, xmm2
	PCMPGTD xmm14, xmm12
	PCMPGTD xmm15, xmm13
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PXOR xmm1, xmm1
	PXOR xmm9, xmm9
	PCMPGTD xmm8, xmm4
	PCMPGTD xmm2, xmm6
	PUNPCKLDQ xmm12, xmm14
	PUNPCKLDQ xmm13, xmm15
	PSUBQ xmm12, xmm13
	MOVQ xmm0, [byte rcx + 24]
	MOVQ xmm10, [byte rdx + 24]
	PXOR xmm11, xmm11
	PXOR xmm7, xmm7
	PCMPGTD xmm1, xmm5
	PCMPGTD xmm9, xmm3
	PUNPCKLDQ xmm4, xmm8
	PUNPCKLDQ xmm6, xmm2
	PSUBQ xmm4, xmm6
	MOVDQA [r8], xmm12
	ADD rcx, 32
	ADD rdx, 32
	PCMPGTD xmm11, xmm0
	PCMPGTD xmm7, xmm10
	PUNPCKLDQ xmm5, xmm1
	PUNPCKLDQ xmm3, xmm9
	PSUBQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 8
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm12, [rcx]
	MOVQ xmm13, [rdx]
	PXOR xmm14, xmm14
	PXOR xmm15, xmm15
	PUNPCKLDQ xmm0, xmm11
	PUNPCKLDQ xmm10, xmm7
	PSUBQ xmm0, xmm10
	MOVDQA [byte r8 + 32], xmm5
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PXOR xmm8, xmm8
	PXOR xmm2, xmm2
	PCMPGTD xmm14, xmm12
	PCMPGTD xmm15, xmm13
	MOVDQA [byte r8 + 48], xmm0
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PXOR xmm1, xmm1
	PXOR xmm9, xmm9
	PCMPGTD xmm8, xmm4
	PCMPGTD xmm2, xmm6
	PUNPCKLDQ xmm12, xmm14
	PUNPCKLDQ xmm13, xmm15
	PSUBQ xmm12, xmm13
	ADD r8, 64
	MOVQ xmm0, [byte rcx + 24]
	MOVQ xmm10, [byte rdx + 24]
	PXOR xmm11, xmm11
	PXOR xmm7, xmm7
	PCMPGTD xmm1, xmm5
	PCMPGTD xmm9, xmm3
	PUNPCKLDQ xmm4, xmm8
	PUNPCKLDQ xmm6, xmm2
	PSUBQ xmm4, xmm6
	MOVDQA [r8], xmm12
	ADD rcx, 32
	ADD rdx, 32
	PCMPGTD xmm11, xmm0
	PCMPGTD xmm7, xmm10
	PUNPCKLDQ xmm5, xmm1
	PUNPCKLDQ xmm3, xmm9
	PSUBQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 8
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLDQ xmm0, xmm11
	PUNPCKLDQ xmm10, xmm7
	PSUBQ xmm0, xmm10
	MOVDQA [byte r8 + 32], xmm5
	MOVDQA [byte r8 + 48], xmm0
	ADD r8, 64
	.batch_process_finish:
	ADD r9, 8
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm12, [rsp]
	MOVAPS xmm13, [byte rsp + 16]
	MOVAPS xmm14, [byte rsp + 32]
	MOVAPS xmm15, [byte rsp + 48]
	MOVAPS xmm6, [byte rsp + 64]
	MOVAPS xmm8, [byte rsp + 80]
	MOVAPS xmm9, [byte rsp + 96]
	MOVAPS xmm10, [byte rsp + 112]
	MOVAPS xmm11, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Subtract_V32sV32s_V64s_Nehalem
_yepCore_Subtract_V32sV32s_V64s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVSXDQ xmm15, [rcx]
	PMOVSXDQ xmm4, [byte rcx + 8]
	PMOVSXDQ xmm14, [rdx]
	PMOVSXDQ xmm5, [byte rcx + 16]
	PMOVSXDQ xmm0, [byte rdx + 8]
	PMOVSXDQ xmm1, [byte rcx + 24]
	PMOVSXDQ xmm3, [byte rdx + 16]
	PMOVSXDQ xmm6, [byte rcx + 32]
	PMOVSXDQ xmm12, [byte rdx + 24]
	PMOVSXDQ xmm2, [byte rcx + 40]
	PMOVSXDQ xmm9, [byte rdx + 32]
	PMOVSXDQ xmm11, [byte rcx + 48]
	PMOVSXDQ xmm8, [byte rdx + 40]
	PSUBQ xmm15, xmm14
	PMOVSXDQ xmm13, [byte rcx + 56]
	PMOVSXDQ xmm10, [byte rdx + 48]
	PSUBQ xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVSXDQ xmm7, [byte rdx + 56]
	PSUBQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVSXDQ xmm15, [rcx]
	ADD rdx, 64
	PSUBQ xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PMOVSXDQ xmm4, [byte rcx + 8]
	PMOVSXDQ xmm14, [rdx]
	PSUBQ xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PMOVSXDQ xmm5, [byte rcx + 16]
	PMOVSXDQ xmm0, [byte rdx + 8]
	PSUBQ xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PMOVSXDQ xmm1, [byte rcx + 24]
	PMOVSXDQ xmm3, [byte rdx + 16]
	PSUBQ xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PMOVSXDQ xmm6, [byte rcx + 32]
	PMOVSXDQ xmm12, [byte rdx + 24]
	PSUBQ xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	PMOVSXDQ xmm2, [byte rcx + 40]
	PMOVSXDQ xmm9, [byte rdx + 32]
	MOVDQA [byte r8 + 112], xmm13
	PMOVSXDQ xmm11, [byte rcx + 48]
	PMOVSXDQ xmm8, [byte rdx + 40]
	PSUBQ xmm15, xmm14
	ADD r8, 128
	PMOVSXDQ xmm13, [byte rcx + 56]
	PMOVSXDQ xmm10, [byte rdx + 48]
	PSUBQ xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVSXDQ xmm7, [byte rdx + 56]
	PSUBQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	PSUBQ xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PSUBQ xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PSUBQ xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PSUBQ xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PSUBQ xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Subtract_V32sV32s_V64s_SandyBridge
_yepCore_Subtract_V32sV32s_V64s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXDQ xmm15, [rcx]
	VPMOVSXDQ xmm4, [byte rcx + 8]
	VPMOVSXDQ xmm14, [rdx]
	VPMOVSXDQ xmm5, [byte rcx + 16]
	VPMOVSXDQ xmm0, [byte rdx + 8]
	VPMOVSXDQ xmm1, [byte rcx + 24]
	VPMOVSXDQ xmm3, [byte rdx + 16]
	VPMOVSXDQ xmm6, [byte rcx + 32]
	VPMOVSXDQ xmm12, [byte rdx + 24]
	VPMOVSXDQ xmm2, [byte rcx + 40]
	VPMOVSXDQ xmm9, [byte rdx + 32]
	VPMOVSXDQ xmm11, [byte rcx + 48]
	VPMOVSXDQ xmm8, [byte rdx + 40]
	VPSUBQ xmm15, xmm15, xmm14
	VPMOVSXDQ xmm13, [byte rcx + 56]
	VPMOVSXDQ xmm10, [byte rdx + 48]
	VPSUBQ xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVSXDQ xmm7, [byte rdx + 56]
	VPSUBQ xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXDQ xmm15, [rcx]
	ADD rdx, 64
	VPSUBQ xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPMOVSXDQ xmm4, [byte rcx + 8]
	VPMOVSXDQ xmm14, [rdx]
	VPSUBQ xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMOVSXDQ xmm5, [byte rcx + 16]
	VPMOVSXDQ xmm0, [byte rdx + 8]
	VPSUBQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPMOVSXDQ xmm1, [byte rcx + 24]
	VPMOVSXDQ xmm3, [byte rdx + 16]
	VPSUBQ xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPMOVSXDQ xmm6, [byte rcx + 32]
	VPMOVSXDQ xmm12, [byte rdx + 24]
	VPSUBQ xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VPMOVSXDQ xmm2, [byte rcx + 40]
	VPMOVSXDQ xmm9, [byte rdx + 32]
	VMOVDQA [byte r8 + 112], xmm13
	VPMOVSXDQ xmm11, [byte rcx + 48]
	VPMOVSXDQ xmm8, [byte rdx + 40]
	VPSUBQ xmm15, xmm15, xmm14
	ADD r8, 128
	VPMOVSXDQ xmm13, [byte rcx + 56]
	VPMOVSXDQ xmm10, [byte rdx + 48]
	VPSUBQ xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVSXDQ xmm7, [byte rdx + 56]
	VPSUBQ xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	VPSUBQ xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPSUBQ xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPSUBQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPSUBQ xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPSUBQ xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Subtract_V32sV32s_V64s_Haswell
_yepCore_Subtract_V32sV32s_V64s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXDQ ymm15, [rcx]
	VPMOVSXDQ ymm4, [byte rcx + 16]
	VPMOVSXDQ ymm8, [rdx]
	VPMOVSXDQ ymm5, [byte rcx + 32]
	VPMOVSXDQ ymm10, [byte rdx + 16]
	VPMOVSXDQ ymm14, [byte rcx + 48]
	VPMOVSXDQ ymm12, [byte rdx + 32]
	VPMOVSXDQ ymm6, [byte rcx + 64]
	VPMOVSXDQ ymm13, [byte rdx + 48]
	VPMOVSXDQ ymm1, [byte rcx + 80]
	VPMOVSXDQ ymm7, [byte rdx + 64]
	VPMOVSXDQ ymm2, [byte rcx + 96]
	VPMOVSXDQ ymm3, [byte rdx + 80]
	VPSUBQ ymm15, ymm15, ymm8
	VPMOVSXDQ ymm0, [byte rcx + 112]
	VPMOVSXDQ ymm11, [byte rdx + 96]
	VPSUBQ ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVSXDQ ymm9, [byte rdx + 112]
	VPSUBQ ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXDQ ymm15, [rcx]
	ADD rdx, 128
	VPSUBQ ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPMOVSXDQ ymm4, [byte rcx + 16]
	VPMOVSXDQ ymm8, [rdx]
	VPSUBQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPMOVSXDQ ymm5, [byte rcx + 32]
	VPMOVSXDQ ymm10, [byte rdx + 16]
	VPSUBQ ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPMOVSXDQ ymm14, [byte rcx + 48]
	VPMOVSXDQ ymm12, [byte rdx + 32]
	VPSUBQ ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPMOVSXDQ ymm6, [byte rcx + 64]
	VPMOVSXDQ ymm13, [byte rdx + 48]
	VPSUBQ ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVSXDQ ymm1, [byte rcx + 80]
	VPMOVSXDQ ymm7, [byte rdx + 64]
	VMOVDQA [dword r8 + 224], ymm0
	VPMOVSXDQ ymm2, [byte rcx + 96]
	VPMOVSXDQ ymm3, [byte rdx + 80]
	VPSUBQ ymm15, ymm15, ymm8
	ADD r8, 256
	VPMOVSXDQ ymm0, [byte rcx + 112]
	VPMOVSXDQ ymm11, [byte rdx + 96]
	VPSUBQ ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVSXDQ ymm9, [byte rdx + 112]
	VPSUBQ ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPSUBQ ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPSUBQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPSUBQ ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPSUBQ ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPSUBQ ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Subtract_V32uV32u_V64u_K10
_yepCore_Subtract_V32uV32u_V64u_K10:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm12
	MOVAPS [byte rsp + 16], xmm13
	MOVAPS [byte rsp + 32], xmm14
	MOVAPS [byte rsp + 48], xmm6
	MOVAPS [byte rsp + 64], xmm7
	MOVAPS [byte rsp + 80], xmm8
	MOVAPS [byte rsp + 96], xmm11
	MOVAPS [byte rsp + 112], xmm10
	MOVAPS [dword rsp + 128], xmm9
	PXOR xmm12, xmm12
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 14
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm13, [rcx]
	MOVQ xmm14, [rdx]
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	MOVQ xmm7, [byte rcx + 24]
	MOVQ xmm0, [byte rdx + 24]
	PUNPCKLDQ xmm13, xmm12
	MOVQ xmm8, [byte rcx + 32]
	MOVQ xmm11, [byte rdx + 32]
	PUNPCKLDQ xmm4, xmm12
	PUNPCKLDQ xmm14, xmm12
	MOVQ xmm1, [byte rcx + 40]
	MOVQ xmm2, [byte rdx + 40]
	PUNPCKLDQ xmm5, xmm12
	PUNPCKLDQ xmm6, xmm12
	PSUBQ xmm13, xmm14
	MOVQ xmm10, [byte rcx + 48]
	MOVQ xmm9, [byte rdx + 48]
	PUNPCKLDQ xmm7, xmm12
	PUNPCKLDQ xmm3, xmm12
	PSUBQ xmm4, xmm6
	MOVDQA [r8], xmm13
	ADD rcx, 56
	ADD rdx, 56
	PUNPCKLDQ xmm8, xmm12
	PUNPCKLDQ xmm0, xmm12
	PSUBQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 14
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm13, [rcx]
	MOVQ xmm14, [rdx]
	PUNPCKLDQ xmm1, xmm12
	PUNPCKLDQ xmm11, xmm12
	PSUBQ xmm7, xmm0
	MOVDQA [byte r8 + 32], xmm5
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PUNPCKLDQ xmm10, xmm12
	PUNPCKLDQ xmm2, xmm12
	PSUBQ xmm8, xmm11
	MOVDQA [byte r8 + 48], xmm7
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PUNPCKLDQ xmm9, xmm12
	PSUBQ xmm1, xmm2
	MOVDQA [byte r8 + 64], xmm8
	MOVQ xmm7, [byte rcx + 24]
	MOVQ xmm0, [byte rdx + 24]
	PUNPCKLDQ xmm13, xmm12
	PSUBQ xmm10, xmm9
	MOVDQA [byte r8 + 80], xmm1
	MOVQ xmm8, [byte rcx + 32]
	MOVQ xmm11, [byte rdx + 32]
	PUNPCKLDQ xmm4, xmm12
	PUNPCKLDQ xmm14, xmm12
	MOVDQA [byte r8 + 96], xmm10
	MOVQ xmm1, [byte rcx + 40]
	MOVQ xmm2, [byte rdx + 40]
	PUNPCKLDQ xmm5, xmm12
	PUNPCKLDQ xmm6, xmm12
	PSUBQ xmm13, xmm14
	ADD r8, 112
	MOVQ xmm10, [byte rcx + 48]
	MOVQ xmm9, [byte rdx + 48]
	PUNPCKLDQ xmm7, xmm12
	PUNPCKLDQ xmm3, xmm12
	PSUBQ xmm4, xmm6
	MOVDQA [r8], xmm13
	ADD rcx, 56
	ADD rdx, 56
	PUNPCKLDQ xmm8, xmm12
	PUNPCKLDQ xmm0, xmm12
	PSUBQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 14
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLDQ xmm1, xmm12
	PUNPCKLDQ xmm11, xmm12
	PSUBQ xmm7, xmm0
	MOVDQA [byte r8 + 32], xmm5
	PUNPCKLDQ xmm10, xmm12
	PUNPCKLDQ xmm2, xmm12
	PSUBQ xmm8, xmm11
	MOVDQA [byte r8 + 48], xmm7
	PUNPCKLDQ xmm9, xmm12
	PSUBQ xmm1, xmm2
	MOVDQA [byte r8 + 64], xmm8
	PSUBQ xmm10, xmm9
	MOVDQA [byte r8 + 80], xmm1
	MOVDQA [byte r8 + 96], xmm10
	ADD r8, 112
	.batch_process_finish:
	ADD r9, 14
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm12, [rsp]
	MOVAPS xmm13, [byte rsp + 16]
	MOVAPS xmm14, [byte rsp + 32]
	MOVAPS xmm6, [byte rsp + 48]
	MOVAPS xmm7, [byte rsp + 64]
	MOVAPS xmm8, [byte rsp + 80]
	MOVAPS xmm11, [byte rsp + 96]
	MOVAPS xmm10, [byte rsp + 112]
	MOVAPS xmm9, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Subtract_V32uV32u_V64u_Nehalem
_yepCore_Subtract_V32uV32u_V64u_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXDQ xmm15, [rcx]
	PMOVZXDQ xmm4, [byte rcx + 8]
	PMOVZXDQ xmm14, [rdx]
	PMOVZXDQ xmm5, [byte rcx + 16]
	PMOVZXDQ xmm0, [byte rdx + 8]
	PMOVZXDQ xmm1, [byte rcx + 24]
	PMOVZXDQ xmm3, [byte rdx + 16]
	PMOVZXDQ xmm6, [byte rcx + 32]
	PMOVZXDQ xmm12, [byte rdx + 24]
	PMOVZXDQ xmm2, [byte rcx + 40]
	PMOVZXDQ xmm9, [byte rdx + 32]
	PMOVZXDQ xmm11, [byte rcx + 48]
	PMOVZXDQ xmm8, [byte rdx + 40]
	PSUBQ xmm15, xmm14
	PMOVZXDQ xmm13, [byte rcx + 56]
	PMOVZXDQ xmm10, [byte rdx + 48]
	PSUBQ xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVZXDQ xmm7, [byte rdx + 56]
	PSUBQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXDQ xmm15, [rcx]
	ADD rdx, 64
	PSUBQ xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PMOVZXDQ xmm4, [byte rcx + 8]
	PMOVZXDQ xmm14, [rdx]
	PSUBQ xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PMOVZXDQ xmm5, [byte rcx + 16]
	PMOVZXDQ xmm0, [byte rdx + 8]
	PSUBQ xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PMOVZXDQ xmm1, [byte rcx + 24]
	PMOVZXDQ xmm3, [byte rdx + 16]
	PSUBQ xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PMOVZXDQ xmm6, [byte rcx + 32]
	PMOVZXDQ xmm12, [byte rdx + 24]
	PSUBQ xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	PMOVZXDQ xmm2, [byte rcx + 40]
	PMOVZXDQ xmm9, [byte rdx + 32]
	MOVDQA [byte r8 + 112], xmm13
	PMOVZXDQ xmm11, [byte rcx + 48]
	PMOVZXDQ xmm8, [byte rdx + 40]
	PSUBQ xmm15, xmm14
	ADD r8, 128
	PMOVZXDQ xmm13, [byte rcx + 56]
	PMOVZXDQ xmm10, [byte rdx + 48]
	PSUBQ xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVZXDQ xmm7, [byte rdx + 56]
	PSUBQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	PSUBQ xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PSUBQ xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PSUBQ xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PSUBQ xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PSUBQ xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Subtract_V32uV32u_V64u_SandyBridge
_yepCore_Subtract_V32uV32u_V64u_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ xmm15, [rcx]
	VPMOVZXDQ xmm4, [byte rcx + 8]
	VPMOVZXDQ xmm14, [rdx]
	VPMOVZXDQ xmm5, [byte rcx + 16]
	VPMOVZXDQ xmm0, [byte rdx + 8]
	VPMOVZXDQ xmm1, [byte rcx + 24]
	VPMOVZXDQ xmm3, [byte rdx + 16]
	VPMOVZXDQ xmm6, [byte rcx + 32]
	VPMOVZXDQ xmm12, [byte rdx + 24]
	VPMOVZXDQ xmm2, [byte rcx + 40]
	VPMOVZXDQ xmm9, [byte rdx + 32]
	VPMOVZXDQ xmm11, [byte rcx + 48]
	VPMOVZXDQ xmm8, [byte rdx + 40]
	VPSUBQ xmm15, xmm15, xmm14
	VPMOVZXDQ xmm13, [byte rcx + 56]
	VPMOVZXDQ xmm10, [byte rdx + 48]
	VPSUBQ xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVZXDQ xmm7, [byte rdx + 56]
	VPSUBQ xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ xmm15, [rcx]
	ADD rdx, 64
	VPSUBQ xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPMOVZXDQ xmm4, [byte rcx + 8]
	VPMOVZXDQ xmm14, [rdx]
	VPSUBQ xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMOVZXDQ xmm5, [byte rcx + 16]
	VPMOVZXDQ xmm0, [byte rdx + 8]
	VPSUBQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPMOVZXDQ xmm1, [byte rcx + 24]
	VPMOVZXDQ xmm3, [byte rdx + 16]
	VPSUBQ xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPMOVZXDQ xmm6, [byte rcx + 32]
	VPMOVZXDQ xmm12, [byte rdx + 24]
	VPSUBQ xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VPMOVZXDQ xmm2, [byte rcx + 40]
	VPMOVZXDQ xmm9, [byte rdx + 32]
	VMOVDQA [byte r8 + 112], xmm13
	VPMOVZXDQ xmm11, [byte rcx + 48]
	VPMOVZXDQ xmm8, [byte rdx + 40]
	VPSUBQ xmm15, xmm15, xmm14
	ADD r8, 128
	VPMOVZXDQ xmm13, [byte rcx + 56]
	VPMOVZXDQ xmm10, [byte rdx + 48]
	VPSUBQ xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVZXDQ xmm7, [byte rdx + 56]
	VPSUBQ xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	VPSUBQ xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPSUBQ xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPSUBQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPSUBQ xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPSUBQ xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Subtract_V32uV32u_V64u_Haswell
_yepCore_Subtract_V32uV32u_V64u_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ ymm15, [rcx]
	VPMOVZXDQ ymm4, [byte rcx + 16]
	VPMOVZXDQ ymm8, [rdx]
	VPMOVZXDQ ymm5, [byte rcx + 32]
	VPMOVZXDQ ymm10, [byte rdx + 16]
	VPMOVZXDQ ymm14, [byte rcx + 48]
	VPMOVZXDQ ymm12, [byte rdx + 32]
	VPMOVZXDQ ymm6, [byte rcx + 64]
	VPMOVZXDQ ymm13, [byte rdx + 48]
	VPMOVZXDQ ymm1, [byte rcx + 80]
	VPMOVZXDQ ymm7, [byte rdx + 64]
	VPMOVZXDQ ymm2, [byte rcx + 96]
	VPMOVZXDQ ymm3, [byte rdx + 80]
	VPSUBQ ymm15, ymm15, ymm8
	VPMOVZXDQ ymm0, [byte rcx + 112]
	VPMOVZXDQ ymm11, [byte rdx + 96]
	VPSUBQ ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVZXDQ ymm9, [byte rdx + 112]
	VPSUBQ ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ ymm15, [rcx]
	ADD rdx, 128
	VPSUBQ ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPMOVZXDQ ymm4, [byte rcx + 16]
	VPMOVZXDQ ymm8, [rdx]
	VPSUBQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPMOVZXDQ ymm5, [byte rcx + 32]
	VPMOVZXDQ ymm10, [byte rdx + 16]
	VPSUBQ ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPMOVZXDQ ymm14, [byte rcx + 48]
	VPMOVZXDQ ymm12, [byte rdx + 32]
	VPSUBQ ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPMOVZXDQ ymm6, [byte rcx + 64]
	VPMOVZXDQ ymm13, [byte rdx + 48]
	VPSUBQ ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVZXDQ ymm1, [byte rcx + 80]
	VPMOVZXDQ ymm7, [byte rdx + 64]
	VMOVDQA [dword r8 + 224], ymm0
	VPMOVZXDQ ymm2, [byte rcx + 96]
	VPMOVZXDQ ymm3, [byte rdx + 80]
	VPSUBQ ymm15, ymm15, ymm8
	ADD r8, 256
	VPMOVZXDQ ymm0, [byte rcx + 112]
	VPMOVZXDQ ymm11, [byte rdx + 96]
	VPSUBQ ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVZXDQ ymm9, [byte rdx + 112]
	VPSUBQ ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPSUBQ ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPSUBQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPSUBQ ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPSUBQ ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPSUBQ ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Subtract_V64sV64s_V64s_Nehalem
_yepCore_Subtract_V64sV64s_V64s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV rax, [rcx]
	ADD rcx, 8
	MOV r10, [rdx]
	ADD rdx, 8
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm15, [rcx]
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PSUBQ xmm15, xmm14
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PSUBQ xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PSUBQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm15, [rcx]
	ADD rdx, 128
	PSUBQ xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	PSUBQ xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	PSUBQ xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	PSUBQ xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	PSUBQ xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQA [byte r8 + 112], xmm13
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PSUBQ xmm15, xmm14
	ADD r8, 128
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PSUBQ xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PSUBQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	PSUBQ xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PSUBQ xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PSUBQ xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PSUBQ xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PSUBQ xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOV rax, [rcx]
	ADD rcx, 8
	MOV r10, [rdx]
	ADD rdx, 8
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Subtract_V64sV64s_V64s_SandyBridge
_yepCore_Subtract_V64sV64s_V64s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV rax, [rcx]
	ADD rcx, 8
	MOV r10, [rdx]
	ADD rdx, 8
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm15, [rcx]
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPSUBQ xmm15, xmm15, xmm14
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPSUBQ xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPSUBQ xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm15, [rcx]
	ADD rdx, 128
	VPSUBQ xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VPSUBQ xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VPSUBQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VPSUBQ xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VPSUBQ xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQA [byte r8 + 112], xmm13
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPSUBQ xmm15, xmm15, xmm14
	ADD r8, 128
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPSUBQ xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPSUBQ xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPSUBQ xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPSUBQ xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPSUBQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPSUBQ xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPSUBQ xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOV rax, [rcx]
	ADD rcx, 8
	MOV r10, [rdx]
	ADD rdx, 8
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Subtract_V64sV64s_V64s_Haswell
_yepCore_Subtract_V64sV64s_V64s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOV rax, [rcx]
	ADD rcx, 8
	MOV r10, [rdx]
	ADD rdx, 8
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm15, [rcx]
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPSUBQ ymm15, ymm15, ymm8
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPSUBQ ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPSUBQ ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm15, [rcx]
	ADD rdx, 256
	VPSUBQ ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VPSUBQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VPSUBQ ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VPSUBQ ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VPSUBQ ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQA [dword r8 + 224], ymm0
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPSUBQ ymm15, ymm15, ymm8
	ADD r8, 256
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPSUBQ ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPSUBQ ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 256
	VPSUBQ ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPSUBQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPSUBQ ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPSUBQ ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPSUBQ ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOV rax, [rcx]
	ADD rcx, 8
	MOV r10, [rdx]
	ADD rdx, 8
	SUB rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Subtract_V32fV32f_V32f_Nehalem
_yepCore_Subtract_V32fV32f_V32f_Nehalem:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm13
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm11
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm6
	MOVAPS [byte rsp + 80], xmm10
	MOVAPS [byte rsp + 96], xmm9
	MOVAPS [byte rsp + 112], xmm8
	MOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSS xmm13, [rcx]
	ADD rcx, 4
	MOVSS xmm14, [rdx]
	ADD rdx, 4
	SUBSS xmm13, xmm14
	MOVSS [r8], xmm13
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 28
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPS xmm13, [rcx]
	MOVUPS xmm4, [byte rcx + 16]
	MOVUPS xmm11, [rdx]
	MOVUPS xmm5, [byte rcx + 32]
	MOVUPS xmm12, [byte rdx + 16]
	MOVUPS xmm1, [byte rcx + 48]
	MOVUPS xmm0, [byte rdx + 32]
	MOVUPS xmm6, [byte rcx + 64]
	MOVUPS xmm3, [byte rdx + 48]
	MOVUPS xmm2, [byte rcx + 80]
	MOVUPS xmm10, [byte rdx + 64]
	SUBPS xmm13, xmm11
	MOVUPS xmm9, [byte rcx + 96]
	MOVUPS xmm8, [byte rdx + 80]
	SUBPS xmm4, xmm12
	MOVAPS [r8], xmm13
	ADD rcx, 112
	MOVUPS xmm7, [byte rdx + 96]
	SUBPS xmm5, xmm0
	MOVAPS [byte r8 + 16], xmm4
	SUB r9, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPS xmm13, [rcx]
	ADD rdx, 112
	SUBPS xmm1, xmm3
	MOVAPS [byte r8 + 32], xmm5
	MOVUPS xmm4, [byte rcx + 16]
	MOVUPS xmm11, [rdx]
	SUBPS xmm6, xmm10
	MOVAPS [byte r8 + 48], xmm1
	MOVUPS xmm5, [byte rcx + 32]
	MOVUPS xmm12, [byte rdx + 16]
	SUBPS xmm2, xmm8
	MOVAPS [byte r8 + 64], xmm6
	MOVUPS xmm1, [byte rcx + 48]
	MOVUPS xmm0, [byte rdx + 32]
	SUBPS xmm9, xmm7
	MOVAPS [byte r8 + 80], xmm2
	MOVUPS xmm6, [byte rcx + 64]
	MOVUPS xmm3, [byte rdx + 48]
	MOVAPS [byte r8 + 96], xmm9
	MOVUPS xmm2, [byte rcx + 80]
	MOVUPS xmm10, [byte rdx + 64]
	SUBPS xmm13, xmm11
	ADD r8, 112
	MOVUPS xmm9, [byte rcx + 96]
	MOVUPS xmm8, [byte rdx + 80]
	SUBPS xmm4, xmm12
	MOVAPS [r8], xmm13
	ADD rcx, 112
	MOVUPS xmm7, [byte rdx + 96]
	SUBPS xmm5, xmm0
	MOVAPS [byte r8 + 16], xmm4
	SUB r9, 28
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 112
	SUBPS xmm1, xmm3
	MOVAPS [byte r8 + 32], xmm5
	SUBPS xmm6, xmm10
	MOVAPS [byte r8 + 48], xmm1
	SUBPS xmm2, xmm8
	MOVAPS [byte r8 + 64], xmm6
	SUBPS xmm9, xmm7
	MOVAPS [byte r8 + 80], xmm2
	MOVAPS [byte r8 + 96], xmm9
	ADD r8, 112
	.batch_process_finish:
	ADD r9, 28
	JZ .return_ok
	.process_single:
	MOVSS xmm4, [rcx]
	ADD rcx, 4
	MOVSS xmm5, [rdx]
	ADD rdx, 4
	SUBSS xmm4, xmm5
	MOVSS [r8], xmm4
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm13, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm11, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm6, [byte rsp + 64]
	MOVAPS xmm10, [byte rsp + 80]
	MOVAPS xmm9, [byte rsp + 96]
	MOVAPS xmm8, [byte rsp + 112]
	MOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Subtract_V32fV32f_V32f_SandyBridge
_yepCore_Subtract_V32fV32f_V32f_SandyBridge:
	.ENTRY:
	SUB rsp, 152
	VMOVAPS [rsp], xmm13
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm10
	VMOVAPS [byte rsp + 112], xmm11
	VMOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	VMOVSS xmm13, [rcx]
	ADD rcx, 4
	VMOVSS xmm14, [rdx]
	ADD rdx, 4
	VSUBSS xmm13, xmm13, xmm14
	VMOVSS [r8], xmm13
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 56
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS ymm13, [rcx]
	VMOVUPS ymm4, [byte rcx + 32]
	VMOVUPS ymm0, [rdx]
	VMOVUPS ymm5, [byte rcx + 64]
	VMOVUPS ymm8, [byte rdx + 32]
	VMOVUPS ymm12, [byte rcx + 96]
	VMOVUPS ymm9, [byte rdx + 64]
	VMOVUPS ymm6, [dword rcx + 128]
	VMOVUPS ymm10, [byte rdx + 96]
	VMOVUPS ymm1, [dword rcx + 160]
	VMOVUPS ymm11, [dword rdx + 128]
	VSUBPS ymm13, ymm13, ymm0
	VMOVUPS ymm2, [dword rcx + 192]
	VMOVUPS ymm7, [dword rdx + 160]
	VSUBPS ymm4, ymm4, ymm8
	VMOVAPS [r8], ymm13
	ADD rcx, 224
	VMOVUPS ymm3, [dword rdx + 192]
	VSUBPS ymm5, ymm5, ymm9
	VMOVAPS [byte r8 + 32], ymm4
	SUB r9, 56
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS ymm13, [rcx]
	ADD rdx, 224
	VSUBPS ymm12, ymm12, ymm10
	VMOVAPS [byte r8 + 64], ymm5
	VMOVUPS ymm4, [byte rcx + 32]
	VMOVUPS ymm0, [rdx]
	VSUBPS ymm6, ymm6, ymm11
	VMOVAPS [byte r8 + 96], ymm12
	VMOVUPS ymm5, [byte rcx + 64]
	VMOVUPS ymm8, [byte rdx + 32]
	VSUBPS ymm1, ymm1, ymm7
	VMOVAPS [dword r8 + 128], ymm6
	VMOVUPS ymm12, [byte rcx + 96]
	VMOVUPS ymm9, [byte rdx + 64]
	VSUBPS ymm2, ymm2, ymm3
	VMOVAPS [dword r8 + 160], ymm1
	VMOVUPS ymm6, [dword rcx + 128]
	VMOVUPS ymm10, [byte rdx + 96]
	VMOVAPS [dword r8 + 192], ymm2
	VMOVUPS ymm1, [dword rcx + 160]
	VMOVUPS ymm11, [dword rdx + 128]
	VSUBPS ymm13, ymm13, ymm0
	ADD r8, 224
	VMOVUPS ymm2, [dword rcx + 192]
	VMOVUPS ymm7, [dword rdx + 160]
	VSUBPS ymm4, ymm4, ymm8
	VMOVAPS [r8], ymm13
	ADD rcx, 224
	VMOVUPS ymm3, [dword rdx + 192]
	VSUBPS ymm5, ymm5, ymm9
	VMOVAPS [byte r8 + 32], ymm4
	SUB r9, 56
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 224
	VSUBPS ymm12, ymm12, ymm10
	VMOVAPS [byte r8 + 64], ymm5
	VSUBPS ymm6, ymm6, ymm11
	VMOVAPS [byte r8 + 96], ymm12
	VSUBPS ymm1, ymm1, ymm7
	VMOVAPS [dword r8 + 128], ymm6
	VSUBPS ymm2, ymm2, ymm3
	VMOVAPS [dword r8 + 160], ymm1
	VMOVAPS [dword r8 + 192], ymm2
	ADD r8, 224
	.batch_process_finish:
	ADD r9, 56
	JZ .return_ok
	.process_single:
	VMOVSS xmm4, [rcx]
	ADD rcx, 4
	VMOVSS xmm5, [rdx]
	ADD rdx, 4
	VSUBSS xmm4, xmm4, xmm5
	VMOVSS [r8], xmm4
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm13, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm10, [byte rsp + 96]
	VMOVAPS xmm11, [byte rsp + 112]
	VMOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Subtract_V64fV64f_V64f_Nehalem
_yepCore_Subtract_V64fV64f_V64f_Nehalem:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm13
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm11
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm6
	MOVAPS [byte rsp + 80], xmm10
	MOVAPS [byte rsp + 96], xmm9
	MOVAPS [byte rsp + 112], xmm8
	MOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSD xmm13, [rcx]
	ADD rcx, 8
	MOVSD xmm14, [rdx]
	ADD rdx, 8
	SUBSD xmm13, xmm14
	MOVSD [r8], xmm13
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 14
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPD xmm13, [rcx]
	MOVUPD xmm4, [byte rcx + 16]
	MOVUPD xmm11, [rdx]
	MOVUPD xmm5, [byte rcx + 32]
	MOVUPD xmm12, [byte rdx + 16]
	MOVUPD xmm1, [byte rcx + 48]
	MOVUPD xmm0, [byte rdx + 32]
	MOVUPD xmm6, [byte rcx + 64]
	MOVUPD xmm3, [byte rdx + 48]
	MOVUPD xmm2, [byte rcx + 80]
	MOVUPD xmm10, [byte rdx + 64]
	SUBPD xmm13, xmm11
	MOVUPD xmm9, [byte rcx + 96]
	MOVUPD xmm8, [byte rdx + 80]
	SUBPD xmm4, xmm12
	MOVAPD [r8], xmm13
	ADD rcx, 112
	MOVUPD xmm7, [byte rdx + 96]
	SUBPD xmm5, xmm0
	MOVAPD [byte r8 + 16], xmm4
	SUB r9, 14
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPD xmm13, [rcx]
	ADD rdx, 112
	SUBPD xmm1, xmm3
	MOVAPD [byte r8 + 32], xmm5
	MOVUPD xmm4, [byte rcx + 16]
	MOVUPD xmm11, [rdx]
	SUBPD xmm6, xmm10
	MOVAPD [byte r8 + 48], xmm1
	MOVUPD xmm5, [byte rcx + 32]
	MOVUPD xmm12, [byte rdx + 16]
	SUBPD xmm2, xmm8
	MOVAPD [byte r8 + 64], xmm6
	MOVUPD xmm1, [byte rcx + 48]
	MOVUPD xmm0, [byte rdx + 32]
	SUBPD xmm9, xmm7
	MOVAPD [byte r8 + 80], xmm2
	MOVUPD xmm6, [byte rcx + 64]
	MOVUPD xmm3, [byte rdx + 48]
	MOVAPD [byte r8 + 96], xmm9
	MOVUPD xmm2, [byte rcx + 80]
	MOVUPD xmm10, [byte rdx + 64]
	SUBPD xmm13, xmm11
	ADD r8, 112
	MOVUPD xmm9, [byte rcx + 96]
	MOVUPD xmm8, [byte rdx + 80]
	SUBPD xmm4, xmm12
	MOVAPD [r8], xmm13
	ADD rcx, 112
	MOVUPD xmm7, [byte rdx + 96]
	SUBPD xmm5, xmm0
	MOVAPD [byte r8 + 16], xmm4
	SUB r9, 14
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 112
	SUBPD xmm1, xmm3
	MOVAPD [byte r8 + 32], xmm5
	SUBPD xmm6, xmm10
	MOVAPD [byte r8 + 48], xmm1
	SUBPD xmm2, xmm8
	MOVAPD [byte r8 + 64], xmm6
	SUBPD xmm9, xmm7
	MOVAPD [byte r8 + 80], xmm2
	MOVAPD [byte r8 + 96], xmm9
	ADD r8, 112
	.batch_process_finish:
	ADD r9, 14
	JZ .return_ok
	.process_single:
	MOVSD xmm4, [rcx]
	ADD rcx, 8
	MOVSD xmm5, [rdx]
	ADD rdx, 8
	SUBSD xmm4, xmm5
	MOVSD [r8], xmm4
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm13, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm11, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm6, [byte rsp + 64]
	MOVAPS xmm10, [byte rsp + 80]
	MOVAPS xmm9, [byte rsp + 96]
	MOVAPS xmm8, [byte rsp + 112]
	MOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Subtract_V64fV64f_V64f_SandyBridge
_yepCore_Subtract_V64fV64f_V64f_SandyBridge:
	.ENTRY:
	SUB rsp, 152
	VMOVAPS [rsp], xmm13
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm10
	VMOVAPS [byte rsp + 112], xmm11
	VMOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	VMOVSD xmm13, [rcx]
	ADD rcx, 8
	VMOVSD xmm14, [rdx]
	ADD rdx, 8
	VSUBSD xmm13, xmm13, xmm14
	VMOVSD [r8], xmm13
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 28
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD ymm13, [rcx]
	VMOVUPD ymm4, [byte rcx + 32]
	VMOVUPD ymm0, [rdx]
	VMOVUPD ymm5, [byte rcx + 64]
	VMOVUPD ymm8, [byte rdx + 32]
	VMOVUPD ymm12, [byte rcx + 96]
	VMOVUPD ymm9, [byte rdx + 64]
	VMOVUPD ymm6, [dword rcx + 128]
	VMOVUPD ymm10, [byte rdx + 96]
	VMOVUPD ymm1, [dword rcx + 160]
	VMOVUPD ymm11, [dword rdx + 128]
	VSUBPD ymm13, ymm13, ymm0
	VMOVUPD ymm2, [dword rcx + 192]
	VMOVUPD ymm7, [dword rdx + 160]
	VSUBPD ymm4, ymm4, ymm8
	VMOVAPD [r8], ymm13
	ADD rcx, 224
	VMOVUPD ymm3, [dword rdx + 192]
	VSUBPD ymm5, ymm5, ymm9
	VMOVAPD [byte r8 + 32], ymm4
	SUB r9, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD ymm13, [rcx]
	ADD rdx, 224
	VSUBPD ymm12, ymm12, ymm10
	VMOVAPD [byte r8 + 64], ymm5
	VMOVUPD ymm4, [byte rcx + 32]
	VMOVUPD ymm0, [rdx]
	VSUBPD ymm6, ymm6, ymm11
	VMOVAPD [byte r8 + 96], ymm12
	VMOVUPD ymm5, [byte rcx + 64]
	VMOVUPD ymm8, [byte rdx + 32]
	VSUBPD ymm1, ymm1, ymm7
	VMOVAPD [dword r8 + 128], ymm6
	VMOVUPD ymm12, [byte rcx + 96]
	VMOVUPD ymm9, [byte rdx + 64]
	VSUBPD ymm2, ymm2, ymm3
	VMOVAPD [dword r8 + 160], ymm1
	VMOVUPD ymm6, [dword rcx + 128]
	VMOVUPD ymm10, [byte rdx + 96]
	VMOVAPD [dword r8 + 192], ymm2
	VMOVUPD ymm1, [dword rcx + 160]
	VMOVUPD ymm11, [dword rdx + 128]
	VSUBPD ymm13, ymm13, ymm0
	ADD r8, 224
	VMOVUPD ymm2, [dword rcx + 192]
	VMOVUPD ymm7, [dword rdx + 160]
	VSUBPD ymm4, ymm4, ymm8
	VMOVAPD [r8], ymm13
	ADD rcx, 224
	VMOVUPD ymm3, [dword rdx + 192]
	VSUBPD ymm5, ymm5, ymm9
	VMOVAPD [byte r8 + 32], ymm4
	SUB r9, 28
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 224
	VSUBPD ymm12, ymm12, ymm10
	VMOVAPD [byte r8 + 64], ymm5
	VSUBPD ymm6, ymm6, ymm11
	VMOVAPD [byte r8 + 96], ymm12
	VSUBPD ymm1, ymm1, ymm7
	VMOVAPD [dword r8 + 128], ymm6
	VSUBPD ymm2, ymm2, ymm3
	VMOVAPD [dword r8 + 160], ymm1
	VMOVAPD [dword r8 + 192], ymm2
	ADD r8, 224
	.batch_process_finish:
	ADD r9, 28
	JZ .return_ok
	.process_single:
	VMOVSD xmm4, [rcx]
	ADD rcx, 8
	VMOVSD xmm5, [rdx]
	ADD rdx, 8
	VSUBSD xmm4, xmm4, xmm5
	VMOVSD [r8], xmm4
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm13, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm10, [byte rsp + 96]
	VMOVAPS xmm11, [byte rsp + 112]
	VMOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return
