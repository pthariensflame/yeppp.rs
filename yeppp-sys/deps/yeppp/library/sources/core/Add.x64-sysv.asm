;                       Yeppp! library implementation
;                   This file is auto-generated by Peach-Py,
;        Portable Efficient Assembly Code-generator in Higher-level Python,
;                  part of the Yeppp! library infrastructure
; This file is part of Yeppp! library and licensed under the New BSD license.
; See LICENSE.txt for the full text of the license.

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V8sV8s_V8s_Nehalem
_yepCore_Add_V8sV8s_V8s_Nehalem:
%else
section .text
global __yepCore_Add_V8sV8s_V8s_Nehalem
__yepCore_Add_V8sV8s_V8s_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	MOVSX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], al
	ADD rdx, 1
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 128
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm0, [rdi]
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm1, [rsi]
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm13, [byte rsi + 16]
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm10, [byte rsi + 32]
	MOVDQU xmm14, [byte rdi + 64]
	MOVDQU xmm3, [byte rsi + 48]
	MOVDQU xmm11, [byte rdi + 80]
	MOVDQU xmm6, [byte rsi + 64]
	MOVDQU xmm4, [byte rdi + 96]
	MOVDQU xmm7, [byte rsi + 80]
	PADDB xmm0, xmm1
	MOVDQU xmm2, [byte rdi + 112]
	MOVDQU xmm5, [byte rsi + 96]
	PADDB xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 128
	MOVDQU xmm15, [byte rsi + 112]
	PADDB xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm0, [rdi]
	ADD rsi, 128
	PADDB xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm1, [rsi]
	PADDB xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm13, [byte rsi + 16]
	PADDB xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm10, [byte rsi + 32]
	PADDB xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	MOVDQU xmm14, [byte rdi + 64]
	MOVDQU xmm3, [byte rsi + 48]
	PADDB xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	MOVDQU xmm11, [byte rdi + 80]
	MOVDQU xmm6, [byte rsi + 64]
	MOVDQA [byte rdx + 112], xmm2
	MOVDQU xmm4, [byte rdi + 96]
	MOVDQU xmm7, [byte rsi + 80]
	PADDB xmm0, xmm1
	ADD rdx, 128
	MOVDQU xmm2, [byte rdi + 112]
	MOVDQU xmm5, [byte rsi + 96]
	PADDB xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 128
	MOVDQU xmm15, [byte rsi + 112]
	PADDB xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	PADDB xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PADDB xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PADDB xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PADDB xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PADDB xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	MOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	MOVSX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], al
	ADD rdx, 1
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V8sV8s_V8s_SandyBridge
_yepCore_Add_V8sV8s_V8s_SandyBridge:
%else
section .text
global __yepCore_Add_V8sV8s_V8s_SandyBridge
__yepCore_Add_V8sV8s_V8s_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	MOVSX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], al
	ADD rdx, 1
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm0, [rdi]
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm1, [rsi]
	VMOVDQU xmm9, [byte rdi + 32]
	VMOVDQU xmm13, [byte rsi + 16]
	VMOVDQU xmm12, [byte rdi + 48]
	VMOVDQU xmm10, [byte rsi + 32]
	VMOVDQU xmm14, [byte rdi + 64]
	VMOVDQU xmm3, [byte rsi + 48]
	VMOVDQU xmm11, [byte rdi + 80]
	VMOVDQU xmm6, [byte rsi + 64]
	VMOVDQU xmm4, [byte rdi + 96]
	VMOVDQU xmm7, [byte rsi + 80]
	VPADDB xmm0, xmm0, xmm1
	VMOVDQU xmm2, [byte rdi + 112]
	VMOVDQU xmm5, [byte rsi + 96]
	VPADDB xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 128
	VMOVDQU xmm15, [byte rsi + 112]
	VPADDB xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm0, [rdi]
	ADD rsi, 128
	VPADDB xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm1, [rsi]
	VPADDB xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VMOVDQU xmm9, [byte rdi + 32]
	VMOVDQU xmm13, [byte rsi + 16]
	VPADDB xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VMOVDQU xmm12, [byte rdi + 48]
	VMOVDQU xmm10, [byte rsi + 32]
	VPADDB xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VMOVDQU xmm14, [byte rdi + 64]
	VMOVDQU xmm3, [byte rsi + 48]
	VPADDB xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQU xmm11, [byte rdi + 80]
	VMOVDQU xmm6, [byte rsi + 64]
	VMOVDQA [byte rdx + 112], xmm2
	VMOVDQU xmm4, [byte rdi + 96]
	VMOVDQU xmm7, [byte rsi + 80]
	VPADDB xmm0, xmm0, xmm1
	ADD rdx, 128
	VMOVDQU xmm2, [byte rdi + 112]
	VMOVDQU xmm5, [byte rsi + 96]
	VPADDB xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 128
	VMOVDQU xmm15, [byte rsi + 112]
	VPADDB xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	VPADDB xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPADDB xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPADDB xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPADDB xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPADDB xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	MOVSX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], al
	ADD rdx, 1
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V8sV8s_V8s_Haswell
_yepCore_Add_V8sV8s_V8s_Haswell:
%else
section .text
global __yepCore_Add_V8sV8s_V8s_Haswell
__yepCore_Add_V8sV8s_V8s_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	MOVSX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], al
	ADD rdx, 1
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 256
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm0, [rdi]
	VMOVDQU ymm8, [byte rdi + 32]
	VMOVDQU ymm7, [rsi]
	VMOVDQU ymm9, [byte rdi + 64]
	VMOVDQU ymm5, [byte rsi + 32]
	VMOVDQU ymm1, [byte rdi + 96]
	VMOVDQU ymm3, [byte rsi + 64]
	VMOVDQU ymm14, [dword rdi + 128]
	VMOVDQU ymm2, [byte rsi + 96]
	VMOVDQU ymm12, [dword rdi + 160]
	VMOVDQU ymm15, [dword rsi + 128]
	VMOVDQU ymm11, [dword rdi + 192]
	VMOVDQU ymm10, [dword rsi + 160]
	VPADDB ymm0, ymm0, ymm7
	VMOVDQU ymm13, [dword rdi + 224]
	VMOVDQU ymm4, [dword rsi + 192]
	VPADDB ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 256
	VMOVDQU ymm6, [dword rsi + 224]
	VPADDB ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 256
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm0, [rdi]
	ADD rsi, 256
	VPADDB ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VMOVDQU ymm8, [byte rdi + 32]
	VMOVDQU ymm7, [rsi]
	VPADDB ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VMOVDQU ymm9, [byte rdi + 64]
	VMOVDQU ymm5, [byte rsi + 32]
	VPADDB ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VMOVDQU ymm1, [byte rdi + 96]
	VMOVDQU ymm3, [byte rsi + 64]
	VPADDB ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VMOVDQU ymm14, [dword rdi + 128]
	VMOVDQU ymm2, [byte rsi + 96]
	VPADDB ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQU ymm12, [dword rdi + 160]
	VMOVDQU ymm15, [dword rsi + 128]
	VMOVDQA [dword rdx + 224], ymm13
	VMOVDQU ymm11, [dword rdi + 192]
	VMOVDQU ymm10, [dword rsi + 160]
	VPADDB ymm0, ymm0, ymm7
	ADD rdx, 256
	VMOVDQU ymm13, [dword rdi + 224]
	VMOVDQU ymm4, [dword rsi + 192]
	VPADDB ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 256
	VMOVDQU ymm6, [dword rsi + 224]
	VPADDB ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 256
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 256
	VPADDB ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPADDB ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPADDB ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPADDB ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPADDB ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm13
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 256
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	MOVSX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], al
	ADD rdx, 1
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.K10 progbits alloc exec nowrite align=16
global _yepCore_Add_V8sV8s_V16s_K10
_yepCore_Add_V8sV8s_V16s_K10:
%else
section .text
global __yepCore_Add_V8sV8s_V16s_K10
__yepCore_Add_V8sV8s_V16s_K10:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	MOVSX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm3, [rdi]
	MOVQ xmm2, [rsi]
	PXOR xmm1, xmm1
	PXOR xmm0, xmm0
	MOVQ xmm8, [byte rdi + 8]
	MOVQ xmm14, [byte rsi + 8]
	PXOR xmm7, xmm7
	PXOR xmm11, xmm11
	PCMPGTB xmm1, xmm3
	PCMPGTB xmm0, xmm2
	MOVQ xmm9, [byte rdi + 16]
	MOVQ xmm10, [byte rsi + 16]
	PXOR xmm12, xmm12
	PXOR xmm6, xmm6
	PCMPGTB xmm7, xmm8
	PCMPGTB xmm11, xmm14
	PUNPCKLBW xmm3, xmm1
	PUNPCKLBW xmm2, xmm0
	PADDW xmm3, xmm2
	MOVQ xmm13, [byte rdi + 24]
	MOVQ xmm5, [byte rsi + 24]
	PXOR xmm4, xmm4
	PXOR xmm15, xmm15
	PCMPGTB xmm12, xmm9
	PCMPGTB xmm6, xmm10
	PUNPCKLBW xmm8, xmm7
	PUNPCKLBW xmm14, xmm11
	PADDW xmm8, xmm14
	MOVDQA [rdx], xmm3
	ADD rdi, 32
	ADD rsi, 32
	PCMPGTB xmm4, xmm13
	PCMPGTB xmm15, xmm5
	PUNPCKLBW xmm9, xmm12
	PUNPCKLBW xmm10, xmm6
	PADDW xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm3, [rdi]
	MOVQ xmm2, [rsi]
	PXOR xmm1, xmm1
	PXOR xmm0, xmm0
	PUNPCKLBW xmm13, xmm4
	PUNPCKLBW xmm5, xmm15
	PADDW xmm13, xmm5
	MOVDQA [byte rdx + 32], xmm9
	MOVQ xmm8, [byte rdi + 8]
	MOVQ xmm14, [byte rsi + 8]
	PXOR xmm7, xmm7
	PXOR xmm11, xmm11
	PCMPGTB xmm1, xmm3
	PCMPGTB xmm0, xmm2
	MOVDQA [byte rdx + 48], xmm13
	MOVQ xmm9, [byte rdi + 16]
	MOVQ xmm10, [byte rsi + 16]
	PXOR xmm12, xmm12
	PXOR xmm6, xmm6
	PCMPGTB xmm7, xmm8
	PCMPGTB xmm11, xmm14
	PUNPCKLBW xmm3, xmm1
	PUNPCKLBW xmm2, xmm0
	PADDW xmm3, xmm2
	ADD rdx, 64
	MOVQ xmm13, [byte rdi + 24]
	MOVQ xmm5, [byte rsi + 24]
	PXOR xmm4, xmm4
	PXOR xmm15, xmm15
	PCMPGTB xmm12, xmm9
	PCMPGTB xmm6, xmm10
	PUNPCKLBW xmm8, xmm7
	PUNPCKLBW xmm14, xmm11
	PADDW xmm8, xmm14
	MOVDQA [rdx], xmm3
	ADD rdi, 32
	ADD rsi, 32
	PCMPGTB xmm4, xmm13
	PCMPGTB xmm15, xmm5
	PUNPCKLBW xmm9, xmm12
	PUNPCKLBW xmm10, xmm6
	PADDW xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLBW xmm13, xmm4
	PUNPCKLBW xmm5, xmm15
	PADDW xmm13, xmm5
	MOVDQA [byte rdx + 32], xmm9
	MOVDQA [byte rdx + 48], xmm13
	ADD rdx, 64
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	MOVSX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V8sV8s_V16s_Nehalem
_yepCore_Add_V8sV8s_V16s_Nehalem:
%else
section .text
global __yepCore_Add_V8sV8s_V16s_Nehalem
__yepCore_Add_V8sV8s_V16s_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	MOVSX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVSXBW xmm0, [rdi]
	PMOVSXBW xmm8, [byte rdi + 8]
	PMOVSXBW xmm1, [rsi]
	PMOVSXBW xmm9, [byte rdi + 16]
	PMOVSXBW xmm13, [byte rsi + 8]
	PMOVSXBW xmm12, [byte rdi + 24]
	PMOVSXBW xmm10, [byte rsi + 16]
	PMOVSXBW xmm14, [byte rdi + 32]
	PMOVSXBW xmm3, [byte rsi + 24]
	PMOVSXBW xmm11, [byte rdi + 40]
	PMOVSXBW xmm6, [byte rsi + 32]
	PMOVSXBW xmm4, [byte rdi + 48]
	PMOVSXBW xmm7, [byte rsi + 40]
	PADDW xmm0, xmm1
	PMOVSXBW xmm2, [byte rdi + 56]
	PMOVSXBW xmm5, [byte rsi + 48]
	PADDW xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 64
	PMOVSXBW xmm15, [byte rsi + 56]
	PADDW xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVSXBW xmm0, [rdi]
	ADD rsi, 64
	PADDW xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PMOVSXBW xmm8, [byte rdi + 8]
	PMOVSXBW xmm1, [rsi]
	PADDW xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PMOVSXBW xmm9, [byte rdi + 16]
	PMOVSXBW xmm13, [byte rsi + 8]
	PADDW xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PMOVSXBW xmm12, [byte rdi + 24]
	PMOVSXBW xmm10, [byte rsi + 16]
	PADDW xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PMOVSXBW xmm14, [byte rdi + 32]
	PMOVSXBW xmm3, [byte rsi + 24]
	PADDW xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	PMOVSXBW xmm11, [byte rdi + 40]
	PMOVSXBW xmm6, [byte rsi + 32]
	MOVDQA [byte rdx + 112], xmm2
	PMOVSXBW xmm4, [byte rdi + 48]
	PMOVSXBW xmm7, [byte rsi + 40]
	PADDW xmm0, xmm1
	ADD rdx, 128
	PMOVSXBW xmm2, [byte rdi + 56]
	PMOVSXBW xmm5, [byte rsi + 48]
	PADDW xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 64
	PMOVSXBW xmm15, [byte rsi + 56]
	PADDW xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 64
	PADDW xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PADDW xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PADDW xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PADDW xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PADDW xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	MOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	MOVSX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V8sV8s_V16s_SandyBridge
_yepCore_Add_V8sV8s_V16s_SandyBridge:
%else
section .text
global __yepCore_Add_V8sV8s_V16s_SandyBridge
__yepCore_Add_V8sV8s_V16s_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	MOVSX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXBW xmm0, [rdi]
	VPMOVSXBW xmm8, [byte rdi + 8]
	VPMOVSXBW xmm1, [rsi]
	VPMOVSXBW xmm9, [byte rdi + 16]
	VPMOVSXBW xmm13, [byte rsi + 8]
	VPMOVSXBW xmm12, [byte rdi + 24]
	VPMOVSXBW xmm10, [byte rsi + 16]
	VPMOVSXBW xmm14, [byte rdi + 32]
	VPMOVSXBW xmm3, [byte rsi + 24]
	VPMOVSXBW xmm11, [byte rdi + 40]
	VPMOVSXBW xmm6, [byte rsi + 32]
	VPMOVSXBW xmm4, [byte rdi + 48]
	VPMOVSXBW xmm7, [byte rsi + 40]
	VPADDW xmm0, xmm0, xmm1
	VPMOVSXBW xmm2, [byte rdi + 56]
	VPMOVSXBW xmm5, [byte rsi + 48]
	VPADDW xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 64
	VPMOVSXBW xmm15, [byte rsi + 56]
	VPADDW xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXBW xmm0, [rdi]
	ADD rsi, 64
	VPADDW xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPMOVSXBW xmm8, [byte rdi + 8]
	VPMOVSXBW xmm1, [rsi]
	VPADDW xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPMOVSXBW xmm9, [byte rdi + 16]
	VPMOVSXBW xmm13, [byte rsi + 8]
	VPADDW xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPMOVSXBW xmm12, [byte rdi + 24]
	VPMOVSXBW xmm10, [byte rsi + 16]
	VPADDW xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPMOVSXBW xmm14, [byte rdi + 32]
	VPMOVSXBW xmm3, [byte rsi + 24]
	VPADDW xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VPMOVSXBW xmm11, [byte rdi + 40]
	VPMOVSXBW xmm6, [byte rsi + 32]
	VMOVDQA [byte rdx + 112], xmm2
	VPMOVSXBW xmm4, [byte rdi + 48]
	VPMOVSXBW xmm7, [byte rsi + 40]
	VPADDW xmm0, xmm0, xmm1
	ADD rdx, 128
	VPMOVSXBW xmm2, [byte rdi + 56]
	VPMOVSXBW xmm5, [byte rsi + 48]
	VPADDW xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 64
	VPMOVSXBW xmm15, [byte rsi + 56]
	VPADDW xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 64
	VPADDW xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPADDW xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPADDW xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPADDW xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPADDW xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	MOVSX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V8sV8s_V16s_Haswell
_yepCore_Add_V8sV8s_V16s_Haswell:
%else
section .text
global __yepCore_Add_V8sV8s_V16s_Haswell
__yepCore_Add_V8sV8s_V16s_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	MOVSX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXBW ymm0, [rdi]
	VPMOVSXBW ymm8, [byte rdi + 16]
	VPMOVSXBW ymm7, [rsi]
	VPMOVSXBW ymm9, [byte rdi + 32]
	VPMOVSXBW ymm5, [byte rsi + 16]
	VPMOVSXBW ymm1, [byte rdi + 48]
	VPMOVSXBW ymm3, [byte rsi + 32]
	VPMOVSXBW ymm14, [byte rdi + 64]
	VPMOVSXBW ymm2, [byte rsi + 48]
	VPMOVSXBW ymm12, [byte rdi + 80]
	VPMOVSXBW ymm15, [byte rsi + 64]
	VPMOVSXBW ymm11, [byte rdi + 96]
	VPMOVSXBW ymm10, [byte rsi + 80]
	VPADDW ymm0, ymm0, ymm7
	VPMOVSXBW ymm13, [byte rdi + 112]
	VPMOVSXBW ymm4, [byte rsi + 96]
	VPADDW ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 128
	VPMOVSXBW ymm6, [byte rsi + 112]
	VPADDW ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXBW ymm0, [rdi]
	ADD rsi, 128
	VPADDW ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPMOVSXBW ymm8, [byte rdi + 16]
	VPMOVSXBW ymm7, [rsi]
	VPADDW ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPMOVSXBW ymm9, [byte rdi + 32]
	VPMOVSXBW ymm5, [byte rsi + 16]
	VPADDW ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPMOVSXBW ymm1, [byte rdi + 48]
	VPMOVSXBW ymm3, [byte rsi + 32]
	VPADDW ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPMOVSXBW ymm14, [byte rdi + 64]
	VPMOVSXBW ymm2, [byte rsi + 48]
	VPADDW ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VPMOVSXBW ymm12, [byte rdi + 80]
	VPMOVSXBW ymm15, [byte rsi + 64]
	VMOVDQA [dword rdx + 224], ymm13
	VPMOVSXBW ymm11, [byte rdi + 96]
	VPMOVSXBW ymm10, [byte rsi + 80]
	VPADDW ymm0, ymm0, ymm7
	ADD rdx, 256
	VPMOVSXBW ymm13, [byte rdi + 112]
	VPMOVSXBW ymm4, [byte rsi + 96]
	VPADDW ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 128
	VPMOVSXBW ymm6, [byte rsi + 112]
	VPADDW ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	VPADDW ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPADDW ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPADDW ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPADDW ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPADDW ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm13
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	MOVSX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.K10 progbits alloc exec nowrite align=16
global _yepCore_Add_V8uV8u_V16u_K10
_yepCore_Add_V8uV8u_V16u_K10:
%else
section .text
global __yepCore_Add_V8uV8u_V16u_K10
__yepCore_Add_V8uV8u_V16u_K10:
%endif
	.ENTRY:
	PXOR xmm3, xmm3
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	MOVZX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 56
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm2, [rdi]
	MOVQ xmm1, [rsi]
	MOVQ xmm8, [byte rdi + 8]
	MOVQ xmm14, [byte rsi + 8]
	MOVQ xmm9, [byte rdi + 16]
	MOVQ xmm10, [byte rsi + 16]
	MOVQ xmm15, [byte rdi + 24]
	MOVQ xmm13, [byte rsi + 24]
	PUNPCKLBW xmm2, xmm3
	MOVQ xmm7, [byte rdi + 32]
	MOVQ xmm4, [byte rsi + 32]
	PUNPCKLBW xmm8, xmm3
	PUNPCKLBW xmm1, xmm3
	MOVQ xmm12, [byte rdi + 40]
	MOVQ xmm11, [byte rsi + 40]
	PUNPCKLBW xmm9, xmm3
	PUNPCKLBW xmm14, xmm3
	PADDW xmm2, xmm1
	MOVQ xmm5, [byte rdi + 48]
	MOVQ xmm6, [byte rsi + 48]
	PUNPCKLBW xmm15, xmm3
	PUNPCKLBW xmm10, xmm3
	PADDW xmm8, xmm14
	MOVDQA [rdx], xmm2
	ADD rdi, 56
	ADD rsi, 56
	PUNPCKLBW xmm7, xmm3
	PUNPCKLBW xmm13, xmm3
	PADDW xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 56
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm2, [rdi]
	MOVQ xmm1, [rsi]
	PUNPCKLBW xmm12, xmm3
	PUNPCKLBW xmm4, xmm3
	PADDW xmm15, xmm13
	MOVDQA [byte rdx + 32], xmm9
	MOVQ xmm8, [byte rdi + 8]
	MOVQ xmm14, [byte rsi + 8]
	PUNPCKLBW xmm5, xmm3
	PUNPCKLBW xmm11, xmm3
	PADDW xmm7, xmm4
	MOVDQA [byte rdx + 48], xmm15
	MOVQ xmm9, [byte rdi + 16]
	MOVQ xmm10, [byte rsi + 16]
	PUNPCKLBW xmm6, xmm3
	PADDW xmm12, xmm11
	MOVDQA [byte rdx + 64], xmm7
	MOVQ xmm15, [byte rdi + 24]
	MOVQ xmm13, [byte rsi + 24]
	PUNPCKLBW xmm2, xmm3
	PADDW xmm5, xmm6
	MOVDQA [byte rdx + 80], xmm12
	MOVQ xmm7, [byte rdi + 32]
	MOVQ xmm4, [byte rsi + 32]
	PUNPCKLBW xmm8, xmm3
	PUNPCKLBW xmm1, xmm3
	MOVDQA [byte rdx + 96], xmm5
	MOVQ xmm12, [byte rdi + 40]
	MOVQ xmm11, [byte rsi + 40]
	PUNPCKLBW xmm9, xmm3
	PUNPCKLBW xmm14, xmm3
	PADDW xmm2, xmm1
	ADD rdx, 112
	MOVQ xmm5, [byte rdi + 48]
	MOVQ xmm6, [byte rsi + 48]
	PUNPCKLBW xmm15, xmm3
	PUNPCKLBW xmm10, xmm3
	PADDW xmm8, xmm14
	MOVDQA [rdx], xmm2
	ADD rdi, 56
	ADD rsi, 56
	PUNPCKLBW xmm7, xmm3
	PUNPCKLBW xmm13, xmm3
	PADDW xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 56
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLBW xmm12, xmm3
	PUNPCKLBW xmm4, xmm3
	PADDW xmm15, xmm13
	MOVDQA [byte rdx + 32], xmm9
	PUNPCKLBW xmm5, xmm3
	PUNPCKLBW xmm11, xmm3
	PADDW xmm7, xmm4
	MOVDQA [byte rdx + 48], xmm15
	PUNPCKLBW xmm6, xmm3
	PADDW xmm12, xmm11
	MOVDQA [byte rdx + 64], xmm7
	PADDW xmm5, xmm6
	MOVDQA [byte rdx + 80], xmm12
	MOVDQA [byte rdx + 96], xmm5
	ADD rdx, 112
	.batch_process_finish:
	ADD rcx, 56
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	MOVZX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V8uV8u_V16u_Nehalem
_yepCore_Add_V8uV8u_V16u_Nehalem:
%else
section .text
global __yepCore_Add_V8uV8u_V16u_Nehalem
__yepCore_Add_V8uV8u_V16u_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	MOVZX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXBW xmm0, [rdi]
	PMOVZXBW xmm8, [byte rdi + 8]
	PMOVZXBW xmm1, [rsi]
	PMOVZXBW xmm9, [byte rdi + 16]
	PMOVZXBW xmm13, [byte rsi + 8]
	PMOVZXBW xmm12, [byte rdi + 24]
	PMOVZXBW xmm10, [byte rsi + 16]
	PMOVZXBW xmm14, [byte rdi + 32]
	PMOVZXBW xmm3, [byte rsi + 24]
	PMOVZXBW xmm11, [byte rdi + 40]
	PMOVZXBW xmm6, [byte rsi + 32]
	PMOVZXBW xmm4, [byte rdi + 48]
	PMOVZXBW xmm7, [byte rsi + 40]
	PADDW xmm0, xmm1
	PMOVZXBW xmm2, [byte rdi + 56]
	PMOVZXBW xmm5, [byte rsi + 48]
	PADDW xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 64
	PMOVZXBW xmm15, [byte rsi + 56]
	PADDW xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXBW xmm0, [rdi]
	ADD rsi, 64
	PADDW xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PMOVZXBW xmm8, [byte rdi + 8]
	PMOVZXBW xmm1, [rsi]
	PADDW xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PMOVZXBW xmm9, [byte rdi + 16]
	PMOVZXBW xmm13, [byte rsi + 8]
	PADDW xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PMOVZXBW xmm12, [byte rdi + 24]
	PMOVZXBW xmm10, [byte rsi + 16]
	PADDW xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PMOVZXBW xmm14, [byte rdi + 32]
	PMOVZXBW xmm3, [byte rsi + 24]
	PADDW xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	PMOVZXBW xmm11, [byte rdi + 40]
	PMOVZXBW xmm6, [byte rsi + 32]
	MOVDQA [byte rdx + 112], xmm2
	PMOVZXBW xmm4, [byte rdi + 48]
	PMOVZXBW xmm7, [byte rsi + 40]
	PADDW xmm0, xmm1
	ADD rdx, 128
	PMOVZXBW xmm2, [byte rdi + 56]
	PMOVZXBW xmm5, [byte rsi + 48]
	PADDW xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 64
	PMOVZXBW xmm15, [byte rsi + 56]
	PADDW xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 64
	PADDW xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PADDW xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PADDW xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PADDW xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PADDW xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	MOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	MOVZX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V8uV8u_V16u_SandyBridge
_yepCore_Add_V8uV8u_V16u_SandyBridge:
%else
section .text
global __yepCore_Add_V8uV8u_V16u_SandyBridge
__yepCore_Add_V8uV8u_V16u_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	MOVZX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXBW xmm0, [rdi]
	VPMOVZXBW xmm8, [byte rdi + 8]
	VPMOVZXBW xmm1, [rsi]
	VPMOVZXBW xmm9, [byte rdi + 16]
	VPMOVZXBW xmm13, [byte rsi + 8]
	VPMOVZXBW xmm12, [byte rdi + 24]
	VPMOVZXBW xmm10, [byte rsi + 16]
	VPMOVZXBW xmm14, [byte rdi + 32]
	VPMOVZXBW xmm3, [byte rsi + 24]
	VPMOVZXBW xmm11, [byte rdi + 40]
	VPMOVZXBW xmm6, [byte rsi + 32]
	VPMOVZXBW xmm4, [byte rdi + 48]
	VPMOVZXBW xmm7, [byte rsi + 40]
	VPADDW xmm0, xmm0, xmm1
	VPMOVZXBW xmm2, [byte rdi + 56]
	VPMOVZXBW xmm5, [byte rsi + 48]
	VPADDW xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 64
	VPMOVZXBW xmm15, [byte rsi + 56]
	VPADDW xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXBW xmm0, [rdi]
	ADD rsi, 64
	VPADDW xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPMOVZXBW xmm8, [byte rdi + 8]
	VPMOVZXBW xmm1, [rsi]
	VPADDW xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPMOVZXBW xmm9, [byte rdi + 16]
	VPMOVZXBW xmm13, [byte rsi + 8]
	VPADDW xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPMOVZXBW xmm12, [byte rdi + 24]
	VPMOVZXBW xmm10, [byte rsi + 16]
	VPADDW xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPMOVZXBW xmm14, [byte rdi + 32]
	VPMOVZXBW xmm3, [byte rsi + 24]
	VPADDW xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VPMOVZXBW xmm11, [byte rdi + 40]
	VPMOVZXBW xmm6, [byte rsi + 32]
	VMOVDQA [byte rdx + 112], xmm2
	VPMOVZXBW xmm4, [byte rdi + 48]
	VPMOVZXBW xmm7, [byte rsi + 40]
	VPADDW xmm0, xmm0, xmm1
	ADD rdx, 128
	VPMOVZXBW xmm2, [byte rdi + 56]
	VPMOVZXBW xmm5, [byte rsi + 48]
	VPADDW xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 64
	VPMOVZXBW xmm15, [byte rsi + 56]
	VPADDW xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 64
	VPADDW xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPADDW xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPADDW xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPADDW xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPADDW xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	MOVZX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V8uV8u_V16u_Haswell
_yepCore_Add_V8uV8u_V16u_Haswell:
%else
section .text
global __yepCore_Add_V8uV8u_V16u_Haswell
__yepCore_Add_V8uV8u_V16u_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	MOVZX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXBW ymm0, [rdi]
	VPMOVZXBW ymm8, [byte rdi + 16]
	VPMOVZXBW ymm7, [rsi]
	VPMOVZXBW ymm9, [byte rdi + 32]
	VPMOVZXBW ymm5, [byte rsi + 16]
	VPMOVZXBW ymm1, [byte rdi + 48]
	VPMOVZXBW ymm3, [byte rsi + 32]
	VPMOVZXBW ymm14, [byte rdi + 64]
	VPMOVZXBW ymm2, [byte rsi + 48]
	VPMOVZXBW ymm12, [byte rdi + 80]
	VPMOVZXBW ymm15, [byte rsi + 64]
	VPMOVZXBW ymm11, [byte rdi + 96]
	VPMOVZXBW ymm10, [byte rsi + 80]
	VPADDW ymm0, ymm0, ymm7
	VPMOVZXBW ymm13, [byte rdi + 112]
	VPMOVZXBW ymm4, [byte rsi + 96]
	VPADDW ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 128
	VPMOVZXBW ymm6, [byte rsi + 112]
	VPADDW ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXBW ymm0, [rdi]
	ADD rsi, 128
	VPADDW ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPMOVZXBW ymm8, [byte rdi + 16]
	VPMOVZXBW ymm7, [rsi]
	VPADDW ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPMOVZXBW ymm9, [byte rdi + 32]
	VPMOVZXBW ymm5, [byte rsi + 16]
	VPADDW ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPMOVZXBW ymm1, [byte rdi + 48]
	VPMOVZXBW ymm3, [byte rsi + 32]
	VPADDW ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPMOVZXBW ymm14, [byte rdi + 64]
	VPMOVZXBW ymm2, [byte rsi + 48]
	VPADDW ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VPMOVZXBW ymm12, [byte rdi + 80]
	VPMOVZXBW ymm15, [byte rsi + 64]
	VMOVDQA [dword rdx + 224], ymm13
	VPMOVZXBW ymm11, [byte rdi + 96]
	VPMOVZXBW ymm10, [byte rsi + 80]
	VPADDW ymm0, ymm0, ymm7
	ADD rdx, 256
	VPMOVZXBW ymm13, [byte rdi + 112]
	VPMOVZXBW ymm4, [byte rsi + 96]
	VPADDW ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 128
	VPMOVZXBW ymm6, [byte rsi + 112]
	VPADDW ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	VPADDW ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPADDW ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPADDW ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPADDW ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPADDW ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm13
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 128
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	MOVZX r10d, byte [rsi]
	ADD rsi, 1
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V16sV16s_V16s_Nehalem
_yepCore_Add_V16sV16s_V16s_Nehalem:
%else
section .text
global __yepCore_Add_V16sV16s_V16s_Nehalem
__yepCore_Add_V16sV16s_V16s_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm0, [rdi]
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm1, [rsi]
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm13, [byte rsi + 16]
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm10, [byte rsi + 32]
	MOVDQU xmm14, [byte rdi + 64]
	MOVDQU xmm3, [byte rsi + 48]
	MOVDQU xmm11, [byte rdi + 80]
	MOVDQU xmm6, [byte rsi + 64]
	MOVDQU xmm4, [byte rdi + 96]
	MOVDQU xmm7, [byte rsi + 80]
	PADDW xmm0, xmm1
	MOVDQU xmm2, [byte rdi + 112]
	MOVDQU xmm5, [byte rsi + 96]
	PADDW xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 128
	MOVDQU xmm15, [byte rsi + 112]
	PADDW xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm0, [rdi]
	ADD rsi, 128
	PADDW xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm1, [rsi]
	PADDW xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm13, [byte rsi + 16]
	PADDW xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm10, [byte rsi + 32]
	PADDW xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	MOVDQU xmm14, [byte rdi + 64]
	MOVDQU xmm3, [byte rsi + 48]
	PADDW xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	MOVDQU xmm11, [byte rdi + 80]
	MOVDQU xmm6, [byte rsi + 64]
	MOVDQA [byte rdx + 112], xmm2
	MOVDQU xmm4, [byte rdi + 96]
	MOVDQU xmm7, [byte rsi + 80]
	PADDW xmm0, xmm1
	ADD rdx, 128
	MOVDQU xmm2, [byte rdi + 112]
	MOVDQU xmm5, [byte rsi + 96]
	PADDW xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 128
	MOVDQU xmm15, [byte rsi + 112]
	PADDW xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	PADDW xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PADDW xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PADDW xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PADDW xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PADDW xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	MOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V16sV16s_V16s_SandyBridge
_yepCore_Add_V16sV16s_V16s_SandyBridge:
%else
section .text
global __yepCore_Add_V16sV16s_V16s_SandyBridge
__yepCore_Add_V16sV16s_V16s_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm0, [rdi]
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm1, [rsi]
	VMOVDQU xmm9, [byte rdi + 32]
	VMOVDQU xmm13, [byte rsi + 16]
	VMOVDQU xmm12, [byte rdi + 48]
	VMOVDQU xmm10, [byte rsi + 32]
	VMOVDQU xmm14, [byte rdi + 64]
	VMOVDQU xmm3, [byte rsi + 48]
	VMOVDQU xmm11, [byte rdi + 80]
	VMOVDQU xmm6, [byte rsi + 64]
	VMOVDQU xmm4, [byte rdi + 96]
	VMOVDQU xmm7, [byte rsi + 80]
	VPADDW xmm0, xmm0, xmm1
	VMOVDQU xmm2, [byte rdi + 112]
	VMOVDQU xmm5, [byte rsi + 96]
	VPADDW xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 128
	VMOVDQU xmm15, [byte rsi + 112]
	VPADDW xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm0, [rdi]
	ADD rsi, 128
	VPADDW xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm1, [rsi]
	VPADDW xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VMOVDQU xmm9, [byte rdi + 32]
	VMOVDQU xmm13, [byte rsi + 16]
	VPADDW xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VMOVDQU xmm12, [byte rdi + 48]
	VMOVDQU xmm10, [byte rsi + 32]
	VPADDW xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VMOVDQU xmm14, [byte rdi + 64]
	VMOVDQU xmm3, [byte rsi + 48]
	VPADDW xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQU xmm11, [byte rdi + 80]
	VMOVDQU xmm6, [byte rsi + 64]
	VMOVDQA [byte rdx + 112], xmm2
	VMOVDQU xmm4, [byte rdi + 96]
	VMOVDQU xmm7, [byte rsi + 80]
	VPADDW xmm0, xmm0, xmm1
	ADD rdx, 128
	VMOVDQU xmm2, [byte rdi + 112]
	VMOVDQU xmm5, [byte rsi + 96]
	VPADDW xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 128
	VMOVDQU xmm15, [byte rsi + 112]
	VPADDW xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	VPADDW xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPADDW xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPADDW xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPADDW xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPADDW xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V16sV16s_V16s_Haswell
_yepCore_Add_V16sV16s_V16s_Haswell:
%else
section .text
global __yepCore_Add_V16sV16s_V16s_Haswell
__yepCore_Add_V16sV16s_V16s_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm0, [rdi]
	VMOVDQU ymm8, [byte rdi + 32]
	VMOVDQU ymm7, [rsi]
	VMOVDQU ymm9, [byte rdi + 64]
	VMOVDQU ymm5, [byte rsi + 32]
	VMOVDQU ymm1, [byte rdi + 96]
	VMOVDQU ymm3, [byte rsi + 64]
	VMOVDQU ymm14, [dword rdi + 128]
	VMOVDQU ymm2, [byte rsi + 96]
	VMOVDQU ymm12, [dword rdi + 160]
	VMOVDQU ymm15, [dword rsi + 128]
	VMOVDQU ymm11, [dword rdi + 192]
	VMOVDQU ymm10, [dword rsi + 160]
	VPADDW ymm0, ymm0, ymm7
	VMOVDQU ymm13, [dword rdi + 224]
	VMOVDQU ymm4, [dword rsi + 192]
	VPADDW ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 256
	VMOVDQU ymm6, [dword rsi + 224]
	VPADDW ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm0, [rdi]
	ADD rsi, 256
	VPADDW ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VMOVDQU ymm8, [byte rdi + 32]
	VMOVDQU ymm7, [rsi]
	VPADDW ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VMOVDQU ymm9, [byte rdi + 64]
	VMOVDQU ymm5, [byte rsi + 32]
	VPADDW ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VMOVDQU ymm1, [byte rdi + 96]
	VMOVDQU ymm3, [byte rsi + 64]
	VPADDW ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VMOVDQU ymm14, [dword rdi + 128]
	VMOVDQU ymm2, [byte rsi + 96]
	VPADDW ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQU ymm12, [dword rdi + 160]
	VMOVDQU ymm15, [dword rsi + 128]
	VMOVDQA [dword rdx + 224], ymm13
	VMOVDQU ymm11, [dword rdi + 192]
	VMOVDQU ymm10, [dword rsi + 160]
	VPADDW ymm0, ymm0, ymm7
	ADD rdx, 256
	VMOVDQU ymm13, [dword rdi + 224]
	VMOVDQU ymm4, [dword rsi + 192]
	VPADDW ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 256
	VMOVDQU ymm6, [dword rsi + 224]
	VPADDW ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 256
	VPADDW ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPADDW ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPADDW ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPADDW ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPADDW ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm13
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.K10 progbits alloc exec nowrite align=16
global _yepCore_Add_V16sV16s_V32s_K10
_yepCore_Add_V16sV16s_V32s_K10:
%else
section .text
global __yepCore_Add_V16sV16s_V32s_K10
__yepCore_Add_V16sV16s_V32s_K10:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm3, [rdi]
	MOVQ xmm2, [rsi]
	PXOR xmm1, xmm1
	PXOR xmm0, xmm0
	MOVQ xmm8, [byte rdi + 8]
	MOVQ xmm14, [byte rsi + 8]
	PXOR xmm7, xmm7
	PXOR xmm11, xmm11
	PCMPGTW xmm1, xmm3
	PCMPGTW xmm0, xmm2
	MOVQ xmm9, [byte rdi + 16]
	MOVQ xmm10, [byte rsi + 16]
	PXOR xmm12, xmm12
	PXOR xmm6, xmm6
	PCMPGTW xmm7, xmm8
	PCMPGTW xmm11, xmm14
	PUNPCKLWD xmm3, xmm1
	PUNPCKLWD xmm2, xmm0
	PADDD xmm3, xmm2
	MOVQ xmm13, [byte rdi + 24]
	MOVQ xmm5, [byte rsi + 24]
	PXOR xmm4, xmm4
	PXOR xmm15, xmm15
	PCMPGTW xmm12, xmm9
	PCMPGTW xmm6, xmm10
	PUNPCKLWD xmm8, xmm7
	PUNPCKLWD xmm14, xmm11
	PADDD xmm8, xmm14
	MOVDQA [rdx], xmm3
	ADD rdi, 32
	ADD rsi, 32
	PCMPGTW xmm4, xmm13
	PCMPGTW xmm15, xmm5
	PUNPCKLWD xmm9, xmm12
	PUNPCKLWD xmm10, xmm6
	PADDD xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm3, [rdi]
	MOVQ xmm2, [rsi]
	PXOR xmm1, xmm1
	PXOR xmm0, xmm0
	PUNPCKLWD xmm13, xmm4
	PUNPCKLWD xmm5, xmm15
	PADDD xmm13, xmm5
	MOVDQA [byte rdx + 32], xmm9
	MOVQ xmm8, [byte rdi + 8]
	MOVQ xmm14, [byte rsi + 8]
	PXOR xmm7, xmm7
	PXOR xmm11, xmm11
	PCMPGTW xmm1, xmm3
	PCMPGTW xmm0, xmm2
	MOVDQA [byte rdx + 48], xmm13
	MOVQ xmm9, [byte rdi + 16]
	MOVQ xmm10, [byte rsi + 16]
	PXOR xmm12, xmm12
	PXOR xmm6, xmm6
	PCMPGTW xmm7, xmm8
	PCMPGTW xmm11, xmm14
	PUNPCKLWD xmm3, xmm1
	PUNPCKLWD xmm2, xmm0
	PADDD xmm3, xmm2
	ADD rdx, 64
	MOVQ xmm13, [byte rdi + 24]
	MOVQ xmm5, [byte rsi + 24]
	PXOR xmm4, xmm4
	PXOR xmm15, xmm15
	PCMPGTW xmm12, xmm9
	PCMPGTW xmm6, xmm10
	PUNPCKLWD xmm8, xmm7
	PUNPCKLWD xmm14, xmm11
	PADDD xmm8, xmm14
	MOVDQA [rdx], xmm3
	ADD rdi, 32
	ADD rsi, 32
	PCMPGTW xmm4, xmm13
	PCMPGTW xmm15, xmm5
	PUNPCKLWD xmm9, xmm12
	PUNPCKLWD xmm10, xmm6
	PADDD xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLWD xmm13, xmm4
	PUNPCKLWD xmm5, xmm15
	PADDD xmm13, xmm5
	MOVDQA [byte rdx + 32], xmm9
	MOVDQA [byte rdx + 48], xmm13
	ADD rdx, 64
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V16sV16s_V32s_Nehalem
_yepCore_Add_V16sV16s_V32s_Nehalem:
%else
section .text
global __yepCore_Add_V16sV16s_V32s_Nehalem
__yepCore_Add_V16sV16s_V32s_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVSXWD xmm0, [rdi]
	PMOVSXWD xmm8, [byte rdi + 8]
	PMOVSXWD xmm1, [rsi]
	PMOVSXWD xmm9, [byte rdi + 16]
	PMOVSXWD xmm13, [byte rsi + 8]
	PMOVSXWD xmm12, [byte rdi + 24]
	PMOVSXWD xmm10, [byte rsi + 16]
	PMOVSXWD xmm14, [byte rdi + 32]
	PMOVSXWD xmm3, [byte rsi + 24]
	PMOVSXWD xmm11, [byte rdi + 40]
	PMOVSXWD xmm6, [byte rsi + 32]
	PMOVSXWD xmm4, [byte rdi + 48]
	PMOVSXWD xmm7, [byte rsi + 40]
	PADDD xmm0, xmm1
	PMOVSXWD xmm2, [byte rdi + 56]
	PMOVSXWD xmm5, [byte rsi + 48]
	PADDD xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 64
	PMOVSXWD xmm15, [byte rsi + 56]
	PADDD xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVSXWD xmm0, [rdi]
	ADD rsi, 64
	PADDD xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PMOVSXWD xmm8, [byte rdi + 8]
	PMOVSXWD xmm1, [rsi]
	PADDD xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PMOVSXWD xmm9, [byte rdi + 16]
	PMOVSXWD xmm13, [byte rsi + 8]
	PADDD xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PMOVSXWD xmm12, [byte rdi + 24]
	PMOVSXWD xmm10, [byte rsi + 16]
	PADDD xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PMOVSXWD xmm14, [byte rdi + 32]
	PMOVSXWD xmm3, [byte rsi + 24]
	PADDD xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	PMOVSXWD xmm11, [byte rdi + 40]
	PMOVSXWD xmm6, [byte rsi + 32]
	MOVDQA [byte rdx + 112], xmm2
	PMOVSXWD xmm4, [byte rdi + 48]
	PMOVSXWD xmm7, [byte rsi + 40]
	PADDD xmm0, xmm1
	ADD rdx, 128
	PMOVSXWD xmm2, [byte rdi + 56]
	PMOVSXWD xmm5, [byte rsi + 48]
	PADDD xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 64
	PMOVSXWD xmm15, [byte rsi + 56]
	PADDD xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 64
	PADDD xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PADDD xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PADDD xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PADDD xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PADDD xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	MOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V16sV16s_V32s_SandyBridge
_yepCore_Add_V16sV16s_V32s_SandyBridge:
%else
section .text
global __yepCore_Add_V16sV16s_V32s_SandyBridge
__yepCore_Add_V16sV16s_V32s_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXWD xmm0, [rdi]
	VPMOVSXWD xmm8, [byte rdi + 8]
	VPMOVSXWD xmm1, [rsi]
	VPMOVSXWD xmm9, [byte rdi + 16]
	VPMOVSXWD xmm13, [byte rsi + 8]
	VPMOVSXWD xmm12, [byte rdi + 24]
	VPMOVSXWD xmm10, [byte rsi + 16]
	VPMOVSXWD xmm14, [byte rdi + 32]
	VPMOVSXWD xmm3, [byte rsi + 24]
	VPMOVSXWD xmm11, [byte rdi + 40]
	VPMOVSXWD xmm6, [byte rsi + 32]
	VPMOVSXWD xmm4, [byte rdi + 48]
	VPMOVSXWD xmm7, [byte rsi + 40]
	VPADDD xmm0, xmm0, xmm1
	VPMOVSXWD xmm2, [byte rdi + 56]
	VPMOVSXWD xmm5, [byte rsi + 48]
	VPADDD xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 64
	VPMOVSXWD xmm15, [byte rsi + 56]
	VPADDD xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXWD xmm0, [rdi]
	ADD rsi, 64
	VPADDD xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPMOVSXWD xmm8, [byte rdi + 8]
	VPMOVSXWD xmm1, [rsi]
	VPADDD xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPMOVSXWD xmm9, [byte rdi + 16]
	VPMOVSXWD xmm13, [byte rsi + 8]
	VPADDD xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPMOVSXWD xmm12, [byte rdi + 24]
	VPMOVSXWD xmm10, [byte rsi + 16]
	VPADDD xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPMOVSXWD xmm14, [byte rdi + 32]
	VPMOVSXWD xmm3, [byte rsi + 24]
	VPADDD xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VPMOVSXWD xmm11, [byte rdi + 40]
	VPMOVSXWD xmm6, [byte rsi + 32]
	VMOVDQA [byte rdx + 112], xmm2
	VPMOVSXWD xmm4, [byte rdi + 48]
	VPMOVSXWD xmm7, [byte rsi + 40]
	VPADDD xmm0, xmm0, xmm1
	ADD rdx, 128
	VPMOVSXWD xmm2, [byte rdi + 56]
	VPMOVSXWD xmm5, [byte rsi + 48]
	VPADDD xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 64
	VPMOVSXWD xmm15, [byte rsi + 56]
	VPADDD xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 64
	VPADDD xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPADDD xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPADDD xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPADDD xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPADDD xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V16sV16s_V32s_Haswell
_yepCore_Add_V16sV16s_V32s_Haswell:
%else
section .text
global __yepCore_Add_V16sV16s_V32s_Haswell
__yepCore_Add_V16sV16s_V32s_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXWD ymm0, [rdi]
	VPMOVSXWD ymm8, [byte rdi + 16]
	VPMOVSXWD ymm7, [rsi]
	VPMOVSXWD ymm9, [byte rdi + 32]
	VPMOVSXWD ymm5, [byte rsi + 16]
	VPMOVSXWD ymm1, [byte rdi + 48]
	VPMOVSXWD ymm3, [byte rsi + 32]
	VPMOVSXWD ymm14, [byte rdi + 64]
	VPMOVSXWD ymm2, [byte rsi + 48]
	VPMOVSXWD ymm12, [byte rdi + 80]
	VPMOVSXWD ymm15, [byte rsi + 64]
	VPMOVSXWD ymm11, [byte rdi + 96]
	VPMOVSXWD ymm10, [byte rsi + 80]
	VPADDD ymm0, ymm0, ymm7
	VPMOVSXWD ymm13, [byte rdi + 112]
	VPMOVSXWD ymm4, [byte rsi + 96]
	VPADDD ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 128
	VPMOVSXWD ymm6, [byte rsi + 112]
	VPADDD ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXWD ymm0, [rdi]
	ADD rsi, 128
	VPADDD ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPMOVSXWD ymm8, [byte rdi + 16]
	VPMOVSXWD ymm7, [rsi]
	VPADDD ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPMOVSXWD ymm9, [byte rdi + 32]
	VPMOVSXWD ymm5, [byte rsi + 16]
	VPADDD ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPMOVSXWD ymm1, [byte rdi + 48]
	VPMOVSXWD ymm3, [byte rsi + 32]
	VPADDD ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPMOVSXWD ymm14, [byte rdi + 64]
	VPMOVSXWD ymm2, [byte rsi + 48]
	VPADDD ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VPMOVSXWD ymm12, [byte rdi + 80]
	VPMOVSXWD ymm15, [byte rsi + 64]
	VMOVDQA [dword rdx + 224], ymm13
	VPMOVSXWD ymm11, [byte rdi + 96]
	VPMOVSXWD ymm10, [byte rsi + 80]
	VPADDD ymm0, ymm0, ymm7
	ADD rdx, 256
	VPMOVSXWD ymm13, [byte rdi + 112]
	VPMOVSXWD ymm4, [byte rsi + 96]
	VPADDD ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 128
	VPMOVSXWD ymm6, [byte rsi + 112]
	VPADDD ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	VPADDD ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPADDD ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPADDD ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPADDD ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPADDD ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm13
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.K10 progbits alloc exec nowrite align=16
global _yepCore_Add_V16uV16u_V32u_K10
_yepCore_Add_V16uV16u_V32u_K10:
%else
section .text
global __yepCore_Add_V16uV16u_V32u_K10
__yepCore_Add_V16uV16u_V32u_K10:
%endif
	.ENTRY:
	PXOR xmm3, xmm3
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 28
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm2, [rdi]
	MOVQ xmm1, [rsi]
	MOVQ xmm8, [byte rdi + 8]
	MOVQ xmm14, [byte rsi + 8]
	MOVQ xmm9, [byte rdi + 16]
	MOVQ xmm10, [byte rsi + 16]
	MOVQ xmm15, [byte rdi + 24]
	MOVQ xmm13, [byte rsi + 24]
	PUNPCKLWD xmm2, xmm3
	MOVQ xmm7, [byte rdi + 32]
	MOVQ xmm4, [byte rsi + 32]
	PUNPCKLWD xmm8, xmm3
	PUNPCKLWD xmm1, xmm3
	MOVQ xmm12, [byte rdi + 40]
	MOVQ xmm11, [byte rsi + 40]
	PUNPCKLWD xmm9, xmm3
	PUNPCKLWD xmm14, xmm3
	PADDD xmm2, xmm1
	MOVQ xmm5, [byte rdi + 48]
	MOVQ xmm6, [byte rsi + 48]
	PUNPCKLWD xmm15, xmm3
	PUNPCKLWD xmm10, xmm3
	PADDD xmm8, xmm14
	MOVDQA [rdx], xmm2
	ADD rdi, 56
	ADD rsi, 56
	PUNPCKLWD xmm7, xmm3
	PUNPCKLWD xmm13, xmm3
	PADDD xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm2, [rdi]
	MOVQ xmm1, [rsi]
	PUNPCKLWD xmm12, xmm3
	PUNPCKLWD xmm4, xmm3
	PADDD xmm15, xmm13
	MOVDQA [byte rdx + 32], xmm9
	MOVQ xmm8, [byte rdi + 8]
	MOVQ xmm14, [byte rsi + 8]
	PUNPCKLWD xmm5, xmm3
	PUNPCKLWD xmm11, xmm3
	PADDD xmm7, xmm4
	MOVDQA [byte rdx + 48], xmm15
	MOVQ xmm9, [byte rdi + 16]
	MOVQ xmm10, [byte rsi + 16]
	PUNPCKLWD xmm6, xmm3
	PADDD xmm12, xmm11
	MOVDQA [byte rdx + 64], xmm7
	MOVQ xmm15, [byte rdi + 24]
	MOVQ xmm13, [byte rsi + 24]
	PUNPCKLWD xmm2, xmm3
	PADDD xmm5, xmm6
	MOVDQA [byte rdx + 80], xmm12
	MOVQ xmm7, [byte rdi + 32]
	MOVQ xmm4, [byte rsi + 32]
	PUNPCKLWD xmm8, xmm3
	PUNPCKLWD xmm1, xmm3
	MOVDQA [byte rdx + 96], xmm5
	MOVQ xmm12, [byte rdi + 40]
	MOVQ xmm11, [byte rsi + 40]
	PUNPCKLWD xmm9, xmm3
	PUNPCKLWD xmm14, xmm3
	PADDD xmm2, xmm1
	ADD rdx, 112
	MOVQ xmm5, [byte rdi + 48]
	MOVQ xmm6, [byte rsi + 48]
	PUNPCKLWD xmm15, xmm3
	PUNPCKLWD xmm10, xmm3
	PADDD xmm8, xmm14
	MOVDQA [rdx], xmm2
	ADD rdi, 56
	ADD rsi, 56
	PUNPCKLWD xmm7, xmm3
	PUNPCKLWD xmm13, xmm3
	PADDD xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 28
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLWD xmm12, xmm3
	PUNPCKLWD xmm4, xmm3
	PADDD xmm15, xmm13
	MOVDQA [byte rdx + 32], xmm9
	PUNPCKLWD xmm5, xmm3
	PUNPCKLWD xmm11, xmm3
	PADDD xmm7, xmm4
	MOVDQA [byte rdx + 48], xmm15
	PUNPCKLWD xmm6, xmm3
	PADDD xmm12, xmm11
	MOVDQA [byte rdx + 64], xmm7
	PADDD xmm5, xmm6
	MOVDQA [byte rdx + 80], xmm12
	MOVDQA [byte rdx + 96], xmm5
	ADD rdx, 112
	.batch_process_finish:
	ADD rcx, 28
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V16uV16u_V32u_Nehalem
_yepCore_Add_V16uV16u_V32u_Nehalem:
%else
section .text
global __yepCore_Add_V16uV16u_V32u_Nehalem
__yepCore_Add_V16uV16u_V32u_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXWD xmm0, [rdi]
	PMOVZXWD xmm8, [byte rdi + 8]
	PMOVZXWD xmm1, [rsi]
	PMOVZXWD xmm9, [byte rdi + 16]
	PMOVZXWD xmm13, [byte rsi + 8]
	PMOVZXWD xmm12, [byte rdi + 24]
	PMOVZXWD xmm10, [byte rsi + 16]
	PMOVZXWD xmm14, [byte rdi + 32]
	PMOVZXWD xmm3, [byte rsi + 24]
	PMOVZXWD xmm11, [byte rdi + 40]
	PMOVZXWD xmm6, [byte rsi + 32]
	PMOVZXWD xmm4, [byte rdi + 48]
	PMOVZXWD xmm7, [byte rsi + 40]
	PADDD xmm0, xmm1
	PMOVZXWD xmm2, [byte rdi + 56]
	PMOVZXWD xmm5, [byte rsi + 48]
	PADDD xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 64
	PMOVZXWD xmm15, [byte rsi + 56]
	PADDD xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXWD xmm0, [rdi]
	ADD rsi, 64
	PADDD xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PMOVZXWD xmm8, [byte rdi + 8]
	PMOVZXWD xmm1, [rsi]
	PADDD xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PMOVZXWD xmm9, [byte rdi + 16]
	PMOVZXWD xmm13, [byte rsi + 8]
	PADDD xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PMOVZXWD xmm12, [byte rdi + 24]
	PMOVZXWD xmm10, [byte rsi + 16]
	PADDD xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PMOVZXWD xmm14, [byte rdi + 32]
	PMOVZXWD xmm3, [byte rsi + 24]
	PADDD xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	PMOVZXWD xmm11, [byte rdi + 40]
	PMOVZXWD xmm6, [byte rsi + 32]
	MOVDQA [byte rdx + 112], xmm2
	PMOVZXWD xmm4, [byte rdi + 48]
	PMOVZXWD xmm7, [byte rsi + 40]
	PADDD xmm0, xmm1
	ADD rdx, 128
	PMOVZXWD xmm2, [byte rdi + 56]
	PMOVZXWD xmm5, [byte rsi + 48]
	PADDD xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 64
	PMOVZXWD xmm15, [byte rsi + 56]
	PADDD xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 64
	PADDD xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PADDD xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PADDD xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PADDD xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PADDD xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	MOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V16uV16u_V32u_SandyBridge
_yepCore_Add_V16uV16u_V32u_SandyBridge:
%else
section .text
global __yepCore_Add_V16uV16u_V32u_SandyBridge
__yepCore_Add_V16uV16u_V32u_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXWD xmm0, [rdi]
	VPMOVZXWD xmm8, [byte rdi + 8]
	VPMOVZXWD xmm1, [rsi]
	VPMOVZXWD xmm9, [byte rdi + 16]
	VPMOVZXWD xmm13, [byte rsi + 8]
	VPMOVZXWD xmm12, [byte rdi + 24]
	VPMOVZXWD xmm10, [byte rsi + 16]
	VPMOVZXWD xmm14, [byte rdi + 32]
	VPMOVZXWD xmm3, [byte rsi + 24]
	VPMOVZXWD xmm11, [byte rdi + 40]
	VPMOVZXWD xmm6, [byte rsi + 32]
	VPMOVZXWD xmm4, [byte rdi + 48]
	VPMOVZXWD xmm7, [byte rsi + 40]
	VPADDD xmm0, xmm0, xmm1
	VPMOVZXWD xmm2, [byte rdi + 56]
	VPMOVZXWD xmm5, [byte rsi + 48]
	VPADDD xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 64
	VPMOVZXWD xmm15, [byte rsi + 56]
	VPADDD xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXWD xmm0, [rdi]
	ADD rsi, 64
	VPADDD xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPMOVZXWD xmm8, [byte rdi + 8]
	VPMOVZXWD xmm1, [rsi]
	VPADDD xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPMOVZXWD xmm9, [byte rdi + 16]
	VPMOVZXWD xmm13, [byte rsi + 8]
	VPADDD xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPMOVZXWD xmm12, [byte rdi + 24]
	VPMOVZXWD xmm10, [byte rsi + 16]
	VPADDD xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPMOVZXWD xmm14, [byte rdi + 32]
	VPMOVZXWD xmm3, [byte rsi + 24]
	VPADDD xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VPMOVZXWD xmm11, [byte rdi + 40]
	VPMOVZXWD xmm6, [byte rsi + 32]
	VMOVDQA [byte rdx + 112], xmm2
	VPMOVZXWD xmm4, [byte rdi + 48]
	VPMOVZXWD xmm7, [byte rsi + 40]
	VPADDD xmm0, xmm0, xmm1
	ADD rdx, 128
	VPMOVZXWD xmm2, [byte rdi + 56]
	VPMOVZXWD xmm5, [byte rsi + 48]
	VPADDD xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 64
	VPMOVZXWD xmm15, [byte rsi + 56]
	VPADDD xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 64
	VPADDD xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPADDD xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPADDD xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPADDD xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPADDD xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V16uV16u_V32u_Haswell
_yepCore_Add_V16uV16u_V32u_Haswell:
%else
section .text
global __yepCore_Add_V16uV16u_V32u_Haswell
__yepCore_Add_V16uV16u_V32u_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXWD ymm0, [rdi]
	VPMOVZXWD ymm8, [byte rdi + 16]
	VPMOVZXWD ymm7, [rsi]
	VPMOVZXWD ymm9, [byte rdi + 32]
	VPMOVZXWD ymm5, [byte rsi + 16]
	VPMOVZXWD ymm1, [byte rdi + 48]
	VPMOVZXWD ymm3, [byte rsi + 32]
	VPMOVZXWD ymm14, [byte rdi + 64]
	VPMOVZXWD ymm2, [byte rsi + 48]
	VPMOVZXWD ymm12, [byte rdi + 80]
	VPMOVZXWD ymm15, [byte rsi + 64]
	VPMOVZXWD ymm11, [byte rdi + 96]
	VPMOVZXWD ymm10, [byte rsi + 80]
	VPADDD ymm0, ymm0, ymm7
	VPMOVZXWD ymm13, [byte rdi + 112]
	VPMOVZXWD ymm4, [byte rsi + 96]
	VPADDD ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 128
	VPMOVZXWD ymm6, [byte rsi + 112]
	VPADDD ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXWD ymm0, [rdi]
	ADD rsi, 128
	VPADDD ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPMOVZXWD ymm8, [byte rdi + 16]
	VPMOVZXWD ymm7, [rsi]
	VPADDD ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPMOVZXWD ymm9, [byte rdi + 32]
	VPMOVZXWD ymm5, [byte rsi + 16]
	VPADDD ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPMOVZXWD ymm1, [byte rdi + 48]
	VPMOVZXWD ymm3, [byte rsi + 32]
	VPADDD ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPMOVZXWD ymm14, [byte rdi + 64]
	VPMOVZXWD ymm2, [byte rsi + 48]
	VPADDD ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VPMOVZXWD ymm12, [byte rdi + 80]
	VPMOVZXWD ymm15, [byte rsi + 64]
	VMOVDQA [dword rdx + 224], ymm13
	VPMOVZXWD ymm11, [byte rdi + 96]
	VPMOVZXWD ymm10, [byte rsi + 80]
	VPADDD ymm0, ymm0, ymm7
	ADD rdx, 256
	VPMOVZXWD ymm13, [byte rdi + 112]
	VPMOVZXWD ymm4, [byte rsi + 96]
	VPADDD ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 128
	VPMOVZXWD ymm6, [byte rsi + 112]
	VPADDD ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	VPADDD ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPADDD ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPADDD ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPADDD ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPADDD ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm13
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V32sV32s_V32s_Nehalem
_yepCore_Add_V32sV32s_V32s_Nehalem:
%else
section .text
global __yepCore_Add_V32sV32s_V32s_Nehalem
__yepCore_Add_V32sV32s_V32s_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm0, [rdi]
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm1, [rsi]
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm13, [byte rsi + 16]
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm10, [byte rsi + 32]
	MOVDQU xmm14, [byte rdi + 64]
	MOVDQU xmm3, [byte rsi + 48]
	MOVDQU xmm11, [byte rdi + 80]
	MOVDQU xmm6, [byte rsi + 64]
	MOVDQU xmm4, [byte rdi + 96]
	MOVDQU xmm7, [byte rsi + 80]
	PADDD xmm0, xmm1
	MOVDQU xmm2, [byte rdi + 112]
	MOVDQU xmm5, [byte rsi + 96]
	PADDD xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 128
	MOVDQU xmm15, [byte rsi + 112]
	PADDD xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm0, [rdi]
	ADD rsi, 128
	PADDD xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm1, [rsi]
	PADDD xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm13, [byte rsi + 16]
	PADDD xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm10, [byte rsi + 32]
	PADDD xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	MOVDQU xmm14, [byte rdi + 64]
	MOVDQU xmm3, [byte rsi + 48]
	PADDD xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	MOVDQU xmm11, [byte rdi + 80]
	MOVDQU xmm6, [byte rsi + 64]
	MOVDQA [byte rdx + 112], xmm2
	MOVDQU xmm4, [byte rdi + 96]
	MOVDQU xmm7, [byte rsi + 80]
	PADDD xmm0, xmm1
	ADD rdx, 128
	MOVDQU xmm2, [byte rdi + 112]
	MOVDQU xmm5, [byte rsi + 96]
	PADDD xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 128
	MOVDQU xmm15, [byte rsi + 112]
	PADDD xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	PADDD xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PADDD xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PADDD xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PADDD xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PADDD xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	MOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V32sV32s_V32s_SandyBridge
_yepCore_Add_V32sV32s_V32s_SandyBridge:
%else
section .text
global __yepCore_Add_V32sV32s_V32s_SandyBridge
__yepCore_Add_V32sV32s_V32s_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm0, [rdi]
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm1, [rsi]
	VMOVDQU xmm9, [byte rdi + 32]
	VMOVDQU xmm13, [byte rsi + 16]
	VMOVDQU xmm12, [byte rdi + 48]
	VMOVDQU xmm10, [byte rsi + 32]
	VMOVDQU xmm14, [byte rdi + 64]
	VMOVDQU xmm3, [byte rsi + 48]
	VMOVDQU xmm11, [byte rdi + 80]
	VMOVDQU xmm6, [byte rsi + 64]
	VMOVDQU xmm4, [byte rdi + 96]
	VMOVDQU xmm7, [byte rsi + 80]
	VPADDD xmm0, xmm0, xmm1
	VMOVDQU xmm2, [byte rdi + 112]
	VMOVDQU xmm5, [byte rsi + 96]
	VPADDD xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 128
	VMOVDQU xmm15, [byte rsi + 112]
	VPADDD xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm0, [rdi]
	ADD rsi, 128
	VPADDD xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm1, [rsi]
	VPADDD xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VMOVDQU xmm9, [byte rdi + 32]
	VMOVDQU xmm13, [byte rsi + 16]
	VPADDD xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VMOVDQU xmm12, [byte rdi + 48]
	VMOVDQU xmm10, [byte rsi + 32]
	VPADDD xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VMOVDQU xmm14, [byte rdi + 64]
	VMOVDQU xmm3, [byte rsi + 48]
	VPADDD xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQU xmm11, [byte rdi + 80]
	VMOVDQU xmm6, [byte rsi + 64]
	VMOVDQA [byte rdx + 112], xmm2
	VMOVDQU xmm4, [byte rdi + 96]
	VMOVDQU xmm7, [byte rsi + 80]
	VPADDD xmm0, xmm0, xmm1
	ADD rdx, 128
	VMOVDQU xmm2, [byte rdi + 112]
	VMOVDQU xmm5, [byte rsi + 96]
	VPADDD xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 128
	VMOVDQU xmm15, [byte rsi + 112]
	VPADDD xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	VPADDD xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPADDD xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPADDD xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPADDD xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPADDD xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V32sV32s_V32s_Haswell
_yepCore_Add_V32sV32s_V32s_Haswell:
%else
section .text
global __yepCore_Add_V32sV32s_V32s_Haswell
__yepCore_Add_V32sV32s_V32s_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm0, [rdi]
	VMOVDQU ymm8, [byte rdi + 32]
	VMOVDQU ymm7, [rsi]
	VMOVDQU ymm9, [byte rdi + 64]
	VMOVDQU ymm5, [byte rsi + 32]
	VMOVDQU ymm1, [byte rdi + 96]
	VMOVDQU ymm3, [byte rsi + 64]
	VMOVDQU ymm14, [dword rdi + 128]
	VMOVDQU ymm2, [byte rsi + 96]
	VMOVDQU ymm12, [dword rdi + 160]
	VMOVDQU ymm15, [dword rsi + 128]
	VMOVDQU ymm11, [dword rdi + 192]
	VMOVDQU ymm10, [dword rsi + 160]
	VPADDD ymm0, ymm0, ymm7
	VMOVDQU ymm13, [dword rdi + 224]
	VMOVDQU ymm4, [dword rsi + 192]
	VPADDD ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 256
	VMOVDQU ymm6, [dword rsi + 224]
	VPADDD ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm0, [rdi]
	ADD rsi, 256
	VPADDD ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VMOVDQU ymm8, [byte rdi + 32]
	VMOVDQU ymm7, [rsi]
	VPADDD ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VMOVDQU ymm9, [byte rdi + 64]
	VMOVDQU ymm5, [byte rsi + 32]
	VPADDD ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VMOVDQU ymm1, [byte rdi + 96]
	VMOVDQU ymm3, [byte rsi + 64]
	VPADDD ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VMOVDQU ymm14, [dword rdi + 128]
	VMOVDQU ymm2, [byte rsi + 96]
	VPADDD ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQU ymm12, [dword rdi + 160]
	VMOVDQU ymm15, [dword rsi + 128]
	VMOVDQA [dword rdx + 224], ymm13
	VMOVDQU ymm11, [dword rdi + 192]
	VMOVDQU ymm10, [dword rsi + 160]
	VPADDD ymm0, ymm0, ymm7
	ADD rdx, 256
	VMOVDQU ymm13, [dword rdi + 224]
	VMOVDQU ymm4, [dword rsi + 192]
	VPADDD ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 256
	VMOVDQU ymm6, [dword rsi + 224]
	VPADDD ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 256
	VPADDD ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPADDD ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPADDD ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPADDD ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPADDD ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm13
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	ADD eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.K10 progbits alloc exec nowrite align=16
global _yepCore_Add_V32sV32s_V64s_K10
_yepCore_Add_V32sV32s_V64s_K10:
%else
section .text
global __yepCore_Add_V32sV32s_V64s_K10
__yepCore_Add_V32sV32s_V64s_K10:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	MOVSX r10, dword [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 8
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm3, [rdi]
	MOVQ xmm2, [rsi]
	PXOR xmm1, xmm1
	PXOR xmm0, xmm0
	MOVQ xmm8, [byte rdi + 8]
	MOVQ xmm14, [byte rsi + 8]
	PXOR xmm7, xmm7
	PXOR xmm11, xmm11
	PCMPGTD xmm1, xmm3
	PCMPGTD xmm0, xmm2
	MOVQ xmm9, [byte rdi + 16]
	MOVQ xmm10, [byte rsi + 16]
	PXOR xmm12, xmm12
	PXOR xmm6, xmm6
	PCMPGTD xmm7, xmm8
	PCMPGTD xmm11, xmm14
	PUNPCKLDQ xmm3, xmm1
	PUNPCKLDQ xmm2, xmm0
	PADDQ xmm3, xmm2
	MOVQ xmm13, [byte rdi + 24]
	MOVQ xmm5, [byte rsi + 24]
	PXOR xmm4, xmm4
	PXOR xmm15, xmm15
	PCMPGTD xmm12, xmm9
	PCMPGTD xmm6, xmm10
	PUNPCKLDQ xmm8, xmm7
	PUNPCKLDQ xmm14, xmm11
	PADDQ xmm8, xmm14
	MOVDQA [rdx], xmm3
	ADD rdi, 32
	ADD rsi, 32
	PCMPGTD xmm4, xmm13
	PCMPGTD xmm15, xmm5
	PUNPCKLDQ xmm9, xmm12
	PUNPCKLDQ xmm10, xmm6
	PADDQ xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 8
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm3, [rdi]
	MOVQ xmm2, [rsi]
	PXOR xmm1, xmm1
	PXOR xmm0, xmm0
	PUNPCKLDQ xmm13, xmm4
	PUNPCKLDQ xmm5, xmm15
	PADDQ xmm13, xmm5
	MOVDQA [byte rdx + 32], xmm9
	MOVQ xmm8, [byte rdi + 8]
	MOVQ xmm14, [byte rsi + 8]
	PXOR xmm7, xmm7
	PXOR xmm11, xmm11
	PCMPGTD xmm1, xmm3
	PCMPGTD xmm0, xmm2
	MOVDQA [byte rdx + 48], xmm13
	MOVQ xmm9, [byte rdi + 16]
	MOVQ xmm10, [byte rsi + 16]
	PXOR xmm12, xmm12
	PXOR xmm6, xmm6
	PCMPGTD xmm7, xmm8
	PCMPGTD xmm11, xmm14
	PUNPCKLDQ xmm3, xmm1
	PUNPCKLDQ xmm2, xmm0
	PADDQ xmm3, xmm2
	ADD rdx, 64
	MOVQ xmm13, [byte rdi + 24]
	MOVQ xmm5, [byte rsi + 24]
	PXOR xmm4, xmm4
	PXOR xmm15, xmm15
	PCMPGTD xmm12, xmm9
	PCMPGTD xmm6, xmm10
	PUNPCKLDQ xmm8, xmm7
	PUNPCKLDQ xmm14, xmm11
	PADDQ xmm8, xmm14
	MOVDQA [rdx], xmm3
	ADD rdi, 32
	ADD rsi, 32
	PCMPGTD xmm4, xmm13
	PCMPGTD xmm15, xmm5
	PUNPCKLDQ xmm9, xmm12
	PUNPCKLDQ xmm10, xmm6
	PADDQ xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 8
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLDQ xmm13, xmm4
	PUNPCKLDQ xmm5, xmm15
	PADDQ xmm13, xmm5
	MOVDQA [byte rdx + 32], xmm9
	MOVDQA [byte rdx + 48], xmm13
	ADD rdx, 64
	.batch_process_finish:
	ADD rcx, 8
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	MOVSX r10, dword [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V32sV32s_V64s_Nehalem
_yepCore_Add_V32sV32s_V64s_Nehalem:
%else
section .text
global __yepCore_Add_V32sV32s_V64s_Nehalem
__yepCore_Add_V32sV32s_V64s_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	MOVSX r10, dword [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVSXDQ xmm0, [rdi]
	PMOVSXDQ xmm8, [byte rdi + 8]
	PMOVSXDQ xmm1, [rsi]
	PMOVSXDQ xmm9, [byte rdi + 16]
	PMOVSXDQ xmm13, [byte rsi + 8]
	PMOVSXDQ xmm12, [byte rdi + 24]
	PMOVSXDQ xmm10, [byte rsi + 16]
	PMOVSXDQ xmm14, [byte rdi + 32]
	PMOVSXDQ xmm3, [byte rsi + 24]
	PMOVSXDQ xmm11, [byte rdi + 40]
	PMOVSXDQ xmm6, [byte rsi + 32]
	PMOVSXDQ xmm4, [byte rdi + 48]
	PMOVSXDQ xmm7, [byte rsi + 40]
	PADDQ xmm0, xmm1
	PMOVSXDQ xmm2, [byte rdi + 56]
	PMOVSXDQ xmm5, [byte rsi + 48]
	PADDQ xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 64
	PMOVSXDQ xmm15, [byte rsi + 56]
	PADDQ xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVSXDQ xmm0, [rdi]
	ADD rsi, 64
	PADDQ xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PMOVSXDQ xmm8, [byte rdi + 8]
	PMOVSXDQ xmm1, [rsi]
	PADDQ xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PMOVSXDQ xmm9, [byte rdi + 16]
	PMOVSXDQ xmm13, [byte rsi + 8]
	PADDQ xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PMOVSXDQ xmm12, [byte rdi + 24]
	PMOVSXDQ xmm10, [byte rsi + 16]
	PADDQ xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PMOVSXDQ xmm14, [byte rdi + 32]
	PMOVSXDQ xmm3, [byte rsi + 24]
	PADDQ xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	PMOVSXDQ xmm11, [byte rdi + 40]
	PMOVSXDQ xmm6, [byte rsi + 32]
	MOVDQA [byte rdx + 112], xmm2
	PMOVSXDQ xmm4, [byte rdi + 48]
	PMOVSXDQ xmm7, [byte rsi + 40]
	PADDQ xmm0, xmm1
	ADD rdx, 128
	PMOVSXDQ xmm2, [byte rdi + 56]
	PMOVSXDQ xmm5, [byte rsi + 48]
	PADDQ xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 64
	PMOVSXDQ xmm15, [byte rsi + 56]
	PADDQ xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 64
	PADDQ xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PADDQ xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PADDQ xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PADDQ xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PADDQ xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	MOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	MOVSX r10, dword [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V32sV32s_V64s_SandyBridge
_yepCore_Add_V32sV32s_V64s_SandyBridge:
%else
section .text
global __yepCore_Add_V32sV32s_V64s_SandyBridge
__yepCore_Add_V32sV32s_V64s_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	MOVSX r10, dword [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXDQ xmm0, [rdi]
	VPMOVSXDQ xmm8, [byte rdi + 8]
	VPMOVSXDQ xmm1, [rsi]
	VPMOVSXDQ xmm9, [byte rdi + 16]
	VPMOVSXDQ xmm13, [byte rsi + 8]
	VPMOVSXDQ xmm12, [byte rdi + 24]
	VPMOVSXDQ xmm10, [byte rsi + 16]
	VPMOVSXDQ xmm14, [byte rdi + 32]
	VPMOVSXDQ xmm3, [byte rsi + 24]
	VPMOVSXDQ xmm11, [byte rdi + 40]
	VPMOVSXDQ xmm6, [byte rsi + 32]
	VPMOVSXDQ xmm4, [byte rdi + 48]
	VPMOVSXDQ xmm7, [byte rsi + 40]
	VPADDQ xmm0, xmm0, xmm1
	VPMOVSXDQ xmm2, [byte rdi + 56]
	VPMOVSXDQ xmm5, [byte rsi + 48]
	VPADDQ xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 64
	VPMOVSXDQ xmm15, [byte rsi + 56]
	VPADDQ xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXDQ xmm0, [rdi]
	ADD rsi, 64
	VPADDQ xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPMOVSXDQ xmm8, [byte rdi + 8]
	VPMOVSXDQ xmm1, [rsi]
	VPADDQ xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPMOVSXDQ xmm9, [byte rdi + 16]
	VPMOVSXDQ xmm13, [byte rsi + 8]
	VPADDQ xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPMOVSXDQ xmm12, [byte rdi + 24]
	VPMOVSXDQ xmm10, [byte rsi + 16]
	VPADDQ xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPMOVSXDQ xmm14, [byte rdi + 32]
	VPMOVSXDQ xmm3, [byte rsi + 24]
	VPADDQ xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VPMOVSXDQ xmm11, [byte rdi + 40]
	VPMOVSXDQ xmm6, [byte rsi + 32]
	VMOVDQA [byte rdx + 112], xmm2
	VPMOVSXDQ xmm4, [byte rdi + 48]
	VPMOVSXDQ xmm7, [byte rsi + 40]
	VPADDQ xmm0, xmm0, xmm1
	ADD rdx, 128
	VPMOVSXDQ xmm2, [byte rdi + 56]
	VPMOVSXDQ xmm5, [byte rsi + 48]
	VPADDQ xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 64
	VPMOVSXDQ xmm15, [byte rsi + 56]
	VPADDQ xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 64
	VPADDQ xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPADDQ xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPADDQ xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPADDQ xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPADDQ xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	MOVSX r10, dword [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V32sV32s_V64s_Haswell
_yepCore_Add_V32sV32s_V64s_Haswell:
%else
section .text
global __yepCore_Add_V32sV32s_V64s_Haswell
__yepCore_Add_V32sV32s_V64s_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	MOVSX r10, dword [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXDQ ymm0, [rdi]
	VPMOVSXDQ ymm8, [byte rdi + 16]
	VPMOVSXDQ ymm7, [rsi]
	VPMOVSXDQ ymm9, [byte rdi + 32]
	VPMOVSXDQ ymm5, [byte rsi + 16]
	VPMOVSXDQ ymm1, [byte rdi + 48]
	VPMOVSXDQ ymm3, [byte rsi + 32]
	VPMOVSXDQ ymm14, [byte rdi + 64]
	VPMOVSXDQ ymm2, [byte rsi + 48]
	VPMOVSXDQ ymm12, [byte rdi + 80]
	VPMOVSXDQ ymm15, [byte rsi + 64]
	VPMOVSXDQ ymm11, [byte rdi + 96]
	VPMOVSXDQ ymm10, [byte rsi + 80]
	VPADDQ ymm0, ymm0, ymm7
	VPMOVSXDQ ymm13, [byte rdi + 112]
	VPMOVSXDQ ymm4, [byte rsi + 96]
	VPADDQ ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 128
	VPMOVSXDQ ymm6, [byte rsi + 112]
	VPADDQ ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXDQ ymm0, [rdi]
	ADD rsi, 128
	VPADDQ ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPMOVSXDQ ymm8, [byte rdi + 16]
	VPMOVSXDQ ymm7, [rsi]
	VPADDQ ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPMOVSXDQ ymm9, [byte rdi + 32]
	VPMOVSXDQ ymm5, [byte rsi + 16]
	VPADDQ ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPMOVSXDQ ymm1, [byte rdi + 48]
	VPMOVSXDQ ymm3, [byte rsi + 32]
	VPADDQ ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPMOVSXDQ ymm14, [byte rdi + 64]
	VPMOVSXDQ ymm2, [byte rsi + 48]
	VPADDQ ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VPMOVSXDQ ymm12, [byte rdi + 80]
	VPMOVSXDQ ymm15, [byte rsi + 64]
	VMOVDQA [dword rdx + 224], ymm13
	VPMOVSXDQ ymm11, [byte rdi + 96]
	VPMOVSXDQ ymm10, [byte rsi + 80]
	VPADDQ ymm0, ymm0, ymm7
	ADD rdx, 256
	VPMOVSXDQ ymm13, [byte rdi + 112]
	VPMOVSXDQ ymm4, [byte rsi + 96]
	VPADDQ ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 128
	VPMOVSXDQ ymm6, [byte rsi + 112]
	VPADDQ ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	VPADDQ ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPADDQ ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPADDQ ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPADDQ ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPADDQ ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm13
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	MOVSX r10, dword [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.K10 progbits alloc exec nowrite align=16
global _yepCore_Add_V32uV32u_V64u_K10
_yepCore_Add_V32uV32u_V64u_K10:
%else
section .text
global __yepCore_Add_V32uV32u_V64u_K10
__yepCore_Add_V32uV32u_V64u_K10:
%endif
	.ENTRY:
	PXOR xmm3, xmm3
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 14
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm2, [rdi]
	MOVQ xmm1, [rsi]
	MOVQ xmm8, [byte rdi + 8]
	MOVQ xmm14, [byte rsi + 8]
	MOVQ xmm9, [byte rdi + 16]
	MOVQ xmm10, [byte rsi + 16]
	MOVQ xmm15, [byte rdi + 24]
	MOVQ xmm13, [byte rsi + 24]
	PUNPCKLDQ xmm2, xmm3
	MOVQ xmm7, [byte rdi + 32]
	MOVQ xmm4, [byte rsi + 32]
	PUNPCKLDQ xmm8, xmm3
	PUNPCKLDQ xmm1, xmm3
	MOVQ xmm12, [byte rdi + 40]
	MOVQ xmm11, [byte rsi + 40]
	PUNPCKLDQ xmm9, xmm3
	PUNPCKLDQ xmm14, xmm3
	PADDQ xmm2, xmm1
	MOVQ xmm5, [byte rdi + 48]
	MOVQ xmm6, [byte rsi + 48]
	PUNPCKLDQ xmm15, xmm3
	PUNPCKLDQ xmm10, xmm3
	PADDQ xmm8, xmm14
	MOVDQA [rdx], xmm2
	ADD rdi, 56
	ADD rsi, 56
	PUNPCKLDQ xmm7, xmm3
	PUNPCKLDQ xmm13, xmm3
	PADDQ xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 14
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm2, [rdi]
	MOVQ xmm1, [rsi]
	PUNPCKLDQ xmm12, xmm3
	PUNPCKLDQ xmm4, xmm3
	PADDQ xmm15, xmm13
	MOVDQA [byte rdx + 32], xmm9
	MOVQ xmm8, [byte rdi + 8]
	MOVQ xmm14, [byte rsi + 8]
	PUNPCKLDQ xmm5, xmm3
	PUNPCKLDQ xmm11, xmm3
	PADDQ xmm7, xmm4
	MOVDQA [byte rdx + 48], xmm15
	MOVQ xmm9, [byte rdi + 16]
	MOVQ xmm10, [byte rsi + 16]
	PUNPCKLDQ xmm6, xmm3
	PADDQ xmm12, xmm11
	MOVDQA [byte rdx + 64], xmm7
	MOVQ xmm15, [byte rdi + 24]
	MOVQ xmm13, [byte rsi + 24]
	PUNPCKLDQ xmm2, xmm3
	PADDQ xmm5, xmm6
	MOVDQA [byte rdx + 80], xmm12
	MOVQ xmm7, [byte rdi + 32]
	MOVQ xmm4, [byte rsi + 32]
	PUNPCKLDQ xmm8, xmm3
	PUNPCKLDQ xmm1, xmm3
	MOVDQA [byte rdx + 96], xmm5
	MOVQ xmm12, [byte rdi + 40]
	MOVQ xmm11, [byte rsi + 40]
	PUNPCKLDQ xmm9, xmm3
	PUNPCKLDQ xmm14, xmm3
	PADDQ xmm2, xmm1
	ADD rdx, 112
	MOVQ xmm5, [byte rdi + 48]
	MOVQ xmm6, [byte rsi + 48]
	PUNPCKLDQ xmm15, xmm3
	PUNPCKLDQ xmm10, xmm3
	PADDQ xmm8, xmm14
	MOVDQA [rdx], xmm2
	ADD rdi, 56
	ADD rsi, 56
	PUNPCKLDQ xmm7, xmm3
	PUNPCKLDQ xmm13, xmm3
	PADDQ xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 14
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLDQ xmm12, xmm3
	PUNPCKLDQ xmm4, xmm3
	PADDQ xmm15, xmm13
	MOVDQA [byte rdx + 32], xmm9
	PUNPCKLDQ xmm5, xmm3
	PUNPCKLDQ xmm11, xmm3
	PADDQ xmm7, xmm4
	MOVDQA [byte rdx + 48], xmm15
	PUNPCKLDQ xmm6, xmm3
	PADDQ xmm12, xmm11
	MOVDQA [byte rdx + 64], xmm7
	PADDQ xmm5, xmm6
	MOVDQA [byte rdx + 80], xmm12
	MOVDQA [byte rdx + 96], xmm5
	ADD rdx, 112
	.batch_process_finish:
	ADD rcx, 14
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V32uV32u_V64u_Nehalem
_yepCore_Add_V32uV32u_V64u_Nehalem:
%else
section .text
global __yepCore_Add_V32uV32u_V64u_Nehalem
__yepCore_Add_V32uV32u_V64u_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXDQ xmm0, [rdi]
	PMOVZXDQ xmm8, [byte rdi + 8]
	PMOVZXDQ xmm1, [rsi]
	PMOVZXDQ xmm9, [byte rdi + 16]
	PMOVZXDQ xmm13, [byte rsi + 8]
	PMOVZXDQ xmm12, [byte rdi + 24]
	PMOVZXDQ xmm10, [byte rsi + 16]
	PMOVZXDQ xmm14, [byte rdi + 32]
	PMOVZXDQ xmm3, [byte rsi + 24]
	PMOVZXDQ xmm11, [byte rdi + 40]
	PMOVZXDQ xmm6, [byte rsi + 32]
	PMOVZXDQ xmm4, [byte rdi + 48]
	PMOVZXDQ xmm7, [byte rsi + 40]
	PADDQ xmm0, xmm1
	PMOVZXDQ xmm2, [byte rdi + 56]
	PMOVZXDQ xmm5, [byte rsi + 48]
	PADDQ xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 64
	PMOVZXDQ xmm15, [byte rsi + 56]
	PADDQ xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXDQ xmm0, [rdi]
	ADD rsi, 64
	PADDQ xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PMOVZXDQ xmm8, [byte rdi + 8]
	PMOVZXDQ xmm1, [rsi]
	PADDQ xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PMOVZXDQ xmm9, [byte rdi + 16]
	PMOVZXDQ xmm13, [byte rsi + 8]
	PADDQ xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PMOVZXDQ xmm12, [byte rdi + 24]
	PMOVZXDQ xmm10, [byte rsi + 16]
	PADDQ xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PMOVZXDQ xmm14, [byte rdi + 32]
	PMOVZXDQ xmm3, [byte rsi + 24]
	PADDQ xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	PMOVZXDQ xmm11, [byte rdi + 40]
	PMOVZXDQ xmm6, [byte rsi + 32]
	MOVDQA [byte rdx + 112], xmm2
	PMOVZXDQ xmm4, [byte rdi + 48]
	PMOVZXDQ xmm7, [byte rsi + 40]
	PADDQ xmm0, xmm1
	ADD rdx, 128
	PMOVZXDQ xmm2, [byte rdi + 56]
	PMOVZXDQ xmm5, [byte rsi + 48]
	PADDQ xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 64
	PMOVZXDQ xmm15, [byte rsi + 56]
	PADDQ xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 64
	PADDQ xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PADDQ xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PADDQ xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PADDQ xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PADDQ xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	MOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V32uV32u_V64u_SandyBridge
_yepCore_Add_V32uV32u_V64u_SandyBridge:
%else
section .text
global __yepCore_Add_V32uV32u_V64u_SandyBridge
__yepCore_Add_V32uV32u_V64u_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ xmm0, [rdi]
	VPMOVZXDQ xmm8, [byte rdi + 8]
	VPMOVZXDQ xmm1, [rsi]
	VPMOVZXDQ xmm9, [byte rdi + 16]
	VPMOVZXDQ xmm13, [byte rsi + 8]
	VPMOVZXDQ xmm12, [byte rdi + 24]
	VPMOVZXDQ xmm10, [byte rsi + 16]
	VPMOVZXDQ xmm14, [byte rdi + 32]
	VPMOVZXDQ xmm3, [byte rsi + 24]
	VPMOVZXDQ xmm11, [byte rdi + 40]
	VPMOVZXDQ xmm6, [byte rsi + 32]
	VPMOVZXDQ xmm4, [byte rdi + 48]
	VPMOVZXDQ xmm7, [byte rsi + 40]
	VPADDQ xmm0, xmm0, xmm1
	VPMOVZXDQ xmm2, [byte rdi + 56]
	VPMOVZXDQ xmm5, [byte rsi + 48]
	VPADDQ xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 64
	VPMOVZXDQ xmm15, [byte rsi + 56]
	VPADDQ xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ xmm0, [rdi]
	ADD rsi, 64
	VPADDQ xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPMOVZXDQ xmm8, [byte rdi + 8]
	VPMOVZXDQ xmm1, [rsi]
	VPADDQ xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPMOVZXDQ xmm9, [byte rdi + 16]
	VPMOVZXDQ xmm13, [byte rsi + 8]
	VPADDQ xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPMOVZXDQ xmm12, [byte rdi + 24]
	VPMOVZXDQ xmm10, [byte rsi + 16]
	VPADDQ xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPMOVZXDQ xmm14, [byte rdi + 32]
	VPMOVZXDQ xmm3, [byte rsi + 24]
	VPADDQ xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VPMOVZXDQ xmm11, [byte rdi + 40]
	VPMOVZXDQ xmm6, [byte rsi + 32]
	VMOVDQA [byte rdx + 112], xmm2
	VPMOVZXDQ xmm4, [byte rdi + 48]
	VPMOVZXDQ xmm7, [byte rsi + 40]
	VPADDQ xmm0, xmm0, xmm1
	ADD rdx, 128
	VPMOVZXDQ xmm2, [byte rdi + 56]
	VPMOVZXDQ xmm5, [byte rsi + 48]
	VPADDQ xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 64
	VPMOVZXDQ xmm15, [byte rsi + 56]
	VPADDQ xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 64
	VPADDQ xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPADDQ xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPADDQ xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPADDQ xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPADDQ xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V32uV32u_V64u_Haswell
_yepCore_Add_V32uV32u_V64u_Haswell:
%else
section .text
global __yepCore_Add_V32uV32u_V64u_Haswell
__yepCore_Add_V32uV32u_V64u_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ ymm0, [rdi]
	VPMOVZXDQ ymm8, [byte rdi + 16]
	VPMOVZXDQ ymm7, [rsi]
	VPMOVZXDQ ymm9, [byte rdi + 32]
	VPMOVZXDQ ymm5, [byte rsi + 16]
	VPMOVZXDQ ymm1, [byte rdi + 48]
	VPMOVZXDQ ymm3, [byte rsi + 32]
	VPMOVZXDQ ymm14, [byte rdi + 64]
	VPMOVZXDQ ymm2, [byte rsi + 48]
	VPMOVZXDQ ymm12, [byte rdi + 80]
	VPMOVZXDQ ymm15, [byte rsi + 64]
	VPMOVZXDQ ymm11, [byte rdi + 96]
	VPMOVZXDQ ymm10, [byte rsi + 80]
	VPADDQ ymm0, ymm0, ymm7
	VPMOVZXDQ ymm13, [byte rdi + 112]
	VPMOVZXDQ ymm4, [byte rsi + 96]
	VPADDQ ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 128
	VPMOVZXDQ ymm6, [byte rsi + 112]
	VPADDQ ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ ymm0, [rdi]
	ADD rsi, 128
	VPADDQ ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPMOVZXDQ ymm8, [byte rdi + 16]
	VPMOVZXDQ ymm7, [rsi]
	VPADDQ ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPMOVZXDQ ymm9, [byte rdi + 32]
	VPMOVZXDQ ymm5, [byte rsi + 16]
	VPADDQ ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPMOVZXDQ ymm1, [byte rdi + 48]
	VPMOVZXDQ ymm3, [byte rsi + 32]
	VPADDQ ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPMOVZXDQ ymm14, [byte rdi + 64]
	VPMOVZXDQ ymm2, [byte rsi + 48]
	VPADDQ ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VPMOVZXDQ ymm12, [byte rdi + 80]
	VPMOVZXDQ ymm15, [byte rsi + 64]
	VMOVDQA [dword rdx + 224], ymm13
	VPMOVZXDQ ymm11, [byte rdi + 96]
	VPMOVZXDQ ymm10, [byte rsi + 80]
	VPADDQ ymm0, ymm0, ymm7
	ADD rdx, 256
	VPMOVZXDQ ymm13, [byte rdi + 112]
	VPMOVZXDQ ymm4, [byte rsi + 96]
	VPADDQ ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 128
	VPMOVZXDQ ymm6, [byte rsi + 112]
	VPADDQ ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	VPADDQ ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPADDQ ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPADDQ ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPADDQ ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPADDQ ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm13
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V64sV64s_V64s_Nehalem
_yepCore_Add_V64sV64s_V64s_Nehalem:
%else
section .text
global __yepCore_Add_V64sV64s_V64s_Nehalem
__yepCore_Add_V64sV64s_V64s_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV rax, [rdi]
	ADD rdi, 8
	MOV r10, [rsi]
	ADD rsi, 8
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm0, [rdi]
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm1, [rsi]
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm13, [byte rsi + 16]
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm10, [byte rsi + 32]
	MOVDQU xmm14, [byte rdi + 64]
	MOVDQU xmm3, [byte rsi + 48]
	MOVDQU xmm11, [byte rdi + 80]
	MOVDQU xmm6, [byte rsi + 64]
	MOVDQU xmm4, [byte rdi + 96]
	MOVDQU xmm7, [byte rsi + 80]
	PADDQ xmm0, xmm1
	MOVDQU xmm2, [byte rdi + 112]
	MOVDQU xmm5, [byte rsi + 96]
	PADDQ xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 128
	MOVDQU xmm15, [byte rsi + 112]
	PADDQ xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm0, [rdi]
	ADD rsi, 128
	PADDQ xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm1, [rsi]
	PADDQ xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm13, [byte rsi + 16]
	PADDQ xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm10, [byte rsi + 32]
	PADDQ xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	MOVDQU xmm14, [byte rdi + 64]
	MOVDQU xmm3, [byte rsi + 48]
	PADDQ xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	MOVDQU xmm11, [byte rdi + 80]
	MOVDQU xmm6, [byte rsi + 64]
	MOVDQA [byte rdx + 112], xmm2
	MOVDQU xmm4, [byte rdi + 96]
	MOVDQU xmm7, [byte rsi + 80]
	PADDQ xmm0, xmm1
	ADD rdx, 128
	MOVDQU xmm2, [byte rdi + 112]
	MOVDQU xmm5, [byte rsi + 96]
	PADDQ xmm8, xmm13
	MOVDQA [rdx], xmm0
	ADD rdi, 128
	MOVDQU xmm15, [byte rsi + 112]
	PADDQ xmm9, xmm10
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	PADDQ xmm12, xmm3
	MOVDQA [byte rdx + 32], xmm9
	PADDQ xmm14, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PADDQ xmm11, xmm7
	MOVDQA [byte rdx + 64], xmm14
	PADDQ xmm4, xmm5
	MOVDQA [byte rdx + 80], xmm11
	PADDQ xmm2, xmm15
	MOVDQA [byte rdx + 96], xmm4
	MOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOV rax, [rdi]
	ADD rdi, 8
	MOV r10, [rsi]
	ADD rsi, 8
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V64sV64s_V64s_SandyBridge
_yepCore_Add_V64sV64s_V64s_SandyBridge:
%else
section .text
global __yepCore_Add_V64sV64s_V64s_SandyBridge
__yepCore_Add_V64sV64s_V64s_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV rax, [rdi]
	ADD rdi, 8
	MOV r10, [rsi]
	ADD rsi, 8
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm0, [rdi]
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm1, [rsi]
	VMOVDQU xmm9, [byte rdi + 32]
	VMOVDQU xmm13, [byte rsi + 16]
	VMOVDQU xmm12, [byte rdi + 48]
	VMOVDQU xmm10, [byte rsi + 32]
	VMOVDQU xmm14, [byte rdi + 64]
	VMOVDQU xmm3, [byte rsi + 48]
	VMOVDQU xmm11, [byte rdi + 80]
	VMOVDQU xmm6, [byte rsi + 64]
	VMOVDQU xmm4, [byte rdi + 96]
	VMOVDQU xmm7, [byte rsi + 80]
	VPADDQ xmm0, xmm0, xmm1
	VMOVDQU xmm2, [byte rdi + 112]
	VMOVDQU xmm5, [byte rsi + 96]
	VPADDQ xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 128
	VMOVDQU xmm15, [byte rsi + 112]
	VPADDQ xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm0, [rdi]
	ADD rsi, 128
	VPADDQ xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm1, [rsi]
	VPADDQ xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VMOVDQU xmm9, [byte rdi + 32]
	VMOVDQU xmm13, [byte rsi + 16]
	VPADDQ xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VMOVDQU xmm12, [byte rdi + 48]
	VMOVDQU xmm10, [byte rsi + 32]
	VPADDQ xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VMOVDQU xmm14, [byte rdi + 64]
	VMOVDQU xmm3, [byte rsi + 48]
	VPADDQ xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQU xmm11, [byte rdi + 80]
	VMOVDQU xmm6, [byte rsi + 64]
	VMOVDQA [byte rdx + 112], xmm2
	VMOVDQU xmm4, [byte rdi + 96]
	VMOVDQU xmm7, [byte rsi + 80]
	VPADDQ xmm0, xmm0, xmm1
	ADD rdx, 128
	VMOVDQU xmm2, [byte rdi + 112]
	VMOVDQU xmm5, [byte rsi + 96]
	VPADDQ xmm8, xmm8, xmm13
	VMOVDQA [rdx], xmm0
	ADD rdi, 128
	VMOVDQU xmm15, [byte rsi + 112]
	VPADDQ xmm9, xmm9, xmm10
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	VPADDQ xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 32], xmm9
	VPADDQ xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPADDQ xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 64], xmm14
	VPADDQ xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 80], xmm11
	VPADDQ xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOV rax, [rdi]
	ADD rdi, 8
	MOV r10, [rsi]
	ADD rsi, 8
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V64sV64s_V64s_Haswell
_yepCore_Add_V64sV64s_V64s_Haswell:
%else
section .text
global __yepCore_Add_V64sV64s_V64s_Haswell
__yepCore_Add_V64sV64s_V64s_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOV rax, [rdi]
	ADD rdi, 8
	MOV r10, [rsi]
	ADD rsi, 8
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm0, [rdi]
	VMOVDQU ymm8, [byte rdi + 32]
	VMOVDQU ymm7, [rsi]
	VMOVDQU ymm9, [byte rdi + 64]
	VMOVDQU ymm5, [byte rsi + 32]
	VMOVDQU ymm1, [byte rdi + 96]
	VMOVDQU ymm3, [byte rsi + 64]
	VMOVDQU ymm14, [dword rdi + 128]
	VMOVDQU ymm2, [byte rsi + 96]
	VMOVDQU ymm12, [dword rdi + 160]
	VMOVDQU ymm15, [dword rsi + 128]
	VMOVDQU ymm11, [dword rdi + 192]
	VMOVDQU ymm10, [dword rsi + 160]
	VPADDQ ymm0, ymm0, ymm7
	VMOVDQU ymm13, [dword rdi + 224]
	VMOVDQU ymm4, [dword rsi + 192]
	VPADDQ ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 256
	VMOVDQU ymm6, [dword rsi + 224]
	VPADDQ ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm0, [rdi]
	ADD rsi, 256
	VPADDQ ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VMOVDQU ymm8, [byte rdi + 32]
	VMOVDQU ymm7, [rsi]
	VPADDQ ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VMOVDQU ymm9, [byte rdi + 64]
	VMOVDQU ymm5, [byte rsi + 32]
	VPADDQ ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VMOVDQU ymm1, [byte rdi + 96]
	VMOVDQU ymm3, [byte rsi + 64]
	VPADDQ ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VMOVDQU ymm14, [dword rdi + 128]
	VMOVDQU ymm2, [byte rsi + 96]
	VPADDQ ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQU ymm12, [dword rdi + 160]
	VMOVDQU ymm15, [dword rsi + 128]
	VMOVDQA [dword rdx + 224], ymm13
	VMOVDQU ymm11, [dword rdi + 192]
	VMOVDQU ymm10, [dword rsi + 160]
	VPADDQ ymm0, ymm0, ymm7
	ADD rdx, 256
	VMOVDQU ymm13, [dword rdi + 224]
	VMOVDQU ymm4, [dword rsi + 192]
	VPADDQ ymm8, ymm8, ymm5
	VMOVDQA [rdx], ymm0
	ADD rdi, 256
	VMOVDQU ymm6, [dword rsi + 224]
	VPADDQ ymm9, ymm9, ymm3
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 256
	VPADDQ ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 64], ymm9
	VPADDQ ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 96], ymm1
	VPADDQ ymm12, ymm12, ymm10
	VMOVDQA [dword rdx + 128], ymm14
	VPADDQ ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 160], ymm12
	VPADDQ ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm13
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOV rax, [rdi]
	ADD rdi, 8
	MOV r10, [rsi]
	ADD rsi, 8
	ADD rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V32fV32f_V32f_Nehalem
_yepCore_Add_V32fV32f_V32f_Nehalem:
%else
section .text
global __yepCore_Add_V32fV32f_V32f_Nehalem
__yepCore_Add_V32fV32f_V32f_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSS xmm2, [rdi]
	ADD rdi, 4
	MOVSS xmm1, [rsi]
	ADD rsi, 4
	ADDSS xmm2, xmm1
	MOVSS [rdx], xmm2
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 28
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPS xmm2, [rdi]
	MOVUPS xmm8, [byte rdi + 16]
	MOVUPS xmm4, [rsi]
	MOVUPS xmm9, [byte rdi + 32]
	MOVUPS xmm3, [byte rsi + 16]
	MOVUPS xmm12, [byte rdi + 48]
	MOVUPS xmm13, [byte rsi + 32]
	MOVUPS xmm14, [byte rdi + 64]
	MOVUPS xmm10, [byte rsi + 48]
	MOVUPS xmm11, [byte rdi + 80]
	MOVUPS xmm5, [byte rsi + 64]
	ADDPS xmm2, xmm4
	MOVUPS xmm6, [byte rdi + 96]
	MOVUPS xmm7, [byte rsi + 80]
	ADDPS xmm8, xmm3
	MOVAPS [rdx], xmm2
	ADD rdi, 112
	MOVUPS xmm15, [byte rsi + 96]
	ADDPS xmm9, xmm13
	MOVAPS [byte rdx + 16], xmm8
	SUB rcx, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPS xmm2, [rdi]
	ADD rsi, 112
	ADDPS xmm12, xmm10
	MOVAPS [byte rdx + 32], xmm9
	MOVUPS xmm8, [byte rdi + 16]
	MOVUPS xmm4, [rsi]
	ADDPS xmm14, xmm5
	MOVAPS [byte rdx + 48], xmm12
	MOVUPS xmm9, [byte rdi + 32]
	MOVUPS xmm3, [byte rsi + 16]
	ADDPS xmm11, xmm7
	MOVAPS [byte rdx + 64], xmm14
	MOVUPS xmm12, [byte rdi + 48]
	MOVUPS xmm13, [byte rsi + 32]
	ADDPS xmm6, xmm15
	MOVAPS [byte rdx + 80], xmm11
	MOVUPS xmm14, [byte rdi + 64]
	MOVUPS xmm10, [byte rsi + 48]
	MOVAPS [byte rdx + 96], xmm6
	MOVUPS xmm11, [byte rdi + 80]
	MOVUPS xmm5, [byte rsi + 64]
	ADDPS xmm2, xmm4
	ADD rdx, 112
	MOVUPS xmm6, [byte rdi + 96]
	MOVUPS xmm7, [byte rsi + 80]
	ADDPS xmm8, xmm3
	MOVAPS [rdx], xmm2
	ADD rdi, 112
	MOVUPS xmm15, [byte rsi + 96]
	ADDPS xmm9, xmm13
	MOVAPS [byte rdx + 16], xmm8
	SUB rcx, 28
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 112
	ADDPS xmm12, xmm10
	MOVAPS [byte rdx + 32], xmm9
	ADDPS xmm14, xmm5
	MOVAPS [byte rdx + 48], xmm12
	ADDPS xmm11, xmm7
	MOVAPS [byte rdx + 64], xmm14
	ADDPS xmm6, xmm15
	MOVAPS [byte rdx + 80], xmm11
	MOVAPS [byte rdx + 96], xmm6
	ADD rdx, 112
	.batch_process_finish:
	ADD rcx, 28
	JZ .return_ok
	.process_single:
	MOVSS xmm8, [rdi]
	ADD rdi, 4
	MOVSS xmm9, [rsi]
	ADD rsi, 4
	ADDSS xmm8, xmm9
	MOVSS [rdx], xmm8
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V32fV32f_V32f_SandyBridge
_yepCore_Add_V32fV32f_V32f_SandyBridge:
%else
section .text
global __yepCore_Add_V32fV32f_V32f_SandyBridge
__yepCore_Add_V32fV32f_V32f_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	VMOVSS xmm2, [rdi]
	ADD rdi, 4
	VMOVSS xmm1, [rsi]
	ADD rsi, 4
	VADDSS xmm2, xmm2, xmm1
	VMOVSS [rdx], xmm2
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 56
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS ymm2, [rdi]
	VMOVUPS ymm8, [byte rdi + 32]
	VMOVUPS ymm13, [rsi]
	VMOVUPS ymm9, [byte rdi + 64]
	VMOVUPS ymm7, [byte rsi + 32]
	VMOVUPS ymm3, [byte rdi + 96]
	VMOVUPS ymm6, [byte rsi + 64]
	VMOVUPS ymm14, [dword rdi + 128]
	VMOVUPS ymm5, [byte rsi + 96]
	VMOVUPS ymm12, [dword rdi + 160]
	VMOVUPS ymm4, [dword rsi + 128]
	VADDPS ymm2, ymm2, ymm13
	VMOVUPS ymm11, [dword rdi + 192]
	VMOVUPS ymm15, [dword rsi + 160]
	VADDPS ymm8, ymm8, ymm7
	VMOVAPS [rdx], ymm2
	ADD rdi, 224
	VMOVUPS ymm10, [dword rsi + 192]
	VADDPS ymm9, ymm9, ymm6
	VMOVAPS [byte rdx + 32], ymm8
	SUB rcx, 56
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS ymm2, [rdi]
	ADD rsi, 224
	VADDPS ymm3, ymm3, ymm5
	VMOVAPS [byte rdx + 64], ymm9
	VMOVUPS ymm8, [byte rdi + 32]
	VMOVUPS ymm13, [rsi]
	VADDPS ymm14, ymm14, ymm4
	VMOVAPS [byte rdx + 96], ymm3
	VMOVUPS ymm9, [byte rdi + 64]
	VMOVUPS ymm7, [byte rsi + 32]
	VADDPS ymm12, ymm12, ymm15
	VMOVAPS [dword rdx + 128], ymm14
	VMOVUPS ymm3, [byte rdi + 96]
	VMOVUPS ymm6, [byte rsi + 64]
	VADDPS ymm11, ymm11, ymm10
	VMOVAPS [dword rdx + 160], ymm12
	VMOVUPS ymm14, [dword rdi + 128]
	VMOVUPS ymm5, [byte rsi + 96]
	VMOVAPS [dword rdx + 192], ymm11
	VMOVUPS ymm12, [dword rdi + 160]
	VMOVUPS ymm4, [dword rsi + 128]
	VADDPS ymm2, ymm2, ymm13
	ADD rdx, 224
	VMOVUPS ymm11, [dword rdi + 192]
	VMOVUPS ymm15, [dword rsi + 160]
	VADDPS ymm8, ymm8, ymm7
	VMOVAPS [rdx], ymm2
	ADD rdi, 224
	VMOVUPS ymm10, [dword rsi + 192]
	VADDPS ymm9, ymm9, ymm6
	VMOVAPS [byte rdx + 32], ymm8
	SUB rcx, 56
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 224
	VADDPS ymm3, ymm3, ymm5
	VMOVAPS [byte rdx + 64], ymm9
	VADDPS ymm14, ymm14, ymm4
	VMOVAPS [byte rdx + 96], ymm3
	VADDPS ymm12, ymm12, ymm15
	VMOVAPS [dword rdx + 128], ymm14
	VADDPS ymm11, ymm11, ymm10
	VMOVAPS [dword rdx + 160], ymm12
	VMOVAPS [dword rdx + 192], ymm11
	ADD rdx, 224
	.batch_process_finish:
	ADD rcx, 56
	JZ .return_ok
	.process_single:
	VMOVSS xmm8, [rdi]
	ADD rdi, 4
	VMOVSS xmm9, [rsi]
	ADD rsi, 4
	VADDSS xmm8, xmm8, xmm9
	VMOVSS [rdx], xmm8
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V64fV64f_V64f_Nehalem
_yepCore_Add_V64fV64f_V64f_Nehalem:
%else
section .text
global __yepCore_Add_V64fV64f_V64f_Nehalem
__yepCore_Add_V64fV64f_V64f_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSD xmm2, [rdi]
	ADD rdi, 8
	MOVSD xmm1, [rsi]
	ADD rsi, 8
	ADDSD xmm2, xmm1
	MOVSD [rdx], xmm2
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 14
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPD xmm2, [rdi]
	MOVUPD xmm8, [byte rdi + 16]
	MOVUPD xmm4, [rsi]
	MOVUPD xmm9, [byte rdi + 32]
	MOVUPD xmm3, [byte rsi + 16]
	MOVUPD xmm12, [byte rdi + 48]
	MOVUPD xmm13, [byte rsi + 32]
	MOVUPD xmm14, [byte rdi + 64]
	MOVUPD xmm10, [byte rsi + 48]
	MOVUPD xmm11, [byte rdi + 80]
	MOVUPD xmm5, [byte rsi + 64]
	ADDPD xmm2, xmm4
	MOVUPD xmm6, [byte rdi + 96]
	MOVUPD xmm7, [byte rsi + 80]
	ADDPD xmm8, xmm3
	MOVAPD [rdx], xmm2
	ADD rdi, 112
	MOVUPD xmm15, [byte rsi + 96]
	ADDPD xmm9, xmm13
	MOVAPD [byte rdx + 16], xmm8
	SUB rcx, 14
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPD xmm2, [rdi]
	ADD rsi, 112
	ADDPD xmm12, xmm10
	MOVAPD [byte rdx + 32], xmm9
	MOVUPD xmm8, [byte rdi + 16]
	MOVUPD xmm4, [rsi]
	ADDPD xmm14, xmm5
	MOVAPD [byte rdx + 48], xmm12
	MOVUPD xmm9, [byte rdi + 32]
	MOVUPD xmm3, [byte rsi + 16]
	ADDPD xmm11, xmm7
	MOVAPD [byte rdx + 64], xmm14
	MOVUPD xmm12, [byte rdi + 48]
	MOVUPD xmm13, [byte rsi + 32]
	ADDPD xmm6, xmm15
	MOVAPD [byte rdx + 80], xmm11
	MOVUPD xmm14, [byte rdi + 64]
	MOVUPD xmm10, [byte rsi + 48]
	MOVAPD [byte rdx + 96], xmm6
	MOVUPD xmm11, [byte rdi + 80]
	MOVUPD xmm5, [byte rsi + 64]
	ADDPD xmm2, xmm4
	ADD rdx, 112
	MOVUPD xmm6, [byte rdi + 96]
	MOVUPD xmm7, [byte rsi + 80]
	ADDPD xmm8, xmm3
	MOVAPD [rdx], xmm2
	ADD rdi, 112
	MOVUPD xmm15, [byte rsi + 96]
	ADDPD xmm9, xmm13
	MOVAPD [byte rdx + 16], xmm8
	SUB rcx, 14
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 112
	ADDPD xmm12, xmm10
	MOVAPD [byte rdx + 32], xmm9
	ADDPD xmm14, xmm5
	MOVAPD [byte rdx + 48], xmm12
	ADDPD xmm11, xmm7
	MOVAPD [byte rdx + 64], xmm14
	ADDPD xmm6, xmm15
	MOVAPD [byte rdx + 80], xmm11
	MOVAPD [byte rdx + 96], xmm6
	ADD rdx, 112
	.batch_process_finish:
	ADD rcx, 14
	JZ .return_ok
	.process_single:
	MOVSD xmm8, [rdi]
	ADD rdi, 8
	MOVSD xmm9, [rsi]
	ADD rsi, 8
	ADDSD xmm8, xmm9
	MOVSD [rdx], xmm8
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V64fV64f_V64f_SandyBridge
_yepCore_Add_V64fV64f_V64f_SandyBridge:
%else
section .text
global __yepCore_Add_V64fV64f_V64f_SandyBridge
__yepCore_Add_V64fV64f_V64f_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	VMOVSD xmm2, [rdi]
	ADD rdi, 8
	VMOVSD xmm1, [rsi]
	ADD rsi, 8
	VADDSD xmm2, xmm2, xmm1
	VMOVSD [rdx], xmm2
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 28
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD ymm2, [rdi]
	VMOVUPD ymm8, [byte rdi + 32]
	VMOVUPD ymm13, [rsi]
	VMOVUPD ymm9, [byte rdi + 64]
	VMOVUPD ymm7, [byte rsi + 32]
	VMOVUPD ymm3, [byte rdi + 96]
	VMOVUPD ymm6, [byte rsi + 64]
	VMOVUPD ymm14, [dword rdi + 128]
	VMOVUPD ymm5, [byte rsi + 96]
	VMOVUPD ymm12, [dword rdi + 160]
	VMOVUPD ymm4, [dword rsi + 128]
	VADDPD ymm2, ymm2, ymm13
	VMOVUPD ymm11, [dword rdi + 192]
	VMOVUPD ymm15, [dword rsi + 160]
	VADDPD ymm8, ymm8, ymm7
	VMOVAPD [rdx], ymm2
	ADD rdi, 224
	VMOVUPD ymm10, [dword rsi + 192]
	VADDPD ymm9, ymm9, ymm6
	VMOVAPD [byte rdx + 32], ymm8
	SUB rcx, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD ymm2, [rdi]
	ADD rsi, 224
	VADDPD ymm3, ymm3, ymm5
	VMOVAPD [byte rdx + 64], ymm9
	VMOVUPD ymm8, [byte rdi + 32]
	VMOVUPD ymm13, [rsi]
	VADDPD ymm14, ymm14, ymm4
	VMOVAPD [byte rdx + 96], ymm3
	VMOVUPD ymm9, [byte rdi + 64]
	VMOVUPD ymm7, [byte rsi + 32]
	VADDPD ymm12, ymm12, ymm15
	VMOVAPD [dword rdx + 128], ymm14
	VMOVUPD ymm3, [byte rdi + 96]
	VMOVUPD ymm6, [byte rsi + 64]
	VADDPD ymm11, ymm11, ymm10
	VMOVAPD [dword rdx + 160], ymm12
	VMOVUPD ymm14, [dword rdi + 128]
	VMOVUPD ymm5, [byte rsi + 96]
	VMOVAPD [dword rdx + 192], ymm11
	VMOVUPD ymm12, [dword rdi + 160]
	VMOVUPD ymm4, [dword rsi + 128]
	VADDPD ymm2, ymm2, ymm13
	ADD rdx, 224
	VMOVUPD ymm11, [dword rdi + 192]
	VMOVUPD ymm15, [dword rsi + 160]
	VADDPD ymm8, ymm8, ymm7
	VMOVAPD [rdx], ymm2
	ADD rdi, 224
	VMOVUPD ymm10, [dword rsi + 192]
	VADDPD ymm9, ymm9, ymm6
	VMOVAPD [byte rdx + 32], ymm8
	SUB rcx, 28
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 224
	VADDPD ymm3, ymm3, ymm5
	VMOVAPD [byte rdx + 64], ymm9
	VADDPD ymm14, ymm14, ymm4
	VMOVAPD [byte rdx + 96], ymm3
	VADDPD ymm12, ymm12, ymm15
	VMOVAPD [dword rdx + 128], ymm14
	VADDPD ymm11, ymm11, ymm10
	VMOVAPD [dword rdx + 160], ymm12
	VMOVAPD [dword rdx + 192], ymm11
	ADD rdx, 224
	.batch_process_finish:
	ADD rcx, 28
	JZ .return_ok
	.process_single:
	VMOVSD xmm8, [rdi]
	ADD rdi, 8
	VMOVSD xmm9, [rsi]
	ADD rsi, 8
	VADDSD xmm8, xmm8, xmm9
	VMOVSD [rdx], xmm8
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.K10 progbits alloc exec nowrite align=16
global _yepCore_Add_V8sS8s_V16s_K10
_yepCore_Add_V8sS8s_V16s_K10:
%else
section .text
global __yepCore_Add_V8sS8s_V16s_K10
__yepCore_Add_V8sS8s_V16s_K10:
%endif
	.ENTRY:
	MOVSX ax, sil
	MOVZX eax, ax
	IMUL eax, eax, 65537
	MOVD xmm5, eax
	PSHUFD xmm5, xmm5, 0
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 48
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm4, [rdi]
	PXOR xmm3, xmm3
	MOVQ xmm8, [byte rdi + 8]
	PXOR xmm7, xmm7
	MOVQ xmm11, [byte rdi + 16]
	PXOR xmm6, xmm6
	PCMPGTB xmm3, xmm4
	MOVQ xmm9, [byte rdi + 24]
	PXOR xmm13, xmm13
	PCMPGTB xmm7, xmm8
	PUNPCKLBW xmm4, xmm3
	MOVQ xmm14, [byte rdi + 32]
	PXOR xmm10, xmm10
	PCMPGTB xmm6, xmm11
	PUNPCKLBW xmm8, xmm7
	PADDW xmm4, xmm5
	MOVQ xmm15, [byte rdi + 40]
	PXOR xmm12, xmm12
	PCMPGTB xmm13, xmm9
	PUNPCKLBW xmm11, xmm6
	PADDW xmm8, xmm5
	MOVDQA [rdx], xmm4
	ADD rdi, 48
	PCMPGTB xmm10, xmm14
	PUNPCKLBW xmm9, xmm13
	PADDW xmm11, xmm5
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 48
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm4, [rdi]
	PXOR xmm3, xmm3
	PCMPGTB xmm12, xmm15
	PUNPCKLBW xmm14, xmm10
	PADDW xmm9, xmm5
	MOVDQA [byte rdx + 32], xmm11
	MOVQ xmm8, [byte rdi + 8]
	PXOR xmm7, xmm7
	PUNPCKLBW xmm15, xmm12
	PADDW xmm14, xmm5
	MOVDQA [byte rdx + 48], xmm9
	MOVQ xmm11, [byte rdi + 16]
	PXOR xmm6, xmm6
	PCMPGTB xmm3, xmm4
	PADDW xmm15, xmm5
	MOVDQA [byte rdx + 64], xmm14
	MOVQ xmm9, [byte rdi + 24]
	PXOR xmm13, xmm13
	PCMPGTB xmm7, xmm8
	PUNPCKLBW xmm4, xmm3
	MOVDQA [byte rdx + 80], xmm15
	MOVQ xmm14, [byte rdi + 32]
	PXOR xmm10, xmm10
	PCMPGTB xmm6, xmm11
	PUNPCKLBW xmm8, xmm7
	PADDW xmm4, xmm5
	ADD rdx, 96
	MOVQ xmm15, [byte rdi + 40]
	PXOR xmm12, xmm12
	PCMPGTB xmm13, xmm9
	PUNPCKLBW xmm11, xmm6
	PADDW xmm8, xmm5
	MOVDQA [rdx], xmm4
	ADD rdi, 48
	PCMPGTB xmm10, xmm14
	PUNPCKLBW xmm9, xmm13
	PADDW xmm11, xmm5
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 48
	JAE .process_batch
	.process_batch_epilogue:
	PCMPGTB xmm12, xmm15
	PUNPCKLBW xmm14, xmm10
	PADDW xmm9, xmm5
	MOVDQA [byte rdx + 32], xmm11
	PUNPCKLBW xmm15, xmm12
	PADDW xmm14, xmm5
	MOVDQA [byte rdx + 48], xmm9
	PADDW xmm15, xmm5
	MOVDQA [byte rdx + 64], xmm14
	MOVDQA [byte rdx + 80], xmm15
	ADD rdx, 96
	.batch_process_finish:
	ADD rcx, 48
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V8sS8s_V16s_Nehalem
_yepCore_Add_V8sS8s_V16s_Nehalem:
%else
section .text
global __yepCore_Add_V8sS8s_V16s_Nehalem
__yepCore_Add_V8sS8s_V16s_Nehalem:
%endif
	.ENTRY:
	MOVSX ax, sil
	MOVZX eax, ax
	IMUL eax, eax, 65537
	MOVD xmm15, eax
	PSHUFD xmm15, xmm15, 0
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVSXBW xmm7, [rdi]
	PMOVSXBW xmm8, [byte rdi + 8]
	PMOVSXBW xmm10, [byte rdi + 16]
	PMOVSXBW xmm9, [byte rdi + 24]
	PMOVSXBW xmm12, [byte rdi + 32]
	PADDW xmm7, xmm15
	PMOVSXBW xmm13, [byte rdi + 40]
	PADDW xmm8, xmm15
	MOVDQA [rdx], xmm7
	PMOVSXBW xmm11, [byte rdi + 48]
	PADDW xmm10, xmm15
	MOVDQA [byte rdx + 16], xmm8
	PMOVSXBW xmm14, [byte rdi + 56]
	PADDW xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	PADDW xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVSXBW xmm7, [rdi]
	PADDW xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	PMOVSXBW xmm8, [byte rdi + 8]
	PADDW xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	PMOVSXBW xmm10, [byte rdi + 16]
	PADDW xmm14, xmm15
	MOVDQA [byte rdx + 96], xmm11
	PMOVSXBW xmm9, [byte rdi + 24]
	MOVDQA [byte rdx + 112], xmm14
	PMOVSXBW xmm12, [byte rdi + 32]
	PADDW xmm7, xmm15
	ADD rdx, 128
	PMOVSXBW xmm13, [byte rdi + 40]
	PADDW xmm8, xmm15
	MOVDQA [rdx], xmm7
	PMOVSXBW xmm11, [byte rdi + 48]
	PADDW xmm10, xmm15
	MOVDQA [byte rdx + 16], xmm8
	PMOVSXBW xmm14, [byte rdi + 56]
	PADDW xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	PADDW xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	PADDW xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	PADDW xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	PADDW xmm14, xmm15
	MOVDQA [byte rdx + 96], xmm11
	MOVDQA [byte rdx + 112], xmm14
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V8sS8s_V16s_SandyBridge
_yepCore_Add_V8sS8s_V16s_SandyBridge:
%else
section .text
global __yepCore_Add_V8sS8s_V16s_SandyBridge
__yepCore_Add_V8sS8s_V16s_SandyBridge:
%endif
	.ENTRY:
	MOVSX ax, sil
	MOVZX eax, ax
	IMUL eax, eax, 65537
	MOVD xmm15, eax
	PSHUFD xmm15, xmm15, 0
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXBW xmm7, [rdi]
	VPMOVSXBW xmm8, [byte rdi + 8]
	VPMOVSXBW xmm10, [byte rdi + 16]
	VPMOVSXBW xmm9, [byte rdi + 24]
	VPMOVSXBW xmm12, [byte rdi + 32]
	VPADDW xmm7, xmm7, xmm15
	VPMOVSXBW xmm13, [byte rdi + 40]
	VPADDW xmm8, xmm8, xmm15
	VMOVDQA [rdx], xmm7
	VPMOVSXBW xmm11, [byte rdi + 48]
	VPADDW xmm10, xmm10, xmm15
	VMOVDQA [byte rdx + 16], xmm8
	VPMOVSXBW xmm14, [byte rdi + 56]
	VPADDW xmm9, xmm9, xmm15
	VMOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	VPADDW xmm12, xmm12, xmm15
	VMOVDQA [byte rdx + 48], xmm9
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXBW xmm7, [rdi]
	VPADDW xmm13, xmm13, xmm15
	VMOVDQA [byte rdx + 64], xmm12
	VPMOVSXBW xmm8, [byte rdi + 8]
	VPADDW xmm11, xmm11, xmm15
	VMOVDQA [byte rdx + 80], xmm13
	VPMOVSXBW xmm10, [byte rdi + 16]
	VPADDW xmm14, xmm14, xmm15
	VMOVDQA [byte rdx + 96], xmm11
	VPMOVSXBW xmm9, [byte rdi + 24]
	VMOVDQA [byte rdx + 112], xmm14
	VPMOVSXBW xmm12, [byte rdi + 32]
	VPADDW xmm7, xmm7, xmm15
	ADD rdx, 128
	VPMOVSXBW xmm13, [byte rdi + 40]
	VPADDW xmm8, xmm8, xmm15
	VMOVDQA [rdx], xmm7
	VPMOVSXBW xmm11, [byte rdi + 48]
	VPADDW xmm10, xmm10, xmm15
	VMOVDQA [byte rdx + 16], xmm8
	VPMOVSXBW xmm14, [byte rdi + 56]
	VPADDW xmm9, xmm9, xmm15
	VMOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	VPADDW xmm12, xmm12, xmm15
	VMOVDQA [byte rdx + 48], xmm9
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	VPADDW xmm13, xmm13, xmm15
	VMOVDQA [byte rdx + 64], xmm12
	VPADDW xmm11, xmm11, xmm15
	VMOVDQA [byte rdx + 80], xmm13
	VPADDW xmm14, xmm14, xmm15
	VMOVDQA [byte rdx + 96], xmm11
	VMOVDQA [byte rdx + 112], xmm14
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V8sS8s_V16s_Haswell
_yepCore_Add_V8sS8s_V16s_Haswell:
%else
section .text
global __yepCore_Add_V8sS8s_V16s_Haswell
__yepCore_Add_V8sS8s_V16s_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXBW ymm7, [rdi]
	VPMOVSXBW ymm8, [byte rdi + 16]
	VPMOVSXBW ymm15, [byte rdi + 32]
	VPMOVSXBW ymm9, [byte rdi + 48]
	VPMOVSXBW ymm13, [byte rdi + 64]
	VPADDW ymm7, ymm7, ymm14
	VPMOVSXBW ymm10, [byte rdi + 80]
	VPADDW ymm8, ymm8, ymm14
	VMOVDQA [rdx], ymm7
	VPMOVSXBW ymm11, [byte rdi + 96]
	VPADDW ymm15, ymm15, ymm14
	VMOVDQA [byte rdx + 32], ymm8
	VPMOVSXBW ymm12, [byte rdi + 112]
	VPADDW ymm9, ymm9, ymm14
	VMOVDQA [byte rdx + 64], ymm15
	ADD rdi, 128
	VPADDW ymm13, ymm13, ymm14
	VMOVDQA [byte rdx + 96], ymm9
	SUB rcx, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXBW ymm7, [rdi]
	VPADDW ymm10, ymm10, ymm14
	VMOVDQA [dword rdx + 128], ymm13
	VPMOVSXBW ymm8, [byte rdi + 16]
	VPADDW ymm11, ymm11, ymm14
	VMOVDQA [dword rdx + 160], ymm10
	VPMOVSXBW ymm15, [byte rdi + 32]
	VPADDW ymm12, ymm12, ymm14
	VMOVDQA [dword rdx + 192], ymm11
	VPMOVSXBW ymm9, [byte rdi + 48]
	VMOVDQA [dword rdx + 224], ymm12
	VPMOVSXBW ymm13, [byte rdi + 64]
	VPADDW ymm7, ymm7, ymm14
	ADD rdx, 256
	VPMOVSXBW ymm10, [byte rdi + 80]
	VPADDW ymm8, ymm8, ymm14
	VMOVDQA [rdx], ymm7
	VPMOVSXBW ymm11, [byte rdi + 96]
	VPADDW ymm15, ymm15, ymm14
	VMOVDQA [byte rdx + 32], ymm8
	VPMOVSXBW ymm12, [byte rdi + 112]
	VPADDW ymm9, ymm9, ymm14
	VMOVDQA [byte rdx + 64], ymm15
	ADD rdi, 128
	VPADDW ymm13, ymm13, ymm14
	VMOVDQA [byte rdx + 96], ymm9
	SUB rcx, 128
	JAE .process_batch
	.process_batch_epilogue:
	VPADDW ymm10, ymm10, ymm14
	VMOVDQA [dword rdx + 128], ymm13
	VPADDW ymm11, ymm11, ymm14
	VMOVDQA [dword rdx + 160], ymm10
	VPADDW ymm12, ymm12, ymm14
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm12
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.K10 progbits alloc exec nowrite align=16
global _yepCore_Add_V8uS8u_V16u_K10
_yepCore_Add_V8uS8u_V16u_K10:
%else
section .text
global __yepCore_Add_V8uS8u_V16u_K10
__yepCore_Add_V8uS8u_V16u_K10:
%endif
	.ENTRY:
	PXOR xmm14, xmm14
	MOVZX eax, sil
	IMUL eax, eax, 65537
	MOVD xmm15, eax
	PSHUFD xmm15, xmm15, 0
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 56
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm7, [rdi]
	MOVQ xmm10, [byte rdi + 8]
	MOVQ xmm8, [byte rdi + 16]
	MOVQ xmm9, [byte rdi + 24]
	MOVQ xmm12, [byte rdi + 32]
	PUNPCKLBW xmm7, xmm14
	MOVQ xmm13, [byte rdi + 40]
	PUNPCKLBW xmm10, xmm14
	PADDW xmm7, xmm15
	MOVQ xmm11, [byte rdi + 48]
	PUNPCKLBW xmm8, xmm14
	PADDW xmm10, xmm15
	MOVDQA [rdx], xmm7
	ADD rdi, 56
	PUNPCKLBW xmm9, xmm14
	PADDW xmm8, xmm15
	MOVDQA [byte rdx + 16], xmm10
	SUB rcx, 56
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm7, [rdi]
	PUNPCKLBW xmm12, xmm14
	PADDW xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm8
	MOVQ xmm10, [byte rdi + 8]
	PUNPCKLBW xmm13, xmm14
	PADDW xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	MOVQ xmm8, [byte rdi + 16]
	PUNPCKLBW xmm11, xmm14
	PADDW xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	MOVQ xmm9, [byte rdi + 24]
	PADDW xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	MOVQ xmm12, [byte rdi + 32]
	PUNPCKLBW xmm7, xmm14
	MOVDQA [byte rdx + 96], xmm11
	MOVQ xmm13, [byte rdi + 40]
	PUNPCKLBW xmm10, xmm14
	PADDW xmm7, xmm15
	ADD rdx, 112
	MOVQ xmm11, [byte rdi + 48]
	PUNPCKLBW xmm8, xmm14
	PADDW xmm10, xmm15
	MOVDQA [rdx], xmm7
	ADD rdi, 56
	PUNPCKLBW xmm9, xmm14
	PADDW xmm8, xmm15
	MOVDQA [byte rdx + 16], xmm10
	SUB rcx, 56
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLBW xmm12, xmm14
	PADDW xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm8
	PUNPCKLBW xmm13, xmm14
	PADDW xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	PUNPCKLBW xmm11, xmm14
	PADDW xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	PADDW xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	MOVDQA [byte rdx + 96], xmm11
	ADD rdx, 112
	.batch_process_finish:
	ADD rcx, 56
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V8uS8u_V16u_Nehalem
_yepCore_Add_V8uS8u_V16u_Nehalem:
%else
section .text
global __yepCore_Add_V8uS8u_V16u_Nehalem
__yepCore_Add_V8uS8u_V16u_Nehalem:
%endif
	.ENTRY:
	MOVZX eax, sil
	IMUL eax, eax, 65537
	MOVD xmm15, eax
	PSHUFD xmm15, xmm15, 0
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXBW xmm7, [rdi]
	PMOVZXBW xmm8, [byte rdi + 8]
	PMOVZXBW xmm10, [byte rdi + 16]
	PMOVZXBW xmm9, [byte rdi + 24]
	PMOVZXBW xmm12, [byte rdi + 32]
	PADDW xmm7, xmm15
	PMOVZXBW xmm13, [byte rdi + 40]
	PADDW xmm8, xmm15
	MOVDQA [rdx], xmm7
	PMOVZXBW xmm11, [byte rdi + 48]
	PADDW xmm10, xmm15
	MOVDQA [byte rdx + 16], xmm8
	PMOVZXBW xmm14, [byte rdi + 56]
	PADDW xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	PADDW xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXBW xmm7, [rdi]
	PADDW xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	PMOVZXBW xmm8, [byte rdi + 8]
	PADDW xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	PMOVZXBW xmm10, [byte rdi + 16]
	PADDW xmm14, xmm15
	MOVDQA [byte rdx + 96], xmm11
	PMOVZXBW xmm9, [byte rdi + 24]
	MOVDQA [byte rdx + 112], xmm14
	PMOVZXBW xmm12, [byte rdi + 32]
	PADDW xmm7, xmm15
	ADD rdx, 128
	PMOVZXBW xmm13, [byte rdi + 40]
	PADDW xmm8, xmm15
	MOVDQA [rdx], xmm7
	PMOVZXBW xmm11, [byte rdi + 48]
	PADDW xmm10, xmm15
	MOVDQA [byte rdx + 16], xmm8
	PMOVZXBW xmm14, [byte rdi + 56]
	PADDW xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	PADDW xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	PADDW xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	PADDW xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	PADDW xmm14, xmm15
	MOVDQA [byte rdx + 96], xmm11
	MOVDQA [byte rdx + 112], xmm14
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V8uS8u_V16u_SandyBridge
_yepCore_Add_V8uS8u_V16u_SandyBridge:
%else
section .text
global __yepCore_Add_V8uS8u_V16u_SandyBridge
__yepCore_Add_V8uS8u_V16u_SandyBridge:
%endif
	.ENTRY:
	MOVZX eax, sil
	IMUL eax, eax, 65537
	MOVD xmm15, eax
	PSHUFD xmm15, xmm15, 0
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXBW xmm7, [rdi]
	VPMOVZXBW xmm8, [byte rdi + 8]
	VPMOVZXBW xmm10, [byte rdi + 16]
	VPMOVZXBW xmm9, [byte rdi + 24]
	VPMOVZXBW xmm12, [byte rdi + 32]
	VPADDW xmm7, xmm7, xmm15
	VPMOVZXBW xmm13, [byte rdi + 40]
	VPADDW xmm8, xmm8, xmm15
	VMOVDQA [rdx], xmm7
	VPMOVZXBW xmm11, [byte rdi + 48]
	VPADDW xmm10, xmm10, xmm15
	VMOVDQA [byte rdx + 16], xmm8
	VPMOVZXBW xmm14, [byte rdi + 56]
	VPADDW xmm9, xmm9, xmm15
	VMOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	VPADDW xmm12, xmm12, xmm15
	VMOVDQA [byte rdx + 48], xmm9
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXBW xmm7, [rdi]
	VPADDW xmm13, xmm13, xmm15
	VMOVDQA [byte rdx + 64], xmm12
	VPMOVZXBW xmm8, [byte rdi + 8]
	VPADDW xmm11, xmm11, xmm15
	VMOVDQA [byte rdx + 80], xmm13
	VPMOVZXBW xmm10, [byte rdi + 16]
	VPADDW xmm14, xmm14, xmm15
	VMOVDQA [byte rdx + 96], xmm11
	VPMOVZXBW xmm9, [byte rdi + 24]
	VMOVDQA [byte rdx + 112], xmm14
	VPMOVZXBW xmm12, [byte rdi + 32]
	VPADDW xmm7, xmm7, xmm15
	ADD rdx, 128
	VPMOVZXBW xmm13, [byte rdi + 40]
	VPADDW xmm8, xmm8, xmm15
	VMOVDQA [rdx], xmm7
	VPMOVZXBW xmm11, [byte rdi + 48]
	VPADDW xmm10, xmm10, xmm15
	VMOVDQA [byte rdx + 16], xmm8
	VPMOVZXBW xmm14, [byte rdi + 56]
	VPADDW xmm9, xmm9, xmm15
	VMOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	VPADDW xmm12, xmm12, xmm15
	VMOVDQA [byte rdx + 48], xmm9
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	VPADDW xmm13, xmm13, xmm15
	VMOVDQA [byte rdx + 64], xmm12
	VPADDW xmm11, xmm11, xmm15
	VMOVDQA [byte rdx + 80], xmm13
	VPADDW xmm14, xmm14, xmm15
	VMOVDQA [byte rdx + 96], xmm11
	VMOVDQA [byte rdx + 112], xmm14
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V8uS8u_V16u_Haswell
_yepCore_Add_V8uS8u_V16u_Haswell:
%else
section .text
global __yepCore_Add_V8uS8u_V16u_Haswell
__yepCore_Add_V8uS8u_V16u_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXBW ymm7, [rdi]
	VPMOVZXBW ymm8, [byte rdi + 16]
	VPMOVZXBW ymm15, [byte rdi + 32]
	VPMOVZXBW ymm9, [byte rdi + 48]
	VPMOVZXBW ymm13, [byte rdi + 64]
	VPADDW ymm7, ymm7, ymm14
	VPMOVZXBW ymm10, [byte rdi + 80]
	VPADDW ymm8, ymm8, ymm14
	VMOVDQA [rdx], ymm7
	VPMOVZXBW ymm11, [byte rdi + 96]
	VPADDW ymm15, ymm15, ymm14
	VMOVDQA [byte rdx + 32], ymm8
	VPMOVZXBW ymm12, [byte rdi + 112]
	VPADDW ymm9, ymm9, ymm14
	VMOVDQA [byte rdx + 64], ymm15
	ADD rdi, 128
	VPADDW ymm13, ymm13, ymm14
	VMOVDQA [byte rdx + 96], ymm9
	SUB rcx, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXBW ymm7, [rdi]
	VPADDW ymm10, ymm10, ymm14
	VMOVDQA [dword rdx + 128], ymm13
	VPMOVZXBW ymm8, [byte rdi + 16]
	VPADDW ymm11, ymm11, ymm14
	VMOVDQA [dword rdx + 160], ymm10
	VPMOVZXBW ymm15, [byte rdi + 32]
	VPADDW ymm12, ymm12, ymm14
	VMOVDQA [dword rdx + 192], ymm11
	VPMOVZXBW ymm9, [byte rdi + 48]
	VMOVDQA [dword rdx + 224], ymm12
	VPMOVZXBW ymm13, [byte rdi + 64]
	VPADDW ymm7, ymm7, ymm14
	ADD rdx, 256
	VPMOVZXBW ymm10, [byte rdi + 80]
	VPADDW ymm8, ymm8, ymm14
	VMOVDQA [rdx], ymm7
	VPMOVZXBW ymm11, [byte rdi + 96]
	VPADDW ymm15, ymm15, ymm14
	VMOVDQA [byte rdx + 32], ymm8
	VPMOVZXBW ymm12, [byte rdi + 112]
	VPADDW ymm9, ymm9, ymm14
	VMOVDQA [byte rdx + 64], ymm15
	ADD rdi, 128
	VPADDW ymm13, ymm13, ymm14
	VMOVDQA [byte rdx + 96], ymm9
	SUB rcx, 128
	JAE .process_batch
	.process_batch_epilogue:
	VPADDW ymm10, ymm10, ymm14
	VMOVDQA [dword rdx + 128], ymm13
	VPADDW ymm11, ymm11, ymm14
	VMOVDQA [dword rdx + 160], ymm10
	VPADDW ymm12, ymm12, ymm14
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm12
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 128
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rdi]
	ADD rdi, 1
	ADD eax, esi
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.K10 progbits alloc exec nowrite align=16
global _yepCore_Add_V16sS16s_V32s_K10
_yepCore_Add_V16sS16s_V32s_K10:
%else
section .text
global __yepCore_Add_V16sS16s_V32s_K10
__yepCore_Add_V16sS16s_V32s_K10:
%endif
	.ENTRY:
	MOVSX rsi, si
	MOVSX eax, si
	MOVD xmm5, eax
	PSHUFD xmm5, xmm5, 0
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 24
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm4, [rdi]
	PXOR xmm3, xmm3
	MOVQ xmm8, [byte rdi + 8]
	PXOR xmm7, xmm7
	MOVQ xmm11, [byte rdi + 16]
	PXOR xmm6, xmm6
	PCMPGTW xmm3, xmm4
	MOVQ xmm9, [byte rdi + 24]
	PXOR xmm13, xmm13
	PCMPGTW xmm7, xmm8
	PUNPCKLWD xmm4, xmm3
	MOVQ xmm14, [byte rdi + 32]
	PXOR xmm10, xmm10
	PCMPGTW xmm6, xmm11
	PUNPCKLWD xmm8, xmm7
	PADDD xmm4, xmm5
	MOVQ xmm15, [byte rdi + 40]
	PXOR xmm12, xmm12
	PCMPGTW xmm13, xmm9
	PUNPCKLWD xmm11, xmm6
	PADDD xmm8, xmm5
	MOVDQA [rdx], xmm4
	ADD rdi, 48
	PCMPGTW xmm10, xmm14
	PUNPCKLWD xmm9, xmm13
	PADDD xmm11, xmm5
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 24
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm4, [rdi]
	PXOR xmm3, xmm3
	PCMPGTW xmm12, xmm15
	PUNPCKLWD xmm14, xmm10
	PADDD xmm9, xmm5
	MOVDQA [byte rdx + 32], xmm11
	MOVQ xmm8, [byte rdi + 8]
	PXOR xmm7, xmm7
	PUNPCKLWD xmm15, xmm12
	PADDD xmm14, xmm5
	MOVDQA [byte rdx + 48], xmm9
	MOVQ xmm11, [byte rdi + 16]
	PXOR xmm6, xmm6
	PCMPGTW xmm3, xmm4
	PADDD xmm15, xmm5
	MOVDQA [byte rdx + 64], xmm14
	MOVQ xmm9, [byte rdi + 24]
	PXOR xmm13, xmm13
	PCMPGTW xmm7, xmm8
	PUNPCKLWD xmm4, xmm3
	MOVDQA [byte rdx + 80], xmm15
	MOVQ xmm14, [byte rdi + 32]
	PXOR xmm10, xmm10
	PCMPGTW xmm6, xmm11
	PUNPCKLWD xmm8, xmm7
	PADDD xmm4, xmm5
	ADD rdx, 96
	MOVQ xmm15, [byte rdi + 40]
	PXOR xmm12, xmm12
	PCMPGTW xmm13, xmm9
	PUNPCKLWD xmm11, xmm6
	PADDD xmm8, xmm5
	MOVDQA [rdx], xmm4
	ADD rdi, 48
	PCMPGTW xmm10, xmm14
	PUNPCKLWD xmm9, xmm13
	PADDD xmm11, xmm5
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 24
	JAE .process_batch
	.process_batch_epilogue:
	PCMPGTW xmm12, xmm15
	PUNPCKLWD xmm14, xmm10
	PADDD xmm9, xmm5
	MOVDQA [byte rdx + 32], xmm11
	PUNPCKLWD xmm15, xmm12
	PADDD xmm14, xmm5
	MOVDQA [byte rdx + 48], xmm9
	PADDD xmm15, xmm5
	MOVDQA [byte rdx + 64], xmm14
	MOVDQA [byte rdx + 80], xmm15
	ADD rdx, 96
	.batch_process_finish:
	ADD rcx, 24
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V16sS16s_V32s_Nehalem
_yepCore_Add_V16sS16s_V32s_Nehalem:
%else
section .text
global __yepCore_Add_V16sS16s_V32s_Nehalem
__yepCore_Add_V16sS16s_V32s_Nehalem:
%endif
	.ENTRY:
	MOVSX rsi, si
	MOVSX eax, si
	MOVD xmm15, eax
	PSHUFD xmm15, xmm15, 0
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVSXWD xmm7, [rdi]
	PMOVSXWD xmm8, [byte rdi + 8]
	PMOVSXWD xmm10, [byte rdi + 16]
	PMOVSXWD xmm9, [byte rdi + 24]
	PMOVSXWD xmm12, [byte rdi + 32]
	PADDD xmm7, xmm15
	PMOVSXWD xmm13, [byte rdi + 40]
	PADDD xmm8, xmm15
	MOVDQA [rdx], xmm7
	PMOVSXWD xmm11, [byte rdi + 48]
	PADDD xmm10, xmm15
	MOVDQA [byte rdx + 16], xmm8
	PMOVSXWD xmm14, [byte rdi + 56]
	PADDD xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	PADDD xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVSXWD xmm7, [rdi]
	PADDD xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	PMOVSXWD xmm8, [byte rdi + 8]
	PADDD xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	PMOVSXWD xmm10, [byte rdi + 16]
	PADDD xmm14, xmm15
	MOVDQA [byte rdx + 96], xmm11
	PMOVSXWD xmm9, [byte rdi + 24]
	MOVDQA [byte rdx + 112], xmm14
	PMOVSXWD xmm12, [byte rdi + 32]
	PADDD xmm7, xmm15
	ADD rdx, 128
	PMOVSXWD xmm13, [byte rdi + 40]
	PADDD xmm8, xmm15
	MOVDQA [rdx], xmm7
	PMOVSXWD xmm11, [byte rdi + 48]
	PADDD xmm10, xmm15
	MOVDQA [byte rdx + 16], xmm8
	PMOVSXWD xmm14, [byte rdi + 56]
	PADDD xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	PADDD xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	PADDD xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	PADDD xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	PADDD xmm14, xmm15
	MOVDQA [byte rdx + 96], xmm11
	MOVDQA [byte rdx + 112], xmm14
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V16sS16s_V32s_SandyBridge
_yepCore_Add_V16sS16s_V32s_SandyBridge:
%else
section .text
global __yepCore_Add_V16sS16s_V32s_SandyBridge
__yepCore_Add_V16sS16s_V32s_SandyBridge:
%endif
	.ENTRY:
	MOVSX rsi, si
	MOVSX eax, si
	MOVD xmm15, eax
	PSHUFD xmm15, xmm15, 0
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXWD xmm7, [rdi]
	VPMOVSXWD xmm8, [byte rdi + 8]
	VPMOVSXWD xmm10, [byte rdi + 16]
	VPMOVSXWD xmm9, [byte rdi + 24]
	VPMOVSXWD xmm12, [byte rdi + 32]
	VPADDD xmm7, xmm7, xmm15
	VPMOVSXWD xmm13, [byte rdi + 40]
	VPADDD xmm8, xmm8, xmm15
	VMOVDQA [rdx], xmm7
	VPMOVSXWD xmm11, [byte rdi + 48]
	VPADDD xmm10, xmm10, xmm15
	VMOVDQA [byte rdx + 16], xmm8
	VPMOVSXWD xmm14, [byte rdi + 56]
	VPADDD xmm9, xmm9, xmm15
	VMOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	VPADDD xmm12, xmm12, xmm15
	VMOVDQA [byte rdx + 48], xmm9
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXWD xmm7, [rdi]
	VPADDD xmm13, xmm13, xmm15
	VMOVDQA [byte rdx + 64], xmm12
	VPMOVSXWD xmm8, [byte rdi + 8]
	VPADDD xmm11, xmm11, xmm15
	VMOVDQA [byte rdx + 80], xmm13
	VPMOVSXWD xmm10, [byte rdi + 16]
	VPADDD xmm14, xmm14, xmm15
	VMOVDQA [byte rdx + 96], xmm11
	VPMOVSXWD xmm9, [byte rdi + 24]
	VMOVDQA [byte rdx + 112], xmm14
	VPMOVSXWD xmm12, [byte rdi + 32]
	VPADDD xmm7, xmm7, xmm15
	ADD rdx, 128
	VPMOVSXWD xmm13, [byte rdi + 40]
	VPADDD xmm8, xmm8, xmm15
	VMOVDQA [rdx], xmm7
	VPMOVSXWD xmm11, [byte rdi + 48]
	VPADDD xmm10, xmm10, xmm15
	VMOVDQA [byte rdx + 16], xmm8
	VPMOVSXWD xmm14, [byte rdi + 56]
	VPADDD xmm9, xmm9, xmm15
	VMOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	VPADDD xmm12, xmm12, xmm15
	VMOVDQA [byte rdx + 48], xmm9
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	VPADDD xmm13, xmm13, xmm15
	VMOVDQA [byte rdx + 64], xmm12
	VPADDD xmm11, xmm11, xmm15
	VMOVDQA [byte rdx + 80], xmm13
	VPADDD xmm14, xmm14, xmm15
	VMOVDQA [byte rdx + 96], xmm11
	VMOVDQA [byte rdx + 112], xmm14
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V16sS16s_V32s_Haswell
_yepCore_Add_V16sS16s_V32s_Haswell:
%else
section .text
global __yepCore_Add_V16sS16s_V32s_Haswell
__yepCore_Add_V16sS16s_V32s_Haswell:
%endif
	.ENTRY:
	MOVSX rsi, si
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXWD ymm7, [rdi]
	VPMOVSXWD ymm8, [byte rdi + 16]
	VPMOVSXWD ymm15, [byte rdi + 32]
	VPMOVSXWD ymm9, [byte rdi + 48]
	VPMOVSXWD ymm13, [byte rdi + 64]
	VPADDD ymm7, ymm7, ymm14
	VPMOVSXWD ymm10, [byte rdi + 80]
	VPADDD ymm8, ymm8, ymm14
	VMOVDQA [rdx], ymm7
	VPMOVSXWD ymm11, [byte rdi + 96]
	VPADDD ymm15, ymm15, ymm14
	VMOVDQA [byte rdx + 32], ymm8
	VPMOVSXWD ymm12, [byte rdi + 112]
	VPADDD ymm9, ymm9, ymm14
	VMOVDQA [byte rdx + 64], ymm15
	ADD rdi, 128
	VPADDD ymm13, ymm13, ymm14
	VMOVDQA [byte rdx + 96], ymm9
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXWD ymm7, [rdi]
	VPADDD ymm10, ymm10, ymm14
	VMOVDQA [dword rdx + 128], ymm13
	VPMOVSXWD ymm8, [byte rdi + 16]
	VPADDD ymm11, ymm11, ymm14
	VMOVDQA [dword rdx + 160], ymm10
	VPMOVSXWD ymm15, [byte rdi + 32]
	VPADDD ymm12, ymm12, ymm14
	VMOVDQA [dword rdx + 192], ymm11
	VPMOVSXWD ymm9, [byte rdi + 48]
	VMOVDQA [dword rdx + 224], ymm12
	VPMOVSXWD ymm13, [byte rdi + 64]
	VPADDD ymm7, ymm7, ymm14
	ADD rdx, 256
	VPMOVSXWD ymm10, [byte rdi + 80]
	VPADDD ymm8, ymm8, ymm14
	VMOVDQA [rdx], ymm7
	VPMOVSXWD ymm11, [byte rdi + 96]
	VPADDD ymm15, ymm15, ymm14
	VMOVDQA [byte rdx + 32], ymm8
	VPMOVSXWD ymm12, [byte rdi + 112]
	VPADDD ymm9, ymm9, ymm14
	VMOVDQA [byte rdx + 64], ymm15
	ADD rdi, 128
	VPADDD ymm13, ymm13, ymm14
	VMOVDQA [byte rdx + 96], ymm9
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	VPADDD ymm10, ymm10, ymm14
	VMOVDQA [dword rdx + 128], ymm13
	VPADDD ymm11, ymm11, ymm14
	VMOVDQA [dword rdx + 160], ymm10
	VPADDD ymm12, ymm12, ymm14
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm12
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.K10 progbits alloc exec nowrite align=16
global _yepCore_Add_V16uS16u_V32u_K10
_yepCore_Add_V16uS16u_V32u_K10:
%else
section .text
global __yepCore_Add_V16uS16u_V32u_K10
__yepCore_Add_V16uS16u_V32u_K10:
%endif
	.ENTRY:
	MOVZX esi, si
	PXOR xmm14, xmm14
	MOVZX eax, si
	MOVD xmm15, eax
	PSHUFD xmm15, xmm15, 0
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 28
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm7, [rdi]
	MOVQ xmm10, [byte rdi + 8]
	MOVQ xmm8, [byte rdi + 16]
	MOVQ xmm9, [byte rdi + 24]
	MOVQ xmm12, [byte rdi + 32]
	PUNPCKLWD xmm7, xmm14
	MOVQ xmm13, [byte rdi + 40]
	PUNPCKLWD xmm10, xmm14
	PADDD xmm7, xmm15
	MOVQ xmm11, [byte rdi + 48]
	PUNPCKLWD xmm8, xmm14
	PADDD xmm10, xmm15
	MOVDQA [rdx], xmm7
	ADD rdi, 56
	PUNPCKLWD xmm9, xmm14
	PADDD xmm8, xmm15
	MOVDQA [byte rdx + 16], xmm10
	SUB rcx, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm7, [rdi]
	PUNPCKLWD xmm12, xmm14
	PADDD xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm8
	MOVQ xmm10, [byte rdi + 8]
	PUNPCKLWD xmm13, xmm14
	PADDD xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	MOVQ xmm8, [byte rdi + 16]
	PUNPCKLWD xmm11, xmm14
	PADDD xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	MOVQ xmm9, [byte rdi + 24]
	PADDD xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	MOVQ xmm12, [byte rdi + 32]
	PUNPCKLWD xmm7, xmm14
	MOVDQA [byte rdx + 96], xmm11
	MOVQ xmm13, [byte rdi + 40]
	PUNPCKLWD xmm10, xmm14
	PADDD xmm7, xmm15
	ADD rdx, 112
	MOVQ xmm11, [byte rdi + 48]
	PUNPCKLWD xmm8, xmm14
	PADDD xmm10, xmm15
	MOVDQA [rdx], xmm7
	ADD rdi, 56
	PUNPCKLWD xmm9, xmm14
	PADDD xmm8, xmm15
	MOVDQA [byte rdx + 16], xmm10
	SUB rcx, 28
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLWD xmm12, xmm14
	PADDD xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm8
	PUNPCKLWD xmm13, xmm14
	PADDD xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	PUNPCKLWD xmm11, xmm14
	PADDD xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	PADDD xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	MOVDQA [byte rdx + 96], xmm11
	ADD rdx, 112
	.batch_process_finish:
	ADD rcx, 28
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V16uS16u_V32u_Nehalem
_yepCore_Add_V16uS16u_V32u_Nehalem:
%else
section .text
global __yepCore_Add_V16uS16u_V32u_Nehalem
__yepCore_Add_V16uS16u_V32u_Nehalem:
%endif
	.ENTRY:
	MOVZX esi, si
	MOVZX eax, si
	MOVD xmm15, eax
	PSHUFD xmm15, xmm15, 0
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXWD xmm7, [rdi]
	PMOVZXWD xmm8, [byte rdi + 8]
	PMOVZXWD xmm10, [byte rdi + 16]
	PMOVZXWD xmm9, [byte rdi + 24]
	PMOVZXWD xmm12, [byte rdi + 32]
	PADDD xmm7, xmm15
	PMOVZXWD xmm13, [byte rdi + 40]
	PADDD xmm8, xmm15
	MOVDQA [rdx], xmm7
	PMOVZXWD xmm11, [byte rdi + 48]
	PADDD xmm10, xmm15
	MOVDQA [byte rdx + 16], xmm8
	PMOVZXWD xmm14, [byte rdi + 56]
	PADDD xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	PADDD xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXWD xmm7, [rdi]
	PADDD xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	PMOVZXWD xmm8, [byte rdi + 8]
	PADDD xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	PMOVZXWD xmm10, [byte rdi + 16]
	PADDD xmm14, xmm15
	MOVDQA [byte rdx + 96], xmm11
	PMOVZXWD xmm9, [byte rdi + 24]
	MOVDQA [byte rdx + 112], xmm14
	PMOVZXWD xmm12, [byte rdi + 32]
	PADDD xmm7, xmm15
	ADD rdx, 128
	PMOVZXWD xmm13, [byte rdi + 40]
	PADDD xmm8, xmm15
	MOVDQA [rdx], xmm7
	PMOVZXWD xmm11, [byte rdi + 48]
	PADDD xmm10, xmm15
	MOVDQA [byte rdx + 16], xmm8
	PMOVZXWD xmm14, [byte rdi + 56]
	PADDD xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	PADDD xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	PADDD xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	PADDD xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	PADDD xmm14, xmm15
	MOVDQA [byte rdx + 96], xmm11
	MOVDQA [byte rdx + 112], xmm14
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V16uS16u_V32u_SandyBridge
_yepCore_Add_V16uS16u_V32u_SandyBridge:
%else
section .text
global __yepCore_Add_V16uS16u_V32u_SandyBridge
__yepCore_Add_V16uS16u_V32u_SandyBridge:
%endif
	.ENTRY:
	MOVZX esi, si
	MOVZX eax, si
	MOVD xmm15, eax
	PSHUFD xmm15, xmm15, 0
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXWD xmm7, [rdi]
	VPMOVZXWD xmm8, [byte rdi + 8]
	VPMOVZXWD xmm10, [byte rdi + 16]
	VPMOVZXWD xmm9, [byte rdi + 24]
	VPMOVZXWD xmm12, [byte rdi + 32]
	VPADDD xmm7, xmm7, xmm15
	VPMOVZXWD xmm13, [byte rdi + 40]
	VPADDD xmm8, xmm8, xmm15
	VMOVDQA [rdx], xmm7
	VPMOVZXWD xmm11, [byte rdi + 48]
	VPADDD xmm10, xmm10, xmm15
	VMOVDQA [byte rdx + 16], xmm8
	VPMOVZXWD xmm14, [byte rdi + 56]
	VPADDD xmm9, xmm9, xmm15
	VMOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	VPADDD xmm12, xmm12, xmm15
	VMOVDQA [byte rdx + 48], xmm9
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXWD xmm7, [rdi]
	VPADDD xmm13, xmm13, xmm15
	VMOVDQA [byte rdx + 64], xmm12
	VPMOVZXWD xmm8, [byte rdi + 8]
	VPADDD xmm11, xmm11, xmm15
	VMOVDQA [byte rdx + 80], xmm13
	VPMOVZXWD xmm10, [byte rdi + 16]
	VPADDD xmm14, xmm14, xmm15
	VMOVDQA [byte rdx + 96], xmm11
	VPMOVZXWD xmm9, [byte rdi + 24]
	VMOVDQA [byte rdx + 112], xmm14
	VPMOVZXWD xmm12, [byte rdi + 32]
	VPADDD xmm7, xmm7, xmm15
	ADD rdx, 128
	VPMOVZXWD xmm13, [byte rdi + 40]
	VPADDD xmm8, xmm8, xmm15
	VMOVDQA [rdx], xmm7
	VPMOVZXWD xmm11, [byte rdi + 48]
	VPADDD xmm10, xmm10, xmm15
	VMOVDQA [byte rdx + 16], xmm8
	VPMOVZXWD xmm14, [byte rdi + 56]
	VPADDD xmm9, xmm9, xmm15
	VMOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	VPADDD xmm12, xmm12, xmm15
	VMOVDQA [byte rdx + 48], xmm9
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	VPADDD xmm13, xmm13, xmm15
	VMOVDQA [byte rdx + 64], xmm12
	VPADDD xmm11, xmm11, xmm15
	VMOVDQA [byte rdx + 80], xmm13
	VPADDD xmm14, xmm14, xmm15
	VMOVDQA [byte rdx + 96], xmm11
	VMOVDQA [byte rdx + 112], xmm14
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V16uS16u_V32u_Haswell
_yepCore_Add_V16uS16u_V32u_Haswell:
%else
section .text
global __yepCore_Add_V16uS16u_V32u_Haswell
__yepCore_Add_V16uS16u_V32u_Haswell:
%endif
	.ENTRY:
	MOVZX esi, si
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXWD ymm7, [rdi]
	VPMOVZXWD ymm8, [byte rdi + 16]
	VPMOVZXWD ymm15, [byte rdi + 32]
	VPMOVZXWD ymm9, [byte rdi + 48]
	VPMOVZXWD ymm13, [byte rdi + 64]
	VPADDD ymm7, ymm7, ymm14
	VPMOVZXWD ymm10, [byte rdi + 80]
	VPADDD ymm8, ymm8, ymm14
	VMOVDQA [rdx], ymm7
	VPMOVZXWD ymm11, [byte rdi + 96]
	VPADDD ymm15, ymm15, ymm14
	VMOVDQA [byte rdx + 32], ymm8
	VPMOVZXWD ymm12, [byte rdi + 112]
	VPADDD ymm9, ymm9, ymm14
	VMOVDQA [byte rdx + 64], ymm15
	ADD rdi, 128
	VPADDD ymm13, ymm13, ymm14
	VMOVDQA [byte rdx + 96], ymm9
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXWD ymm7, [rdi]
	VPADDD ymm10, ymm10, ymm14
	VMOVDQA [dword rdx + 128], ymm13
	VPMOVZXWD ymm8, [byte rdi + 16]
	VPADDD ymm11, ymm11, ymm14
	VMOVDQA [dword rdx + 160], ymm10
	VPMOVZXWD ymm15, [byte rdi + 32]
	VPADDD ymm12, ymm12, ymm14
	VMOVDQA [dword rdx + 192], ymm11
	VPMOVZXWD ymm9, [byte rdi + 48]
	VMOVDQA [dword rdx + 224], ymm12
	VPMOVZXWD ymm13, [byte rdi + 64]
	VPADDD ymm7, ymm7, ymm14
	ADD rdx, 256
	VPMOVZXWD ymm10, [byte rdi + 80]
	VPADDD ymm8, ymm8, ymm14
	VMOVDQA [rdx], ymm7
	VPMOVZXWD ymm11, [byte rdi + 96]
	VPADDD ymm15, ymm15, ymm14
	VMOVDQA [byte rdx + 32], ymm8
	VPMOVZXWD ymm12, [byte rdi + 112]
	VPADDD ymm9, ymm9, ymm14
	VMOVDQA [byte rdx + 64], ymm15
	ADD rdi, 128
	VPADDD ymm13, ymm13, ymm14
	VMOVDQA [byte rdx + 96], ymm9
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	VPADDD ymm10, ymm10, ymm14
	VMOVDQA [dword rdx + 128], ymm13
	VPADDD ymm11, ymm11, ymm14
	VMOVDQA [dword rdx + 160], ymm10
	VPADDD ymm12, ymm12, ymm14
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm12
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	ADD eax, esi
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.K10 progbits alloc exec nowrite align=16
global _yepCore_Add_V32uS32u_V64u_K10
_yepCore_Add_V32uS32u_V64u_K10:
%else
section .text
global __yepCore_Add_V32uS32u_V64u_K10
__yepCore_Add_V32uS32u_V64u_K10:
%endif
	.ENTRY:
	MOV esi, esi
	PXOR xmm14, xmm14
	MOVD xmm15, esi
	PUNPCKLQDQ xmm15, xmm15
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 14
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm7, [rdi]
	MOVQ xmm10, [byte rdi + 8]
	MOVQ xmm8, [byte rdi + 16]
	MOVQ xmm9, [byte rdi + 24]
	MOVQ xmm12, [byte rdi + 32]
	PUNPCKLDQ xmm7, xmm14
	MOVQ xmm13, [byte rdi + 40]
	PUNPCKLDQ xmm10, xmm14
	PADDQ xmm7, xmm15
	MOVQ xmm11, [byte rdi + 48]
	PUNPCKLDQ xmm8, xmm14
	PADDQ xmm10, xmm15
	MOVDQA [rdx], xmm7
	ADD rdi, 56
	PUNPCKLDQ xmm9, xmm14
	PADDQ xmm8, xmm15
	MOVDQA [byte rdx + 16], xmm10
	SUB rcx, 14
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm7, [rdi]
	PUNPCKLDQ xmm12, xmm14
	PADDQ xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm8
	MOVQ xmm10, [byte rdi + 8]
	PUNPCKLDQ xmm13, xmm14
	PADDQ xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	MOVQ xmm8, [byte rdi + 16]
	PUNPCKLDQ xmm11, xmm14
	PADDQ xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	MOVQ xmm9, [byte rdi + 24]
	PADDQ xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	MOVQ xmm12, [byte rdi + 32]
	PUNPCKLDQ xmm7, xmm14
	MOVDQA [byte rdx + 96], xmm11
	MOVQ xmm13, [byte rdi + 40]
	PUNPCKLDQ xmm10, xmm14
	PADDQ xmm7, xmm15
	ADD rdx, 112
	MOVQ xmm11, [byte rdi + 48]
	PUNPCKLDQ xmm8, xmm14
	PADDQ xmm10, xmm15
	MOVDQA [rdx], xmm7
	ADD rdi, 56
	PUNPCKLDQ xmm9, xmm14
	PADDQ xmm8, xmm15
	MOVDQA [byte rdx + 16], xmm10
	SUB rcx, 14
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLDQ xmm12, xmm14
	PADDQ xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm8
	PUNPCKLDQ xmm13, xmm14
	PADDQ xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	PUNPCKLDQ xmm11, xmm14
	PADDQ xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	PADDQ xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	MOVDQA [byte rdx + 96], xmm11
	ADD rdx, 112
	.batch_process_finish:
	ADD rcx, 14
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V32uS32u_V64u_Nehalem
_yepCore_Add_V32uS32u_V64u_Nehalem:
%else
section .text
global __yepCore_Add_V32uS32u_V64u_Nehalem
__yepCore_Add_V32uS32u_V64u_Nehalem:
%endif
	.ENTRY:
	MOV esi, esi
	MOVD xmm15, esi
	PUNPCKLQDQ xmm15, xmm15
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXDQ xmm7, [rdi]
	PMOVZXDQ xmm8, [byte rdi + 8]
	PMOVZXDQ xmm10, [byte rdi + 16]
	PMOVZXDQ xmm9, [byte rdi + 24]
	PMOVZXDQ xmm12, [byte rdi + 32]
	PADDQ xmm7, xmm15
	PMOVZXDQ xmm13, [byte rdi + 40]
	PADDQ xmm8, xmm15
	MOVDQA [rdx], xmm7
	PMOVZXDQ xmm11, [byte rdi + 48]
	PADDQ xmm10, xmm15
	MOVDQA [byte rdx + 16], xmm8
	PMOVZXDQ xmm14, [byte rdi + 56]
	PADDQ xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	PADDQ xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXDQ xmm7, [rdi]
	PADDQ xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	PMOVZXDQ xmm8, [byte rdi + 8]
	PADDQ xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	PMOVZXDQ xmm10, [byte rdi + 16]
	PADDQ xmm14, xmm15
	MOVDQA [byte rdx + 96], xmm11
	PMOVZXDQ xmm9, [byte rdi + 24]
	MOVDQA [byte rdx + 112], xmm14
	PMOVZXDQ xmm12, [byte rdi + 32]
	PADDQ xmm7, xmm15
	ADD rdx, 128
	PMOVZXDQ xmm13, [byte rdi + 40]
	PADDQ xmm8, xmm15
	MOVDQA [rdx], xmm7
	PMOVZXDQ xmm11, [byte rdi + 48]
	PADDQ xmm10, xmm15
	MOVDQA [byte rdx + 16], xmm8
	PMOVZXDQ xmm14, [byte rdi + 56]
	PADDQ xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	PADDQ xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	PADDQ xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	PADDQ xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	PADDQ xmm14, xmm15
	MOVDQA [byte rdx + 96], xmm11
	MOVDQA [byte rdx + 112], xmm14
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V32uS32u_V64u_SandyBridge
_yepCore_Add_V32uS32u_V64u_SandyBridge:
%else
section .text
global __yepCore_Add_V32uS32u_V64u_SandyBridge
__yepCore_Add_V32uS32u_V64u_SandyBridge:
%endif
	.ENTRY:
	MOV esi, esi
	MOVD xmm15, esi
	PUNPCKLQDQ xmm15, xmm15
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ xmm7, [rdi]
	VPMOVZXDQ xmm8, [byte rdi + 8]
	VPMOVZXDQ xmm10, [byte rdi + 16]
	VPMOVZXDQ xmm9, [byte rdi + 24]
	VPMOVZXDQ xmm12, [byte rdi + 32]
	VPADDQ xmm7, xmm7, xmm15
	VPMOVZXDQ xmm13, [byte rdi + 40]
	VPADDQ xmm8, xmm8, xmm15
	VMOVDQA [rdx], xmm7
	VPMOVZXDQ xmm11, [byte rdi + 48]
	VPADDQ xmm10, xmm10, xmm15
	VMOVDQA [byte rdx + 16], xmm8
	VPMOVZXDQ xmm14, [byte rdi + 56]
	VPADDQ xmm9, xmm9, xmm15
	VMOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	VPADDQ xmm12, xmm12, xmm15
	VMOVDQA [byte rdx + 48], xmm9
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ xmm7, [rdi]
	VPADDQ xmm13, xmm13, xmm15
	VMOVDQA [byte rdx + 64], xmm12
	VPMOVZXDQ xmm8, [byte rdi + 8]
	VPADDQ xmm11, xmm11, xmm15
	VMOVDQA [byte rdx + 80], xmm13
	VPMOVZXDQ xmm10, [byte rdi + 16]
	VPADDQ xmm14, xmm14, xmm15
	VMOVDQA [byte rdx + 96], xmm11
	VPMOVZXDQ xmm9, [byte rdi + 24]
	VMOVDQA [byte rdx + 112], xmm14
	VPMOVZXDQ xmm12, [byte rdi + 32]
	VPADDQ xmm7, xmm7, xmm15
	ADD rdx, 128
	VPMOVZXDQ xmm13, [byte rdi + 40]
	VPADDQ xmm8, xmm8, xmm15
	VMOVDQA [rdx], xmm7
	VPMOVZXDQ xmm11, [byte rdi + 48]
	VPADDQ xmm10, xmm10, xmm15
	VMOVDQA [byte rdx + 16], xmm8
	VPMOVZXDQ xmm14, [byte rdi + 56]
	VPADDQ xmm9, xmm9, xmm15
	VMOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	VPADDQ xmm12, xmm12, xmm15
	VMOVDQA [byte rdx + 48], xmm9
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	VPADDQ xmm13, xmm13, xmm15
	VMOVDQA [byte rdx + 64], xmm12
	VPADDQ xmm11, xmm11, xmm15
	VMOVDQA [byte rdx + 80], xmm13
	VPADDQ xmm14, xmm14, xmm15
	VMOVDQA [byte rdx + 96], xmm11
	VMOVDQA [byte rdx + 112], xmm14
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V32uS32u_V64u_Haswell
_yepCore_Add_V32uS32u_V64u_Haswell:
%else
section .text
global __yepCore_Add_V32uS32u_V64u_Haswell
__yepCore_Add_V32uS32u_V64u_Haswell:
%endif
	.ENTRY:
	MOV esi, esi
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ ymm7, [rdi]
	VPMOVZXDQ ymm8, [byte rdi + 16]
	VPMOVZXDQ ymm15, [byte rdi + 32]
	VPMOVZXDQ ymm9, [byte rdi + 48]
	VPMOVZXDQ ymm13, [byte rdi + 64]
	VPADDQ ymm7, ymm7, ymm14
	VPMOVZXDQ ymm10, [byte rdi + 80]
	VPADDQ ymm8, ymm8, ymm14
	VMOVDQA [rdx], ymm7
	VPMOVZXDQ ymm11, [byte rdi + 96]
	VPADDQ ymm15, ymm15, ymm14
	VMOVDQA [byte rdx + 32], ymm8
	VPMOVZXDQ ymm12, [byte rdi + 112]
	VPADDQ ymm9, ymm9, ymm14
	VMOVDQA [byte rdx + 64], ymm15
	ADD rdi, 128
	VPADDQ ymm13, ymm13, ymm14
	VMOVDQA [byte rdx + 96], ymm9
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ ymm7, [rdi]
	VPADDQ ymm10, ymm10, ymm14
	VMOVDQA [dword rdx + 128], ymm13
	VPMOVZXDQ ymm8, [byte rdi + 16]
	VPADDQ ymm11, ymm11, ymm14
	VMOVDQA [dword rdx + 160], ymm10
	VPMOVZXDQ ymm15, [byte rdi + 32]
	VPADDQ ymm12, ymm12, ymm14
	VMOVDQA [dword rdx + 192], ymm11
	VPMOVZXDQ ymm9, [byte rdi + 48]
	VMOVDQA [dword rdx + 224], ymm12
	VPMOVZXDQ ymm13, [byte rdi + 64]
	VPADDQ ymm7, ymm7, ymm14
	ADD rdx, 256
	VPMOVZXDQ ymm10, [byte rdi + 80]
	VPADDQ ymm8, ymm8, ymm14
	VMOVDQA [rdx], ymm7
	VPMOVZXDQ ymm11, [byte rdi + 96]
	VPADDQ ymm15, ymm15, ymm14
	VMOVDQA [byte rdx + 32], ymm8
	VPMOVZXDQ ymm12, [byte rdi + 112]
	VPADDQ ymm9, ymm9, ymm14
	VMOVDQA [byte rdx + 64], ymm15
	ADD rdi, 128
	VPADDQ ymm13, ymm13, ymm14
	VMOVDQA [byte rdx + 96], ymm9
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	VPADDQ ymm10, ymm10, ymm14
	VMOVDQA [dword rdx + 128], ymm13
	VPADDQ ymm11, ymm11, ymm14
	VMOVDQA [dword rdx + 160], ymm10
	VPADDQ ymm12, ymm12, ymm14
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm12
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.K10 progbits alloc exec nowrite align=16
global _yepCore_Add_V32sS32s_V64s_K10
_yepCore_Add_V32sS32s_V64s_K10:
%else
section .text
global __yepCore_Add_V32sS32s_V64s_K10
__yepCore_Add_V32sS32s_V64s_K10:
%endif
	.ENTRY:
	MOVSX rsi, esi
	MOVSX rax, esi
	MOVQ xmm5, rax
	PUNPCKLQDQ xmm5, xmm5
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 12
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm4, [rdi]
	PXOR xmm3, xmm3
	MOVQ xmm8, [byte rdi + 8]
	PXOR xmm7, xmm7
	MOVQ xmm11, [byte rdi + 16]
	PXOR xmm6, xmm6
	PCMPGTD xmm3, xmm4
	MOVQ xmm9, [byte rdi + 24]
	PXOR xmm13, xmm13
	PCMPGTD xmm7, xmm8
	PUNPCKLDQ xmm4, xmm3
	MOVQ xmm14, [byte rdi + 32]
	PXOR xmm10, xmm10
	PCMPGTD xmm6, xmm11
	PUNPCKLDQ xmm8, xmm7
	PADDQ xmm4, xmm5
	MOVQ xmm15, [byte rdi + 40]
	PXOR xmm12, xmm12
	PCMPGTD xmm13, xmm9
	PUNPCKLDQ xmm11, xmm6
	PADDQ xmm8, xmm5
	MOVDQA [rdx], xmm4
	ADD rdi, 48
	PCMPGTD xmm10, xmm14
	PUNPCKLDQ xmm9, xmm13
	PADDQ xmm11, xmm5
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 12
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm4, [rdi]
	PXOR xmm3, xmm3
	PCMPGTD xmm12, xmm15
	PUNPCKLDQ xmm14, xmm10
	PADDQ xmm9, xmm5
	MOVDQA [byte rdx + 32], xmm11
	MOVQ xmm8, [byte rdi + 8]
	PXOR xmm7, xmm7
	PUNPCKLDQ xmm15, xmm12
	PADDQ xmm14, xmm5
	MOVDQA [byte rdx + 48], xmm9
	MOVQ xmm11, [byte rdi + 16]
	PXOR xmm6, xmm6
	PCMPGTD xmm3, xmm4
	PADDQ xmm15, xmm5
	MOVDQA [byte rdx + 64], xmm14
	MOVQ xmm9, [byte rdi + 24]
	PXOR xmm13, xmm13
	PCMPGTD xmm7, xmm8
	PUNPCKLDQ xmm4, xmm3
	MOVDQA [byte rdx + 80], xmm15
	MOVQ xmm14, [byte rdi + 32]
	PXOR xmm10, xmm10
	PCMPGTD xmm6, xmm11
	PUNPCKLDQ xmm8, xmm7
	PADDQ xmm4, xmm5
	ADD rdx, 96
	MOVQ xmm15, [byte rdi + 40]
	PXOR xmm12, xmm12
	PCMPGTD xmm13, xmm9
	PUNPCKLDQ xmm11, xmm6
	PADDQ xmm8, xmm5
	MOVDQA [rdx], xmm4
	ADD rdi, 48
	PCMPGTD xmm10, xmm14
	PUNPCKLDQ xmm9, xmm13
	PADDQ xmm11, xmm5
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 12
	JAE .process_batch
	.process_batch_epilogue:
	PCMPGTD xmm12, xmm15
	PUNPCKLDQ xmm14, xmm10
	PADDQ xmm9, xmm5
	MOVDQA [byte rdx + 32], xmm11
	PUNPCKLDQ xmm15, xmm12
	PADDQ xmm14, xmm5
	MOVDQA [byte rdx + 48], xmm9
	PADDQ xmm15, xmm5
	MOVDQA [byte rdx + 64], xmm14
	MOVDQA [byte rdx + 80], xmm15
	ADD rdx, 96
	.batch_process_finish:
	ADD rcx, 12
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Add_V32sS32s_V64s_Nehalem
_yepCore_Add_V32sS32s_V64s_Nehalem:
%else
section .text
global __yepCore_Add_V32sS32s_V64s_Nehalem
__yepCore_Add_V32sS32s_V64s_Nehalem:
%endif
	.ENTRY:
	MOVSX rsi, esi
	MOVSX rax, esi
	MOVQ xmm15, rax
	PUNPCKLQDQ xmm15, xmm15
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVSXDQ xmm7, [rdi]
	PMOVSXDQ xmm8, [byte rdi + 8]
	PMOVSXDQ xmm10, [byte rdi + 16]
	PMOVSXDQ xmm9, [byte rdi + 24]
	PMOVSXDQ xmm12, [byte rdi + 32]
	PADDQ xmm7, xmm15
	PMOVSXDQ xmm13, [byte rdi + 40]
	PADDQ xmm8, xmm15
	MOVDQA [rdx], xmm7
	PMOVSXDQ xmm11, [byte rdi + 48]
	PADDQ xmm10, xmm15
	MOVDQA [byte rdx + 16], xmm8
	PMOVSXDQ xmm14, [byte rdi + 56]
	PADDQ xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	PADDQ xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVSXDQ xmm7, [rdi]
	PADDQ xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	PMOVSXDQ xmm8, [byte rdi + 8]
	PADDQ xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	PMOVSXDQ xmm10, [byte rdi + 16]
	PADDQ xmm14, xmm15
	MOVDQA [byte rdx + 96], xmm11
	PMOVSXDQ xmm9, [byte rdi + 24]
	MOVDQA [byte rdx + 112], xmm14
	PMOVSXDQ xmm12, [byte rdi + 32]
	PADDQ xmm7, xmm15
	ADD rdx, 128
	PMOVSXDQ xmm13, [byte rdi + 40]
	PADDQ xmm8, xmm15
	MOVDQA [rdx], xmm7
	PMOVSXDQ xmm11, [byte rdi + 48]
	PADDQ xmm10, xmm15
	MOVDQA [byte rdx + 16], xmm8
	PMOVSXDQ xmm14, [byte rdi + 56]
	PADDQ xmm9, xmm15
	MOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	PADDQ xmm12, xmm15
	MOVDQA [byte rdx + 48], xmm9
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	PADDQ xmm13, xmm15
	MOVDQA [byte rdx + 64], xmm12
	PADDQ xmm11, xmm15
	MOVDQA [byte rdx + 80], xmm13
	PADDQ xmm14, xmm15
	MOVDQA [byte rdx + 96], xmm11
	MOVDQA [byte rdx + 112], xmm14
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Add_V32sS32s_V64s_SandyBridge
_yepCore_Add_V32sS32s_V64s_SandyBridge:
%else
section .text
global __yepCore_Add_V32sS32s_V64s_SandyBridge
__yepCore_Add_V32sS32s_V64s_SandyBridge:
%endif
	.ENTRY:
	MOVSX rsi, esi
	MOVSX rax, esi
	MOVQ xmm15, rax
	PUNPCKLQDQ xmm15, xmm15
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXDQ xmm7, [rdi]
	VPMOVSXDQ xmm8, [byte rdi + 8]
	VPMOVSXDQ xmm10, [byte rdi + 16]
	VPMOVSXDQ xmm9, [byte rdi + 24]
	VPMOVSXDQ xmm12, [byte rdi + 32]
	VPADDQ xmm7, xmm7, xmm15
	VPMOVSXDQ xmm13, [byte rdi + 40]
	VPADDQ xmm8, xmm8, xmm15
	VMOVDQA [rdx], xmm7
	VPMOVSXDQ xmm11, [byte rdi + 48]
	VPADDQ xmm10, xmm10, xmm15
	VMOVDQA [byte rdx + 16], xmm8
	VPMOVSXDQ xmm14, [byte rdi + 56]
	VPADDQ xmm9, xmm9, xmm15
	VMOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	VPADDQ xmm12, xmm12, xmm15
	VMOVDQA [byte rdx + 48], xmm9
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXDQ xmm7, [rdi]
	VPADDQ xmm13, xmm13, xmm15
	VMOVDQA [byte rdx + 64], xmm12
	VPMOVSXDQ xmm8, [byte rdi + 8]
	VPADDQ xmm11, xmm11, xmm15
	VMOVDQA [byte rdx + 80], xmm13
	VPMOVSXDQ xmm10, [byte rdi + 16]
	VPADDQ xmm14, xmm14, xmm15
	VMOVDQA [byte rdx + 96], xmm11
	VPMOVSXDQ xmm9, [byte rdi + 24]
	VMOVDQA [byte rdx + 112], xmm14
	VPMOVSXDQ xmm12, [byte rdi + 32]
	VPADDQ xmm7, xmm7, xmm15
	ADD rdx, 128
	VPMOVSXDQ xmm13, [byte rdi + 40]
	VPADDQ xmm8, xmm8, xmm15
	VMOVDQA [rdx], xmm7
	VPMOVSXDQ xmm11, [byte rdi + 48]
	VPADDQ xmm10, xmm10, xmm15
	VMOVDQA [byte rdx + 16], xmm8
	VPMOVSXDQ xmm14, [byte rdi + 56]
	VPADDQ xmm9, xmm9, xmm15
	VMOVDQA [byte rdx + 32], xmm10
	ADD rdi, 64
	VPADDQ xmm12, xmm12, xmm15
	VMOVDQA [byte rdx + 48], xmm9
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	VPADDQ xmm13, xmm13, xmm15
	VMOVDQA [byte rdx + 64], xmm12
	VPADDQ xmm11, xmm11, xmm15
	VMOVDQA [byte rdx + 80], xmm13
	VPADDQ xmm14, xmm14, xmm15
	VMOVDQA [byte rdx + 96], xmm11
	VMOVDQA [byte rdx + 112], xmm14
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Add_V32sS32s_V64s_Haswell
_yepCore_Add_V32sS32s_V64s_Haswell:
%else
section .text
global __yepCore_Add_V32sS32s_V64s_Haswell
__yepCore_Add_V32sS32s_V64s_Haswell:
%endif
	.ENTRY:
	MOVSX rsi, esi
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXDQ ymm7, [rdi]
	VPMOVSXDQ ymm8, [byte rdi + 16]
	VPMOVSXDQ ymm15, [byte rdi + 32]
	VPMOVSXDQ ymm9, [byte rdi + 48]
	VPMOVSXDQ ymm13, [byte rdi + 64]
	VPADDQ ymm7, ymm7, ymm14
	VPMOVSXDQ ymm10, [byte rdi + 80]
	VPADDQ ymm8, ymm8, ymm14
	VMOVDQA [rdx], ymm7
	VPMOVSXDQ ymm11, [byte rdi + 96]
	VPADDQ ymm15, ymm15, ymm14
	VMOVDQA [byte rdx + 32], ymm8
	VPMOVSXDQ ymm12, [byte rdi + 112]
	VPADDQ ymm9, ymm9, ymm14
	VMOVDQA [byte rdx + 64], ymm15
	ADD rdi, 128
	VPADDQ ymm13, ymm13, ymm14
	VMOVDQA [byte rdx + 96], ymm9
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXDQ ymm7, [rdi]
	VPADDQ ymm10, ymm10, ymm14
	VMOVDQA [dword rdx + 128], ymm13
	VPMOVSXDQ ymm8, [byte rdi + 16]
	VPADDQ ymm11, ymm11, ymm14
	VMOVDQA [dword rdx + 160], ymm10
	VPMOVSXDQ ymm15, [byte rdi + 32]
	VPADDQ ymm12, ymm12, ymm14
	VMOVDQA [dword rdx + 192], ymm11
	VPMOVSXDQ ymm9, [byte rdi + 48]
	VMOVDQA [dword rdx + 224], ymm12
	VPMOVSXDQ ymm13, [byte rdi + 64]
	VPADDQ ymm7, ymm7, ymm14
	ADD rdx, 256
	VPMOVSXDQ ymm10, [byte rdi + 80]
	VPADDQ ymm8, ymm8, ymm14
	VMOVDQA [rdx], ymm7
	VPMOVSXDQ ymm11, [byte rdi + 96]
	VPADDQ ymm15, ymm15, ymm14
	VMOVDQA [byte rdx + 32], ymm8
	VPMOVSXDQ ymm12, [byte rdi + 112]
	VPADDQ ymm9, ymm9, ymm14
	VMOVDQA [byte rdx + 64], ymm15
	ADD rdi, 128
	VPADDQ ymm13, ymm13, ymm14
	VMOVDQA [byte rdx + 96], ymm9
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	VPADDQ ymm10, ymm10, ymm14
	VMOVDQA [dword rdx + 128], ymm13
	VPADDQ ymm11, ymm11, ymm14
	VMOVDQA [dword rdx + 160], ymm10
	VPADDQ ymm12, ymm12, ymm14
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm12
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	ADD rax, rsi
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return
