;                       Yeppp! library implementation
;                   This file is auto-generated by Peach-Py,
;        Portable Efficient Assembly Code-generator in Higher-level Python,
;                  part of the Yeppp! library infrastructure
; This file is part of Yeppp! library and licensed under the New BSD license.
; See LICENSE.txt for the full text of the license.

section .text$e code align=16
global _yepCore_Multiply_V16sV16s_V16s_Nehalem
_yepCore_Multiply_V16sV16s_V16s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm15, [rcx]
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	PMULLW xmm15, xmm14
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PMULLW xmm4, xmm0
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PMULLW xmm5, xmm3
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PMULLW xmm1, xmm12
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm15, [rcx]
	ADD rdx, 128
	PMULLW xmm6, xmm9
	MOVDQA [byte r8 + 32], xmm5
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	PMULLW xmm2, xmm8
	MOVDQA [byte r8 + 48], xmm1
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	PMULLW xmm11, xmm10
	MOVDQA [byte r8 + 64], xmm6
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	PMULLW xmm13, xmm7
	MOVDQA [byte r8 + 80], xmm2
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	MOVDQA [byte r8 + 96], xmm11
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	PMULLW xmm15, xmm14
	MOVDQA [byte r8 + 112], xmm13
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PMULLW xmm4, xmm0
	ADD r8, 128
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PMULLW xmm5, xmm3
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PMULLW xmm1, xmm12
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	PMULLW xmm6, xmm9
	MOVDQA [byte r8 + 32], xmm5
	PMULLW xmm2, xmm8
	MOVDQA [byte r8 + 48], xmm1
	PMULLW xmm11, xmm10
	MOVDQA [byte r8 + 64], xmm6
	PMULLW xmm13, xmm7
	MOVDQA [byte r8 + 80], xmm2
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Multiply_V16sV16s_V16s_SandyBridge
_yepCore_Multiply_V16sV16s_V16s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm15, [rcx]
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VPMULLW xmm15, xmm15, xmm14
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPMULLW xmm4, xmm4, xmm0
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPMULLW xmm5, xmm5, xmm3
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPMULLW xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm15, [rcx]
	ADD rdx, 128
	VPMULLW xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 32], xmm5
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VPMULLW xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 48], xmm1
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VPMULLW xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 64], xmm6
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VPMULLW xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 80], xmm2
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VPMULLW xmm15, xmm15, xmm14
	VMOVDQA [byte r8 + 112], xmm13
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPMULLW xmm4, xmm4, xmm0
	ADD r8, 128
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPMULLW xmm5, xmm5, xmm3
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPMULLW xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPMULLW xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 32], xmm5
	VPMULLW xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 48], xmm1
	VPMULLW xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 64], xmm6
	VPMULLW xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 80], xmm2
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Multiply_V16sV16s_V16s_Haswell
_yepCore_Multiply_V16sV16s_V16s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm15, [rcx]
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VPMULLW ymm15, ymm15, ymm8
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPMULLW ymm4, ymm4, ymm10
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPMULLW ymm5, ymm5, ymm12
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPMULLW ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm15, [rcx]
	ADD rdx, 256
	VPMULLW ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 64], ymm5
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VPMULLW ymm1, ymm1, ymm3
	VMOVDQA [byte r8 + 96], ymm14
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VPMULLW ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 128], ymm6
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VPMULLW ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 160], ymm1
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VPMULLW ymm15, ymm15, ymm8
	VMOVDQA [dword r8 + 224], ymm0
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPMULLW ymm4, ymm4, ymm10
	ADD r8, 256
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPMULLW ymm5, ymm5, ymm12
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPMULLW ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 256
	VPMULLW ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 64], ymm5
	VPMULLW ymm1, ymm1, ymm3
	VMOVDQA [byte r8 + 96], ymm14
	VPMULLW ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 128], ymm6
	VPMULLW ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 160], ymm1
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Multiply_V16sV16s_V32s_Nehalem
_yepCore_Multiply_V16sV16s_V32s_Nehalem:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm11
	MOVAPS [byte rsp + 16], xmm12
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm10
	MOVAPS [byte rsp + 64], xmm8
	MOVAPS [byte rsp + 80], xmm13
	MOVAPS [byte rsp + 96], xmm14
	MOVAPS [byte rsp + 112], xmm9
	MOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm11, [rcx]
	MOVDQU xmm12, [rdx]
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm0, [byte rdx + 16]
	MOVDQA xmm6, xmm11
	PMULLW xmm11, xmm12
	PMULHW xmm6, xmm12
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm3, [byte rdx + 32]
	MOVDQA xmm2, xmm4
	PMULLW xmm4, xmm0
	PMULHW xmm2, xmm0
	MOVDQA xmm10, xmm11
	PUNPCKLWD xmm11, xmm6
	PUNPCKHWD xmm10, xmm6
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm8, [byte rdx + 48]
	MOVDQA xmm13, xmm5
	PMULLW xmm5, xmm3
	PMULHW xmm13, xmm3
	MOVDQA xmm14, xmm4
	PUNPCKLWD xmm4, xmm2
	PUNPCKHWD xmm14, xmm2
	MOVDQA [r8], xmm11
	MOVDQA [byte r8 + 16], xmm10
	ADD rcx, 64
	ADD rdx, 64
	MOVDQA xmm9, xmm1
	PMULLW xmm1, xmm8
	PMULHW xmm9, xmm8
	MOVDQA xmm8, xmm5
	PUNPCKLWD xmm5, xmm13
	PUNPCKHWD xmm8, xmm13
	MOVDQA [byte r8 + 32], xmm4
	MOVDQA [byte r8 + 48], xmm14
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm11, [rcx]
	MOVDQU xmm12, [rdx]
	MOVDQA xmm7, xmm1
	PUNPCKLWD xmm1, xmm9
	PUNPCKHWD xmm7, xmm9
	MOVDQA [byte r8 + 64], xmm5
	MOVDQA [byte r8 + 80], xmm8
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm0, [byte rdx + 16]
	MOVDQA xmm6, xmm11
	PMULLW xmm11, xmm12
	PMULHW xmm6, xmm12
	MOVDQA [byte r8 + 96], xmm1
	MOVDQA [byte r8 + 112], xmm7
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm3, [byte rdx + 32]
	MOVDQA xmm2, xmm4
	PMULLW xmm4, xmm0
	PMULHW xmm2, xmm0
	MOVDQA xmm10, xmm11
	PUNPCKLWD xmm11, xmm6
	PUNPCKHWD xmm10, xmm6
	ADD r8, 128
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm8, [byte rdx + 48]
	MOVDQA xmm13, xmm5
	PMULLW xmm5, xmm3
	PMULHW xmm13, xmm3
	MOVDQA xmm14, xmm4
	PUNPCKLWD xmm4, xmm2
	PUNPCKHWD xmm14, xmm2
	MOVDQA [r8], xmm11
	MOVDQA [byte r8 + 16], xmm10
	ADD rcx, 64
	ADD rdx, 64
	MOVDQA xmm9, xmm1
	PMULLW xmm1, xmm8
	PMULHW xmm9, xmm8
	MOVDQA xmm8, xmm5
	PUNPCKLWD xmm5, xmm13
	PUNPCKHWD xmm8, xmm13
	MOVDQA [byte r8 + 32], xmm4
	MOVDQA [byte r8 + 48], xmm14
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	MOVDQA xmm7, xmm1
	PUNPCKLWD xmm1, xmm9
	PUNPCKHWD xmm7, xmm9
	MOVDQA [byte r8 + 64], xmm5
	MOVDQA [byte r8 + 80], xmm8
	MOVDQA [byte r8 + 96], xmm1
	MOVDQA [byte r8 + 112], xmm7
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm11, [rsp]
	MOVAPS xmm12, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm10, [byte rsp + 48]
	MOVAPS xmm8, [byte rsp + 64]
	MOVAPS xmm13, [byte rsp + 80]
	MOVAPS xmm14, [byte rsp + 96]
	MOVAPS xmm9, [byte rsp + 112]
	MOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$n code align=16
global _yepCore_Multiply_V16sV16s_V32s_Bulldozer
_yepCore_Multiply_V16sV16s_V32s_Bulldozer:
	.ENTRY:
	SUB rsp, 104
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 24
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm6, [rcx]
	VMOVDQU xmm7, [rdx]
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm5, [byte rdx + 16]
	VPMULLW xmm0, xmm6, xmm7
	VPMULHW xmm1, xmm6, xmm7
	VMOVDQU xmm2, [byte rcx + 32]
	VMOVDQU xmm8, [byte rdx + 32]
	VPMULLW xmm9, xmm4, xmm5
	VPMULHW xmm10, xmm4, xmm5
	VPUNPCKLWD xmm3, xmm0, xmm1
	VPUNPCKHWD xmm11, xmm0, xmm1
	VMOVDQA [r8], xmm3
	VMOVDQA [byte r8 + 16], xmm11
	ADD rcx, 48
	ADD rdx, 48
	VPMULLW xmm3, xmm2, xmm8
	VPMULHW xmm2, xmm2, xmm8
	VPUNPCKLWD xmm8, xmm9, xmm10
	VPUNPCKHWD xmm9, xmm9, xmm10
	VMOVDQA [byte r8 + 32], xmm8
	VMOVDQA [byte r8 + 48], xmm9
	SUB r9, 24
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm6, [rcx]
	VMOVDQU xmm7, [rdx]
	VPUNPCKLWD xmm8, xmm3, xmm2
	VPUNPCKHWD xmm3, xmm3, xmm2
	VMOVDQA [byte r8 + 64], xmm8
	VMOVDQA [byte r8 + 80], xmm3
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm5, [byte rdx + 16]
	VPMULLW xmm0, xmm6, xmm7
	VPMULHW xmm1, xmm6, xmm7
	ADD r8, 96
	VMOVDQU xmm2, [byte rcx + 32]
	VMOVDQU xmm8, [byte rdx + 32]
	VPMULLW xmm9, xmm4, xmm5
	VPMULHW xmm10, xmm4, xmm5
	VPUNPCKLWD xmm3, xmm0, xmm1
	VPUNPCKHWD xmm11, xmm0, xmm1
	VMOVDQA [r8], xmm3
	VMOVDQA [byte r8 + 16], xmm11
	ADD rcx, 48
	ADD rdx, 48
	VPMULLW xmm3, xmm2, xmm8
	VPMULHW xmm2, xmm2, xmm8
	VPUNPCKLWD xmm8, xmm9, xmm10
	VPUNPCKHWD xmm9, xmm9, xmm10
	VMOVDQA [byte r8 + 32], xmm8
	VMOVDQA [byte r8 + 48], xmm9
	SUB r9, 24
	JAE .process_batch
	.process_batch_epilogue:
	VPUNPCKLWD xmm8, xmm3, xmm2
	VPUNPCKHWD xmm3, xmm3, xmm2
	VMOVDQA [byte r8 + 64], xmm8
	VMOVDQA [byte r8 + 80], xmm3
	ADD r8, 96
	.batch_process_finish:
	ADD r9, 24
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	ADD rsp, 104
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Multiply_V16sV16s_V32s_SandyBridge
_yepCore_Multiply_V16sV16s_V32s_SandyBridge:
	.ENTRY:
	SUB rsp, 152
	VMOVAPS [rsp], xmm13
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm11
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 20
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXWD xmm13, [rcx]
	VPMOVSXWD xmm14, [rdx]
	VPMOVSXWD xmm4, [byte rcx + 8]
	VPMOVSXWD xmm2, [byte rdx + 8]
	VPMOVSXWD xmm5, [byte rcx + 16]
	VPMOVSXWD xmm11, [byte rdx + 16]
	VPMULLD xmm3, xmm13, xmm14
	VPMOVSXWD xmm1, [byte rcx + 24]
	VPMOVSXWD xmm12, [byte rdx + 24]
	VPMULLD xmm10, xmm4, xmm2
	VPMOVSXWD xmm6, [byte rcx + 32]
	VPMOVSXWD xmm0, [byte rdx + 32]
	VPMULLD xmm8, xmm5, xmm11
	VMOVDQA [r8], xmm3
	ADD rcx, 40
	ADD rdx, 40
	VPMULLD xmm7, xmm1, xmm12
	VMOVDQA [byte r8 + 16], xmm10
	SUB r9, 20
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXWD xmm13, [rcx]
	VPMOVSXWD xmm14, [rdx]
	VPMULLD xmm9, xmm6, xmm0
	VMOVDQA [byte r8 + 32], xmm8
	VPMOVSXWD xmm4, [byte rcx + 8]
	VPMOVSXWD xmm2, [byte rdx + 8]
	VMOVDQA [byte r8 + 48], xmm7
	VPMOVSXWD xmm5, [byte rcx + 16]
	VPMOVSXWD xmm11, [byte rdx + 16]
	VPMULLD xmm3, xmm13, xmm14
	VMOVDQA [byte r8 + 64], xmm9
	VPMOVSXWD xmm1, [byte rcx + 24]
	VPMOVSXWD xmm12, [byte rdx + 24]
	VPMULLD xmm10, xmm4, xmm2
	ADD r8, 80
	VPMOVSXWD xmm6, [byte rcx + 32]
	VPMOVSXWD xmm0, [byte rdx + 32]
	VPMULLD xmm8, xmm5, xmm11
	VMOVDQA [r8], xmm3
	ADD rcx, 40
	ADD rdx, 40
	VPMULLD xmm7, xmm1, xmm12
	VMOVDQA [byte r8 + 16], xmm10
	SUB r9, 20
	JAE .process_batch
	.process_batch_epilogue:
	VPMULLD xmm9, xmm6, xmm0
	VMOVDQA [byte r8 + 32], xmm8
	VMOVDQA [byte r8 + 48], xmm7
	VMOVDQA [byte r8 + 64], xmm9
	ADD r8, 80
	.batch_process_finish:
	ADD r9, 20
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm13, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm11, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm9, [dword rsp + 128]
	ADD rsp, 152
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Multiply_V16sV16s_V32s_Haswell
_yepCore_Multiply_V16sV16s_V32s_Haswell:
	.ENTRY:
	SUB rsp, 152
	VMOVAPS [rsp], xmm13
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm8
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm6
	VMOVAPS [byte rsp + 112], xmm9
	VMOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 40
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXWD ymm13, [rcx]
	VPMOVSXWD ymm14, [rdx]
	VPMOVSXWD ymm4, [byte rcx + 16]
	VPMOVSXWD ymm1, [byte rdx + 16]
	VPMOVSXWD ymm5, [byte rcx + 32]
	VPMOVSXWD ymm0, [byte rdx + 32]
	VPMULLD ymm10, ymm13, ymm14
	VPMOVSXWD ymm12, [byte rcx + 48]
	VPMOVSXWD ymm8, [byte rdx + 48]
	VPMULLD ymm11, ymm4, ymm1
	VPMOVSXWD ymm6, [byte rcx + 64]
	VPMOVSXWD ymm9, [byte rdx + 64]
	VPMULLD ymm7, ymm5, ymm0
	VMOVDQA [r8], ymm10
	ADD rcx, 80
	ADD rdx, 80
	VPMULLD ymm3, ymm12, ymm8
	VMOVDQA [byte r8 + 32], ymm11
	SUB r9, 40
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXWD ymm13, [rcx]
	VPMOVSXWD ymm14, [rdx]
	VPMULLD ymm2, ymm6, ymm9
	VMOVDQA [byte r8 + 64], ymm7
	VPMOVSXWD ymm4, [byte rcx + 16]
	VPMOVSXWD ymm1, [byte rdx + 16]
	VMOVDQA [byte r8 + 96], ymm3
	VPMOVSXWD ymm5, [byte rcx + 32]
	VPMOVSXWD ymm0, [byte rdx + 32]
	VPMULLD ymm10, ymm13, ymm14
	VMOVDQA [dword r8 + 128], ymm2
	VPMOVSXWD ymm12, [byte rcx + 48]
	VPMOVSXWD ymm8, [byte rdx + 48]
	VPMULLD ymm11, ymm4, ymm1
	ADD r8, 160
	VPMOVSXWD ymm6, [byte rcx + 64]
	VPMOVSXWD ymm9, [byte rdx + 64]
	VPMULLD ymm7, ymm5, ymm0
	VMOVDQA [r8], ymm10
	ADD rcx, 80
	ADD rdx, 80
	VPMULLD ymm3, ymm12, ymm8
	VMOVDQA [byte r8 + 32], ymm11
	SUB r9, 40
	JAE .process_batch
	.process_batch_epilogue:
	VPMULLD ymm2, ymm6, ymm9
	VMOVDQA [byte r8 + 64], ymm7
	VMOVDQA [byte r8 + 96], ymm3
	VMOVDQA [dword r8 + 128], ymm2
	ADD r8, 160
	.batch_process_finish:
	ADD r9, 40
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm13, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm8, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm6, [byte rsp + 96]
	VMOVAPS xmm9, [byte rsp + 112]
	VMOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Multiply_V16uV16u_V32u_Nehalem
_yepCore_Multiply_V16uV16u_V32u_Nehalem:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm11
	MOVAPS [byte rsp + 16], xmm12
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm10
	MOVAPS [byte rsp + 64], xmm8
	MOVAPS [byte rsp + 80], xmm13
	MOVAPS [byte rsp + 96], xmm14
	MOVAPS [byte rsp + 112], xmm9
	MOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm11, [rcx]
	MOVDQU xmm12, [rdx]
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm0, [byte rdx + 16]
	MOVDQA xmm6, xmm11
	PMULLW xmm11, xmm12
	PMULHUW xmm6, xmm12
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm3, [byte rdx + 32]
	MOVDQA xmm2, xmm4
	PMULLW xmm4, xmm0
	PMULHUW xmm2, xmm0
	MOVDQA xmm10, xmm11
	PUNPCKLWD xmm11, xmm6
	PUNPCKHWD xmm10, xmm6
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm8, [byte rdx + 48]
	MOVDQA xmm13, xmm5
	PMULLW xmm5, xmm3
	PMULHUW xmm13, xmm3
	MOVDQA xmm14, xmm4
	PUNPCKLWD xmm4, xmm2
	PUNPCKHWD xmm14, xmm2
	MOVDQA [r8], xmm11
	MOVDQA [byte r8 + 16], xmm10
	ADD rcx, 64
	ADD rdx, 64
	MOVDQA xmm9, xmm1
	PMULLW xmm1, xmm8
	PMULHUW xmm9, xmm8
	MOVDQA xmm8, xmm5
	PUNPCKLWD xmm5, xmm13
	PUNPCKHWD xmm8, xmm13
	MOVDQA [byte r8 + 32], xmm4
	MOVDQA [byte r8 + 48], xmm14
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm11, [rcx]
	MOVDQU xmm12, [rdx]
	MOVDQA xmm7, xmm1
	PUNPCKLWD xmm1, xmm9
	PUNPCKHWD xmm7, xmm9
	MOVDQA [byte r8 + 64], xmm5
	MOVDQA [byte r8 + 80], xmm8
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm0, [byte rdx + 16]
	MOVDQA xmm6, xmm11
	PMULLW xmm11, xmm12
	PMULHUW xmm6, xmm12
	MOVDQA [byte r8 + 96], xmm1
	MOVDQA [byte r8 + 112], xmm7
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm3, [byte rdx + 32]
	MOVDQA xmm2, xmm4
	PMULLW xmm4, xmm0
	PMULHUW xmm2, xmm0
	MOVDQA xmm10, xmm11
	PUNPCKLWD xmm11, xmm6
	PUNPCKHWD xmm10, xmm6
	ADD r8, 128
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm8, [byte rdx + 48]
	MOVDQA xmm13, xmm5
	PMULLW xmm5, xmm3
	PMULHUW xmm13, xmm3
	MOVDQA xmm14, xmm4
	PUNPCKLWD xmm4, xmm2
	PUNPCKHWD xmm14, xmm2
	MOVDQA [r8], xmm11
	MOVDQA [byte r8 + 16], xmm10
	ADD rcx, 64
	ADD rdx, 64
	MOVDQA xmm9, xmm1
	PMULLW xmm1, xmm8
	PMULHUW xmm9, xmm8
	MOVDQA xmm8, xmm5
	PUNPCKLWD xmm5, xmm13
	PUNPCKHWD xmm8, xmm13
	MOVDQA [byte r8 + 32], xmm4
	MOVDQA [byte r8 + 48], xmm14
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	MOVDQA xmm7, xmm1
	PUNPCKLWD xmm1, xmm9
	PUNPCKHWD xmm7, xmm9
	MOVDQA [byte r8 + 64], xmm5
	MOVDQA [byte r8 + 80], xmm8
	MOVDQA [byte r8 + 96], xmm1
	MOVDQA [byte r8 + 112], xmm7
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm11, [rsp]
	MOVAPS xmm12, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm10, [byte rsp + 48]
	MOVAPS xmm8, [byte rsp + 64]
	MOVAPS xmm13, [byte rsp + 80]
	MOVAPS xmm14, [byte rsp + 96]
	MOVAPS xmm9, [byte rsp + 112]
	MOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$n code align=16
global _yepCore_Multiply_V16uV16u_V32u_Bulldozer
_yepCore_Multiply_V16uV16u_V32u_Bulldozer:
	.ENTRY:
	SUB rsp, 104
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 24
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm6, [rcx]
	VMOVDQU xmm7, [rdx]
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm5, [byte rdx + 16]
	VPMULLW xmm0, xmm6, xmm7
	VPMULHUW xmm1, xmm6, xmm7
	VMOVDQU xmm2, [byte rcx + 32]
	VMOVDQU xmm8, [byte rdx + 32]
	VPMULLW xmm9, xmm4, xmm5
	VPMULHUW xmm10, xmm4, xmm5
	VPUNPCKLWD xmm3, xmm0, xmm1
	VPUNPCKHWD xmm11, xmm0, xmm1
	VMOVDQA [r8], xmm3
	VMOVDQA [byte r8 + 16], xmm11
	ADD rcx, 48
	ADD rdx, 48
	VPMULLW xmm3, xmm2, xmm8
	VPMULHUW xmm2, xmm2, xmm8
	VPUNPCKLWD xmm8, xmm9, xmm10
	VPUNPCKHWD xmm9, xmm9, xmm10
	VMOVDQA [byte r8 + 32], xmm8
	VMOVDQA [byte r8 + 48], xmm9
	SUB r9, 24
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm6, [rcx]
	VMOVDQU xmm7, [rdx]
	VPUNPCKLWD xmm8, xmm3, xmm2
	VPUNPCKHWD xmm3, xmm3, xmm2
	VMOVDQA [byte r8 + 64], xmm8
	VMOVDQA [byte r8 + 80], xmm3
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm5, [byte rdx + 16]
	VPMULLW xmm0, xmm6, xmm7
	VPMULHUW xmm1, xmm6, xmm7
	ADD r8, 96
	VMOVDQU xmm2, [byte rcx + 32]
	VMOVDQU xmm8, [byte rdx + 32]
	VPMULLW xmm9, xmm4, xmm5
	VPMULHUW xmm10, xmm4, xmm5
	VPUNPCKLWD xmm3, xmm0, xmm1
	VPUNPCKHWD xmm11, xmm0, xmm1
	VMOVDQA [r8], xmm3
	VMOVDQA [byte r8 + 16], xmm11
	ADD rcx, 48
	ADD rdx, 48
	VPMULLW xmm3, xmm2, xmm8
	VPMULHUW xmm2, xmm2, xmm8
	VPUNPCKLWD xmm8, xmm9, xmm10
	VPUNPCKHWD xmm9, xmm9, xmm10
	VMOVDQA [byte r8 + 32], xmm8
	VMOVDQA [byte r8 + 48], xmm9
	SUB r9, 24
	JAE .process_batch
	.process_batch_epilogue:
	VPUNPCKLWD xmm8, xmm3, xmm2
	VPUNPCKHWD xmm3, xmm3, xmm2
	VMOVDQA [byte r8 + 64], xmm8
	VMOVDQA [byte r8 + 80], xmm3
	ADD r8, 96
	.batch_process_finish:
	ADD r9, 24
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	ADD rsp, 104
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Multiply_V16uV16u_V32u_SandyBridge
_yepCore_Multiply_V16uV16u_V32u_SandyBridge:
	.ENTRY:
	SUB rsp, 152
	VMOVAPS [rsp], xmm13
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm11
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 20
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXWD xmm13, [rcx]
	VPMOVZXWD xmm14, [rdx]
	VPMOVZXWD xmm4, [byte rcx + 8]
	VPMOVZXWD xmm2, [byte rdx + 8]
	VPMOVZXWD xmm5, [byte rcx + 16]
	VPMOVZXWD xmm11, [byte rdx + 16]
	VPMULLD xmm3, xmm13, xmm14
	VPMOVZXWD xmm1, [byte rcx + 24]
	VPMOVZXWD xmm12, [byte rdx + 24]
	VPMULLD xmm10, xmm4, xmm2
	VPMOVZXWD xmm6, [byte rcx + 32]
	VPMOVZXWD xmm0, [byte rdx + 32]
	VPMULLD xmm8, xmm5, xmm11
	VMOVDQA [r8], xmm3
	ADD rcx, 40
	ADD rdx, 40
	VPMULLD xmm7, xmm1, xmm12
	VMOVDQA [byte r8 + 16], xmm10
	SUB r9, 20
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXWD xmm13, [rcx]
	VPMOVZXWD xmm14, [rdx]
	VPMULLD xmm9, xmm6, xmm0
	VMOVDQA [byte r8 + 32], xmm8
	VPMOVZXWD xmm4, [byte rcx + 8]
	VPMOVZXWD xmm2, [byte rdx + 8]
	VMOVDQA [byte r8 + 48], xmm7
	VPMOVZXWD xmm5, [byte rcx + 16]
	VPMOVZXWD xmm11, [byte rdx + 16]
	VPMULLD xmm3, xmm13, xmm14
	VMOVDQA [byte r8 + 64], xmm9
	VPMOVZXWD xmm1, [byte rcx + 24]
	VPMOVZXWD xmm12, [byte rdx + 24]
	VPMULLD xmm10, xmm4, xmm2
	ADD r8, 80
	VPMOVZXWD xmm6, [byte rcx + 32]
	VPMOVZXWD xmm0, [byte rdx + 32]
	VPMULLD xmm8, xmm5, xmm11
	VMOVDQA [r8], xmm3
	ADD rcx, 40
	ADD rdx, 40
	VPMULLD xmm7, xmm1, xmm12
	VMOVDQA [byte r8 + 16], xmm10
	SUB r9, 20
	JAE .process_batch
	.process_batch_epilogue:
	VPMULLD xmm9, xmm6, xmm0
	VMOVDQA [byte r8 + 32], xmm8
	VMOVDQA [byte r8 + 48], xmm7
	VMOVDQA [byte r8 + 64], xmm9
	ADD r8, 80
	.batch_process_finish:
	ADD r9, 20
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm13, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm11, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm9, [dword rsp + 128]
	ADD rsp, 152
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Multiply_V16uV16u_V32u_Haswell
_yepCore_Multiply_V16uV16u_V32u_Haswell:
	.ENTRY:
	SUB rsp, 152
	VMOVAPS [rsp], xmm13
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm8
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm6
	VMOVAPS [byte rsp + 112], xmm9
	VMOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 40
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXWD ymm13, [rcx]
	VPMOVZXWD ymm14, [rdx]
	VPMOVZXWD ymm4, [byte rcx + 16]
	VPMOVZXWD ymm1, [byte rdx + 16]
	VPMOVZXWD ymm5, [byte rcx + 32]
	VPMOVZXWD ymm0, [byte rdx + 32]
	VPMULLD ymm10, ymm13, ymm14
	VPMOVZXWD ymm12, [byte rcx + 48]
	VPMOVZXWD ymm8, [byte rdx + 48]
	VPMULLD ymm11, ymm4, ymm1
	VPMOVZXWD ymm6, [byte rcx + 64]
	VPMOVZXWD ymm9, [byte rdx + 64]
	VPMULLD ymm7, ymm5, ymm0
	VMOVDQA [r8], ymm10
	ADD rcx, 80
	ADD rdx, 80
	VPMULLD ymm3, ymm12, ymm8
	VMOVDQA [byte r8 + 32], ymm11
	SUB r9, 40
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXWD ymm13, [rcx]
	VPMOVZXWD ymm14, [rdx]
	VPMULLD ymm2, ymm6, ymm9
	VMOVDQA [byte r8 + 64], ymm7
	VPMOVZXWD ymm4, [byte rcx + 16]
	VPMOVZXWD ymm1, [byte rdx + 16]
	VMOVDQA [byte r8 + 96], ymm3
	VPMOVZXWD ymm5, [byte rcx + 32]
	VPMOVZXWD ymm0, [byte rdx + 32]
	VPMULLD ymm10, ymm13, ymm14
	VMOVDQA [dword r8 + 128], ymm2
	VPMOVZXWD ymm12, [byte rcx + 48]
	VPMOVZXWD ymm8, [byte rdx + 48]
	VPMULLD ymm11, ymm4, ymm1
	ADD r8, 160
	VPMOVZXWD ymm6, [byte rcx + 64]
	VPMOVZXWD ymm9, [byte rdx + 64]
	VPMULLD ymm7, ymm5, ymm0
	VMOVDQA [r8], ymm10
	ADD rcx, 80
	ADD rdx, 80
	VPMULLD ymm3, ymm12, ymm8
	VMOVDQA [byte r8 + 32], ymm11
	SUB r9, 40
	JAE .process_batch
	.process_batch_epilogue:
	VPMULLD ymm2, ymm6, ymm9
	VMOVDQA [byte r8 + 64], ymm7
	VMOVDQA [byte r8 + 96], ymm3
	VMOVDQA [dword r8 + 128], ymm2
	ADD r8, 160
	.batch_process_finish:
	ADD r9, 40
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm13, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm8, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm6, [byte rsp + 96]
	VMOVAPS xmm9, [byte rsp + 112]
	VMOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Multiply_V32sV32s_V32s_Nehalem
_yepCore_Multiply_V32sV32s_V32s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm15, [rcx]
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	PMULLD xmm15, xmm14
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PMULLD xmm4, xmm0
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PMULLD xmm5, xmm3
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PMULLD xmm1, xmm12
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm15, [rcx]
	ADD rdx, 128
	PMULLD xmm6, xmm9
	MOVDQA [byte r8 + 32], xmm5
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	PMULLD xmm2, xmm8
	MOVDQA [byte r8 + 48], xmm1
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	PMULLD xmm11, xmm10
	MOVDQA [byte r8 + 64], xmm6
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	PMULLD xmm13, xmm7
	MOVDQA [byte r8 + 80], xmm2
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	MOVDQA [byte r8 + 96], xmm11
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	PMULLD xmm15, xmm14
	MOVDQA [byte r8 + 112], xmm13
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PMULLD xmm4, xmm0
	ADD r8, 128
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PMULLD xmm5, xmm3
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PMULLD xmm1, xmm12
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	PMULLD xmm6, xmm9
	MOVDQA [byte r8 + 32], xmm5
	PMULLD xmm2, xmm8
	MOVDQA [byte r8 + 48], xmm1
	PMULLD xmm11, xmm10
	MOVDQA [byte r8 + 64], xmm6
	PMULLD xmm13, xmm7
	MOVDQA [byte r8 + 80], xmm2
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Multiply_V32sV32s_V32s_SandyBridge
_yepCore_Multiply_V32sV32s_V32s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm15, [rcx]
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VPMULLD xmm15, xmm15, xmm14
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPMULLD xmm4, xmm4, xmm0
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPMULLD xmm5, xmm5, xmm3
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPMULLD xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm15, [rcx]
	ADD rdx, 128
	VPMULLD xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 32], xmm5
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VPMULLD xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 48], xmm1
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VPMULLD xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 64], xmm6
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VPMULLD xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 80], xmm2
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VPMULLD xmm15, xmm15, xmm14
	VMOVDQA [byte r8 + 112], xmm13
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPMULLD xmm4, xmm4, xmm0
	ADD r8, 128
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPMULLD xmm5, xmm5, xmm3
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPMULLD xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPMULLD xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 32], xmm5
	VPMULLD xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 48], xmm1
	VPMULLD xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 64], xmm6
	VPMULLD xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 80], xmm2
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Multiply_V32sV32s_V32s_Haswell
_yepCore_Multiply_V32sV32s_V32s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm15, [rcx]
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VPMULLD ymm15, ymm15, ymm8
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPMULLD ymm4, ymm4, ymm10
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPMULLD ymm5, ymm5, ymm12
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPMULLD ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm15, [rcx]
	ADD rdx, 256
	VPMULLD ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 64], ymm5
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VPMULLD ymm1, ymm1, ymm3
	VMOVDQA [byte r8 + 96], ymm14
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VPMULLD ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 128], ymm6
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VPMULLD ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 160], ymm1
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VPMULLD ymm15, ymm15, ymm8
	VMOVDQA [dword r8 + 224], ymm0
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPMULLD ymm4, ymm4, ymm10
	ADD r8, 256
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPMULLD ymm5, ymm5, ymm12
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPMULLD ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 256
	VPMULLD ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 64], ymm5
	VPMULLD ymm1, ymm1, ymm3
	VMOVDQA [byte r8 + 96], ymm14
	VPMULLD ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 128], ymm6
	VPMULLD ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 160], ymm1
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	IMUL eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Multiply_V32sV32s_V64s_Nehalem
_yepCore_Multiply_V32sV32s_V64s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm14
	MOVAPS [byte rsp + 16], xmm15
	MOVAPS [byte rsp + 32], xmm11
	MOVAPS [byte rsp + 48], xmm6
	MOVAPS [byte rsp + 64], xmm13
	MOVAPS [byte rsp + 80], xmm8
	MOVAPS [byte rsp + 96], xmm10
	MOVAPS [byte rsp + 112], xmm9
	MOVAPS [dword rsp + 128], xmm12
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	IMUL rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXDQ xmm14, [rcx]
	PMOVZXDQ xmm15, [rdx]
	PMOVZXDQ xmm4, [byte rcx + 8]
	PMOVZXDQ xmm0, [byte rdx + 8]
	PMOVZXDQ xmm5, [byte rcx + 16]
	PMOVZXDQ xmm3, [byte rdx + 16]
	PMOVZXDQ xmm1, [byte rcx + 24]
	PMOVZXDQ xmm11, [byte rdx + 24]
	PMOVZXDQ xmm6, [byte rcx + 32]
	PMOVZXDQ xmm13, [byte rdx + 32]
	PMOVZXDQ xmm2, [byte rcx + 40]
	PMOVZXDQ xmm8, [byte rdx + 40]
	PMULDQ xmm14, xmm15
	PMOVZXDQ xmm10, [byte rcx + 48]
	PMOVZXDQ xmm9, [byte rdx + 48]
	PMULDQ xmm4, xmm0
	PMOVZXDQ xmm12, [byte rcx + 56]
	PMOVZXDQ xmm7, [byte rdx + 56]
	PMULDQ xmm5, xmm3
	MOVDQA [r8], xmm14
	ADD rcx, 64
	ADD rdx, 64
	PMULDQ xmm1, xmm11
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXDQ xmm14, [rcx]
	PMOVZXDQ xmm15, [rdx]
	PMULDQ xmm6, xmm13
	MOVDQA [byte r8 + 32], xmm5
	PMOVZXDQ xmm4, [byte rcx + 8]
	PMOVZXDQ xmm0, [byte rdx + 8]
	PMULDQ xmm2, xmm8
	MOVDQA [byte r8 + 48], xmm1
	PMOVZXDQ xmm5, [byte rcx + 16]
	PMOVZXDQ xmm3, [byte rdx + 16]
	PMULDQ xmm10, xmm9
	MOVDQA [byte r8 + 64], xmm6
	PMOVZXDQ xmm1, [byte rcx + 24]
	PMOVZXDQ xmm11, [byte rdx + 24]
	PMULDQ xmm12, xmm7
	MOVDQA [byte r8 + 80], xmm2
	PMOVZXDQ xmm6, [byte rcx + 32]
	PMOVZXDQ xmm13, [byte rdx + 32]
	MOVDQA [byte r8 + 96], xmm10
	PMOVZXDQ xmm2, [byte rcx + 40]
	PMOVZXDQ xmm8, [byte rdx + 40]
	PMULDQ xmm14, xmm15
	MOVDQA [byte r8 + 112], xmm12
	PMOVZXDQ xmm10, [byte rcx + 48]
	PMOVZXDQ xmm9, [byte rdx + 48]
	PMULDQ xmm4, xmm0
	ADD r8, 128
	PMOVZXDQ xmm12, [byte rcx + 56]
	PMOVZXDQ xmm7, [byte rdx + 56]
	PMULDQ xmm5, xmm3
	MOVDQA [r8], xmm14
	ADD rcx, 64
	ADD rdx, 64
	PMULDQ xmm1, xmm11
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	PMULDQ xmm6, xmm13
	MOVDQA [byte r8 + 32], xmm5
	PMULDQ xmm2, xmm8
	MOVDQA [byte r8 + 48], xmm1
	PMULDQ xmm10, xmm9
	MOVDQA [byte r8 + 64], xmm6
	PMULDQ xmm12, xmm7
	MOVDQA [byte r8 + 80], xmm2
	MOVDQA [byte r8 + 96], xmm10
	MOVDQA [byte r8 + 112], xmm12
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	IMUL rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm14, [rsp]
	MOVAPS xmm15, [byte rsp + 16]
	MOVAPS xmm11, [byte rsp + 32]
	MOVAPS xmm6, [byte rsp + 48]
	MOVAPS xmm13, [byte rsp + 64]
	MOVAPS xmm8, [byte rsp + 80]
	MOVAPS xmm10, [byte rsp + 96]
	MOVAPS xmm9, [byte rsp + 112]
	MOVAPS xmm12, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Multiply_V32sV32s_V64s_SandyBridge
_yepCore_Multiply_V32sV32s_V64s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm14
	VMOVAPS [byte rsp + 16], xmm15
	VMOVAPS [byte rsp + 32], xmm11
	VMOVAPS [byte rsp + 48], xmm6
	VMOVAPS [byte rsp + 64], xmm13
	VMOVAPS [byte rsp + 80], xmm8
	VMOVAPS [byte rsp + 96], xmm10
	VMOVAPS [byte rsp + 112], xmm9
	VMOVAPS [dword rsp + 128], xmm12
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	IMUL rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ xmm14, [rcx]
	VPMOVZXDQ xmm15, [rdx]
	VPMOVZXDQ xmm4, [byte rcx + 8]
	VPMOVZXDQ xmm0, [byte rdx + 8]
	VPMOVZXDQ xmm5, [byte rcx + 16]
	VPMOVZXDQ xmm3, [byte rdx + 16]
	VPMOVZXDQ xmm1, [byte rcx + 24]
	VPMOVZXDQ xmm11, [byte rdx + 24]
	VPMOVZXDQ xmm6, [byte rcx + 32]
	VPMOVZXDQ xmm13, [byte rdx + 32]
	VPMULDQ xmm14, xmm14, xmm15
	VPMOVZXDQ xmm2, [byte rcx + 40]
	VPMOVZXDQ xmm8, [byte rdx + 40]
	VPMULDQ xmm4, xmm4, xmm0
	VPMOVZXDQ xmm10, [byte rcx + 48]
	VPMOVZXDQ xmm9, [byte rdx + 48]
	VPMULDQ xmm5, xmm5, xmm3
	VPMOVZXDQ xmm12, [byte rcx + 56]
	VPMOVZXDQ xmm7, [byte rdx + 56]
	VPMULDQ xmm1, xmm1, xmm11
	VMOVDQA [r8], xmm14
	ADD rcx, 64
	ADD rdx, 64
	VPMULDQ xmm6, xmm6, xmm13
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ xmm14, [rcx]
	VPMOVZXDQ xmm15, [rdx]
	VPMULDQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 32], xmm5
	VPMOVZXDQ xmm4, [byte rcx + 8]
	VPMOVZXDQ xmm0, [byte rdx + 8]
	VPMULDQ xmm10, xmm10, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMOVZXDQ xmm5, [byte rcx + 16]
	VPMOVZXDQ xmm3, [byte rdx + 16]
	VPMULDQ xmm12, xmm12, xmm7
	VMOVDQA [byte r8 + 64], xmm6
	VPMOVZXDQ xmm1, [byte rcx + 24]
	VPMOVZXDQ xmm11, [byte rdx + 24]
	VMOVDQA [byte r8 + 80], xmm2
	VPMOVZXDQ xmm6, [byte rcx + 32]
	VPMOVZXDQ xmm13, [byte rdx + 32]
	VPMULDQ xmm14, xmm14, xmm15
	VMOVDQA [byte r8 + 96], xmm10
	VPMOVZXDQ xmm2, [byte rcx + 40]
	VPMOVZXDQ xmm8, [byte rdx + 40]
	VPMULDQ xmm4, xmm4, xmm0
	VMOVDQA [byte r8 + 112], xmm12
	VPMOVZXDQ xmm10, [byte rcx + 48]
	VPMOVZXDQ xmm9, [byte rdx + 48]
	VPMULDQ xmm5, xmm5, xmm3
	ADD r8, 128
	VPMOVZXDQ xmm12, [byte rcx + 56]
	VPMOVZXDQ xmm7, [byte rdx + 56]
	VPMULDQ xmm1, xmm1, xmm11
	VMOVDQA [r8], xmm14
	ADD rcx, 64
	ADD rdx, 64
	VPMULDQ xmm6, xmm6, xmm13
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	VPMULDQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 32], xmm5
	VPMULDQ xmm10, xmm10, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMULDQ xmm12, xmm12, xmm7
	VMOVDQA [byte r8 + 64], xmm6
	VMOVDQA [byte r8 + 80], xmm2
	VMOVDQA [byte r8 + 96], xmm10
	VMOVDQA [byte r8 + 112], xmm12
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	IMUL rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm14, [rsp]
	VMOVAPS xmm15, [byte rsp + 16]
	VMOVAPS xmm11, [byte rsp + 32]
	VMOVAPS xmm6, [byte rsp + 48]
	VMOVAPS xmm13, [byte rsp + 64]
	VMOVAPS xmm8, [byte rsp + 80]
	VMOVAPS xmm10, [byte rsp + 96]
	VMOVAPS xmm9, [byte rsp + 112]
	VMOVAPS xmm12, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Multiply_V32sV32s_V64s_Haswell
_yepCore_Multiply_V32sV32s_V64s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm14
	VMOVAPS [byte rsp + 16], xmm15
	VMOVAPS [byte rsp + 32], xmm9
	VMOVAPS [byte rsp + 48], xmm11
	VMOVAPS [byte rsp + 64], xmm13
	VMOVAPS [byte rsp + 80], xmm12
	VMOVAPS [byte rsp + 96], xmm6
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm8
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	IMUL rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ ymm14, [rcx]
	VPMOVZXDQ ymm15, [rdx]
	VPMOVZXDQ ymm4, [byte rcx + 16]
	VPMOVZXDQ ymm9, [byte rdx + 16]
	VPMOVZXDQ ymm5, [byte rcx + 32]
	VPMOVZXDQ ymm11, [byte rdx + 32]
	VPMOVZXDQ ymm13, [byte rcx + 48]
	VPMOVZXDQ ymm12, [byte rdx + 48]
	VPMOVZXDQ ymm6, [byte rcx + 64]
	VPMOVZXDQ ymm7, [byte rdx + 64]
	VPMULDQ ymm14, ymm14, ymm15
	VPMOVZXDQ ymm1, [byte rcx + 80]
	VPMOVZXDQ ymm3, [byte rdx + 80]
	VPMULDQ ymm4, ymm4, ymm9
	VPMOVZXDQ ymm2, [byte rcx + 96]
	VPMOVZXDQ ymm10, [byte rdx + 96]
	VPMULDQ ymm5, ymm5, ymm11
	VPMOVZXDQ ymm0, [byte rcx + 112]
	VPMOVZXDQ ymm8, [byte rdx + 112]
	VPMULDQ ymm13, ymm13, ymm12
	VMOVDQA [r8], ymm14
	ADD rcx, 128
	ADD rdx, 128
	VPMULDQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ ymm14, [rcx]
	VPMOVZXDQ ymm15, [rdx]
	VPMULDQ ymm1, ymm1, ymm3
	VMOVDQA [byte r8 + 64], ymm5
	VPMOVZXDQ ymm4, [byte rcx + 16]
	VPMOVZXDQ ymm9, [byte rdx + 16]
	VPMULDQ ymm2, ymm2, ymm10
	VMOVDQA [byte r8 + 96], ymm13
	VPMOVZXDQ ymm5, [byte rcx + 32]
	VPMOVZXDQ ymm11, [byte rdx + 32]
	VPMULDQ ymm0, ymm0, ymm8
	VMOVDQA [dword r8 + 128], ymm6
	VPMOVZXDQ ymm13, [byte rcx + 48]
	VPMOVZXDQ ymm12, [byte rdx + 48]
	VMOVDQA [dword r8 + 160], ymm1
	VPMOVZXDQ ymm6, [byte rcx + 64]
	VPMOVZXDQ ymm7, [byte rdx + 64]
	VPMULDQ ymm14, ymm14, ymm15
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVZXDQ ymm1, [byte rcx + 80]
	VPMOVZXDQ ymm3, [byte rdx + 80]
	VPMULDQ ymm4, ymm4, ymm9
	VMOVDQA [dword r8 + 224], ymm0
	VPMOVZXDQ ymm2, [byte rcx + 96]
	VPMOVZXDQ ymm10, [byte rdx + 96]
	VPMULDQ ymm5, ymm5, ymm11
	ADD r8, 256
	VPMOVZXDQ ymm0, [byte rcx + 112]
	VPMOVZXDQ ymm8, [byte rdx + 112]
	VPMULDQ ymm13, ymm13, ymm12
	VMOVDQA [r8], ymm14
	ADD rcx, 128
	ADD rdx, 128
	VPMULDQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	VPMULDQ ymm1, ymm1, ymm3
	VMOVDQA [byte r8 + 64], ymm5
	VPMULDQ ymm2, ymm2, ymm10
	VMOVDQA [byte r8 + 96], ymm13
	VPMULDQ ymm0, ymm0, ymm8
	VMOVDQA [dword r8 + 128], ymm6
	VMOVDQA [dword r8 + 160], ymm1
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	IMUL rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm14, [rsp]
	VMOVAPS xmm15, [byte rsp + 16]
	VMOVAPS xmm9, [byte rsp + 32]
	VMOVAPS xmm11, [byte rsp + 48]
	VMOVAPS xmm13, [byte rsp + 64]
	VMOVAPS xmm12, [byte rsp + 80]
	VMOVAPS xmm6, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm8, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Multiply_V32uV32u_V64u_K10
_yepCore_Multiply_V32uV32u_V64u_K10:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm14
	MOVAPS [byte rsp + 16], xmm15
	MOVAPS [byte rsp + 32], xmm11
	MOVAPS [byte rsp + 48], xmm6
	MOVAPS [byte rsp + 64], xmm13
	MOVAPS [byte rsp + 80], xmm8
	MOVAPS [byte rsp + 96], xmm10
	MOVAPS [byte rsp + 112], xmm9
	MOVAPS [dword rsp + 128], xmm12
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	IMUL rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm14, [rcx]
	MOVQ xmm15, [rdx]
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm0, [byte rdx + 8]
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PUNPCKLDQ xmm14, xmm14
	PUNPCKLDQ xmm15, xmm15
	MOVQ xmm1, [byte rcx + 24]
	MOVQ xmm11, [byte rdx + 24]
	PUNPCKLDQ xmm4, xmm4
	PUNPCKLDQ xmm0, xmm0
	MOVQ xmm6, [byte rcx + 32]
	MOVQ xmm13, [byte rdx + 32]
	PUNPCKLDQ xmm5, xmm5
	PUNPCKLDQ xmm3, xmm3
	PMULUDQ xmm14, xmm15
	MOVQ xmm2, [byte rcx + 40]
	MOVQ xmm8, [byte rdx + 40]
	PUNPCKLDQ xmm1, xmm1
	PUNPCKLDQ xmm11, xmm11
	PMULUDQ xmm4, xmm0
	MOVQ xmm10, [byte rcx + 48]
	MOVQ xmm9, [byte rdx + 48]
	PUNPCKLDQ xmm6, xmm6
	PUNPCKLDQ xmm13, xmm13
	PMULUDQ xmm5, xmm3
	MOVQ xmm12, [byte rcx + 56]
	MOVQ xmm7, [byte rdx + 56]
	PUNPCKLDQ xmm2, xmm2
	PUNPCKLDQ xmm8, xmm8
	PMULUDQ xmm1, xmm11
	MOVDQA [r8], xmm14
	ADD rcx, 64
	ADD rdx, 64
	PUNPCKLDQ xmm10, xmm10
	PUNPCKLDQ xmm9, xmm9
	PMULUDQ xmm6, xmm13
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm14, [rcx]
	MOVQ xmm15, [rdx]
	PUNPCKLDQ xmm12, xmm12
	PUNPCKLDQ xmm7, xmm7
	PMULUDQ xmm2, xmm8
	MOVDQA [byte r8 + 32], xmm5
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm0, [byte rdx + 8]
	PMULUDQ xmm10, xmm9
	MOVDQA [byte r8 + 48], xmm1
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PUNPCKLDQ xmm14, xmm14
	PUNPCKLDQ xmm15, xmm15
	PMULUDQ xmm12, xmm7
	MOVDQA [byte r8 + 64], xmm6
	MOVQ xmm1, [byte rcx + 24]
	MOVQ xmm11, [byte rdx + 24]
	PUNPCKLDQ xmm4, xmm4
	PUNPCKLDQ xmm0, xmm0
	MOVDQA [byte r8 + 80], xmm2
	MOVQ xmm6, [byte rcx + 32]
	MOVQ xmm13, [byte rdx + 32]
	PUNPCKLDQ xmm5, xmm5
	PUNPCKLDQ xmm3, xmm3
	PMULUDQ xmm14, xmm15
	MOVDQA [byte r8 + 96], xmm10
	MOVQ xmm2, [byte rcx + 40]
	MOVQ xmm8, [byte rdx + 40]
	PUNPCKLDQ xmm1, xmm1
	PUNPCKLDQ xmm11, xmm11
	PMULUDQ xmm4, xmm0
	MOVDQA [byte r8 + 112], xmm12
	MOVQ xmm10, [byte rcx + 48]
	MOVQ xmm9, [byte rdx + 48]
	PUNPCKLDQ xmm6, xmm6
	PUNPCKLDQ xmm13, xmm13
	PMULUDQ xmm5, xmm3
	ADD r8, 128
	MOVQ xmm12, [byte rcx + 56]
	MOVQ xmm7, [byte rdx + 56]
	PUNPCKLDQ xmm2, xmm2
	PUNPCKLDQ xmm8, xmm8
	PMULUDQ xmm1, xmm11
	MOVDQA [r8], xmm14
	ADD rcx, 64
	ADD rdx, 64
	PUNPCKLDQ xmm10, xmm10
	PUNPCKLDQ xmm9, xmm9
	PMULUDQ xmm6, xmm13
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLDQ xmm12, xmm12
	PUNPCKLDQ xmm7, xmm7
	PMULUDQ xmm2, xmm8
	MOVDQA [byte r8 + 32], xmm5
	PMULUDQ xmm10, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PMULUDQ xmm12, xmm7
	MOVDQA [byte r8 + 64], xmm6
	MOVDQA [byte r8 + 80], xmm2
	MOVDQA [byte r8 + 96], xmm10
	MOVDQA [byte r8 + 112], xmm12
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	IMUL rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm14, [rsp]
	MOVAPS xmm15, [byte rsp + 16]
	MOVAPS xmm11, [byte rsp + 32]
	MOVAPS xmm6, [byte rsp + 48]
	MOVAPS xmm13, [byte rsp + 64]
	MOVAPS xmm8, [byte rsp + 80]
	MOVAPS xmm10, [byte rsp + 96]
	MOVAPS xmm9, [byte rsp + 112]
	MOVAPS xmm12, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Multiply_V32uV32u_V64u_Nehalem
_yepCore_Multiply_V32uV32u_V64u_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm14
	MOVAPS [byte rsp + 16], xmm15
	MOVAPS [byte rsp + 32], xmm11
	MOVAPS [byte rsp + 48], xmm6
	MOVAPS [byte rsp + 64], xmm13
	MOVAPS [byte rsp + 80], xmm8
	MOVAPS [byte rsp + 96], xmm10
	MOVAPS [byte rsp + 112], xmm9
	MOVAPS [dword rsp + 128], xmm12
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	IMUL rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXDQ xmm14, [rcx]
	PMOVZXDQ xmm15, [rdx]
	PMOVZXDQ xmm4, [byte rcx + 8]
	PMOVZXDQ xmm0, [byte rdx + 8]
	PMOVZXDQ xmm5, [byte rcx + 16]
	PMOVZXDQ xmm3, [byte rdx + 16]
	PMOVZXDQ xmm1, [byte rcx + 24]
	PMOVZXDQ xmm11, [byte rdx + 24]
	PMOVZXDQ xmm6, [byte rcx + 32]
	PMOVZXDQ xmm13, [byte rdx + 32]
	PMOVZXDQ xmm2, [byte rcx + 40]
	PMOVZXDQ xmm8, [byte rdx + 40]
	PMULUDQ xmm14, xmm15
	PMOVZXDQ xmm10, [byte rcx + 48]
	PMOVZXDQ xmm9, [byte rdx + 48]
	PMULUDQ xmm4, xmm0
	PMOVZXDQ xmm12, [byte rcx + 56]
	PMOVZXDQ xmm7, [byte rdx + 56]
	PMULUDQ xmm5, xmm3
	MOVDQA [r8], xmm14
	ADD rcx, 64
	ADD rdx, 64
	PMULUDQ xmm1, xmm11
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXDQ xmm14, [rcx]
	PMOVZXDQ xmm15, [rdx]
	PMULUDQ xmm6, xmm13
	MOVDQA [byte r8 + 32], xmm5
	PMOVZXDQ xmm4, [byte rcx + 8]
	PMOVZXDQ xmm0, [byte rdx + 8]
	PMULUDQ xmm2, xmm8
	MOVDQA [byte r8 + 48], xmm1
	PMOVZXDQ xmm5, [byte rcx + 16]
	PMOVZXDQ xmm3, [byte rdx + 16]
	PMULUDQ xmm10, xmm9
	MOVDQA [byte r8 + 64], xmm6
	PMOVZXDQ xmm1, [byte rcx + 24]
	PMOVZXDQ xmm11, [byte rdx + 24]
	PMULUDQ xmm12, xmm7
	MOVDQA [byte r8 + 80], xmm2
	PMOVZXDQ xmm6, [byte rcx + 32]
	PMOVZXDQ xmm13, [byte rdx + 32]
	MOVDQA [byte r8 + 96], xmm10
	PMOVZXDQ xmm2, [byte rcx + 40]
	PMOVZXDQ xmm8, [byte rdx + 40]
	PMULUDQ xmm14, xmm15
	MOVDQA [byte r8 + 112], xmm12
	PMOVZXDQ xmm10, [byte rcx + 48]
	PMOVZXDQ xmm9, [byte rdx + 48]
	PMULUDQ xmm4, xmm0
	ADD r8, 128
	PMOVZXDQ xmm12, [byte rcx + 56]
	PMOVZXDQ xmm7, [byte rdx + 56]
	PMULUDQ xmm5, xmm3
	MOVDQA [r8], xmm14
	ADD rcx, 64
	ADD rdx, 64
	PMULUDQ xmm1, xmm11
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	PMULUDQ xmm6, xmm13
	MOVDQA [byte r8 + 32], xmm5
	PMULUDQ xmm2, xmm8
	MOVDQA [byte r8 + 48], xmm1
	PMULUDQ xmm10, xmm9
	MOVDQA [byte r8 + 64], xmm6
	PMULUDQ xmm12, xmm7
	MOVDQA [byte r8 + 80], xmm2
	MOVDQA [byte r8 + 96], xmm10
	MOVDQA [byte r8 + 112], xmm12
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	IMUL rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm14, [rsp]
	MOVAPS xmm15, [byte rsp + 16]
	MOVAPS xmm11, [byte rsp + 32]
	MOVAPS xmm6, [byte rsp + 48]
	MOVAPS xmm13, [byte rsp + 64]
	MOVAPS xmm8, [byte rsp + 80]
	MOVAPS xmm10, [byte rsp + 96]
	MOVAPS xmm9, [byte rsp + 112]
	MOVAPS xmm12, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Multiply_V32uV32u_V64u_SandyBridge
_yepCore_Multiply_V32uV32u_V64u_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm14
	VMOVAPS [byte rsp + 16], xmm15
	VMOVAPS [byte rsp + 32], xmm11
	VMOVAPS [byte rsp + 48], xmm6
	VMOVAPS [byte rsp + 64], xmm13
	VMOVAPS [byte rsp + 80], xmm8
	VMOVAPS [byte rsp + 96], xmm10
	VMOVAPS [byte rsp + 112], xmm9
	VMOVAPS [dword rsp + 128], xmm12
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	IMUL rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ xmm14, [rcx]
	VPMOVZXDQ xmm15, [rdx]
	VPMOVZXDQ xmm4, [byte rcx + 8]
	VPMOVZXDQ xmm0, [byte rdx + 8]
	VPMOVZXDQ xmm5, [byte rcx + 16]
	VPMOVZXDQ xmm3, [byte rdx + 16]
	VPMOVZXDQ xmm1, [byte rcx + 24]
	VPMOVZXDQ xmm11, [byte rdx + 24]
	VPMOVZXDQ xmm6, [byte rcx + 32]
	VPMOVZXDQ xmm13, [byte rdx + 32]
	VPMULUDQ xmm14, xmm14, xmm15
	VPMOVZXDQ xmm2, [byte rcx + 40]
	VPMOVZXDQ xmm8, [byte rdx + 40]
	VPMULUDQ xmm4, xmm4, xmm0
	VPMOVZXDQ xmm10, [byte rcx + 48]
	VPMOVZXDQ xmm9, [byte rdx + 48]
	VPMULUDQ xmm5, xmm5, xmm3
	VPMOVZXDQ xmm12, [byte rcx + 56]
	VPMOVZXDQ xmm7, [byte rdx + 56]
	VPMULUDQ xmm1, xmm1, xmm11
	VMOVDQA [r8], xmm14
	ADD rcx, 64
	ADD rdx, 64
	VPMULUDQ xmm6, xmm6, xmm13
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ xmm14, [rcx]
	VPMOVZXDQ xmm15, [rdx]
	VPMULUDQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 32], xmm5
	VPMOVZXDQ xmm4, [byte rcx + 8]
	VPMOVZXDQ xmm0, [byte rdx + 8]
	VPMULUDQ xmm10, xmm10, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMOVZXDQ xmm5, [byte rcx + 16]
	VPMOVZXDQ xmm3, [byte rdx + 16]
	VPMULUDQ xmm12, xmm12, xmm7
	VMOVDQA [byte r8 + 64], xmm6
	VPMOVZXDQ xmm1, [byte rcx + 24]
	VPMOVZXDQ xmm11, [byte rdx + 24]
	VMOVDQA [byte r8 + 80], xmm2
	VPMOVZXDQ xmm6, [byte rcx + 32]
	VPMOVZXDQ xmm13, [byte rdx + 32]
	VPMULUDQ xmm14, xmm14, xmm15
	VMOVDQA [byte r8 + 96], xmm10
	VPMOVZXDQ xmm2, [byte rcx + 40]
	VPMOVZXDQ xmm8, [byte rdx + 40]
	VPMULUDQ xmm4, xmm4, xmm0
	VMOVDQA [byte r8 + 112], xmm12
	VPMOVZXDQ xmm10, [byte rcx + 48]
	VPMOVZXDQ xmm9, [byte rdx + 48]
	VPMULUDQ xmm5, xmm5, xmm3
	ADD r8, 128
	VPMOVZXDQ xmm12, [byte rcx + 56]
	VPMOVZXDQ xmm7, [byte rdx + 56]
	VPMULUDQ xmm1, xmm1, xmm11
	VMOVDQA [r8], xmm14
	ADD rcx, 64
	ADD rdx, 64
	VPMULUDQ xmm6, xmm6, xmm13
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	VPMULUDQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 32], xmm5
	VPMULUDQ xmm10, xmm10, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMULUDQ xmm12, xmm12, xmm7
	VMOVDQA [byte r8 + 64], xmm6
	VMOVDQA [byte r8 + 80], xmm2
	VMOVDQA [byte r8 + 96], xmm10
	VMOVDQA [byte r8 + 112], xmm12
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	IMUL rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm14, [rsp]
	VMOVAPS xmm15, [byte rsp + 16]
	VMOVAPS xmm11, [byte rsp + 32]
	VMOVAPS xmm6, [byte rsp + 48]
	VMOVAPS xmm13, [byte rsp + 64]
	VMOVAPS xmm8, [byte rsp + 80]
	VMOVAPS xmm10, [byte rsp + 96]
	VMOVAPS xmm9, [byte rsp + 112]
	VMOVAPS xmm12, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Multiply_V32uV32u_V64u_Haswell
_yepCore_Multiply_V32uV32u_V64u_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm14
	VMOVAPS [byte rsp + 16], xmm15
	VMOVAPS [byte rsp + 32], xmm9
	VMOVAPS [byte rsp + 48], xmm11
	VMOVAPS [byte rsp + 64], xmm13
	VMOVAPS [byte rsp + 80], xmm12
	VMOVAPS [byte rsp + 96], xmm6
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm8
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	IMUL rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ ymm14, [rcx]
	VPMOVZXDQ ymm15, [rdx]
	VPMOVZXDQ ymm4, [byte rcx + 16]
	VPMOVZXDQ ymm9, [byte rdx + 16]
	VPMOVZXDQ ymm5, [byte rcx + 32]
	VPMOVZXDQ ymm11, [byte rdx + 32]
	VPMOVZXDQ ymm13, [byte rcx + 48]
	VPMOVZXDQ ymm12, [byte rdx + 48]
	VPMOVZXDQ ymm6, [byte rcx + 64]
	VPMOVZXDQ ymm7, [byte rdx + 64]
	VPMULUDQ ymm14, ymm14, ymm15
	VPMOVZXDQ ymm1, [byte rcx + 80]
	VPMOVZXDQ ymm3, [byte rdx + 80]
	VPMULUDQ ymm4, ymm4, ymm9
	VPMOVZXDQ ymm2, [byte rcx + 96]
	VPMOVZXDQ ymm10, [byte rdx + 96]
	VPMULUDQ ymm5, ymm5, ymm11
	VPMOVZXDQ ymm0, [byte rcx + 112]
	VPMOVZXDQ ymm8, [byte rdx + 112]
	VPMULUDQ ymm13, ymm13, ymm12
	VMOVDQA [r8], ymm14
	ADD rcx, 128
	ADD rdx, 128
	VPMULUDQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ ymm14, [rcx]
	VPMOVZXDQ ymm15, [rdx]
	VPMULUDQ ymm1, ymm1, ymm3
	VMOVDQA [byte r8 + 64], ymm5
	VPMOVZXDQ ymm4, [byte rcx + 16]
	VPMOVZXDQ ymm9, [byte rdx + 16]
	VPMULUDQ ymm2, ymm2, ymm10
	VMOVDQA [byte r8 + 96], ymm13
	VPMOVZXDQ ymm5, [byte rcx + 32]
	VPMOVZXDQ ymm11, [byte rdx + 32]
	VPMULUDQ ymm0, ymm0, ymm8
	VMOVDQA [dword r8 + 128], ymm6
	VPMOVZXDQ ymm13, [byte rcx + 48]
	VPMOVZXDQ ymm12, [byte rdx + 48]
	VMOVDQA [dword r8 + 160], ymm1
	VPMOVZXDQ ymm6, [byte rcx + 64]
	VPMOVZXDQ ymm7, [byte rdx + 64]
	VPMULUDQ ymm14, ymm14, ymm15
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVZXDQ ymm1, [byte rcx + 80]
	VPMOVZXDQ ymm3, [byte rdx + 80]
	VPMULUDQ ymm4, ymm4, ymm9
	VMOVDQA [dword r8 + 224], ymm0
	VPMOVZXDQ ymm2, [byte rcx + 96]
	VPMOVZXDQ ymm10, [byte rdx + 96]
	VPMULUDQ ymm5, ymm5, ymm11
	ADD r8, 256
	VPMOVZXDQ ymm0, [byte rcx + 112]
	VPMOVZXDQ ymm8, [byte rdx + 112]
	VPMULUDQ ymm13, ymm13, ymm12
	VMOVDQA [r8], ymm14
	ADD rcx, 128
	ADD rdx, 128
	VPMULUDQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	VPMULUDQ ymm1, ymm1, ymm3
	VMOVDQA [byte r8 + 64], ymm5
	VPMULUDQ ymm2, ymm2, ymm10
	VMOVDQA [byte r8 + 96], ymm13
	VPMULUDQ ymm0, ymm0, ymm8
	VMOVDQA [dword r8 + 128], ymm6
	VMOVDQA [dword r8 + 160], ymm1
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	IMUL rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm14, [rsp]
	VMOVAPS xmm15, [byte rsp + 16]
	VMOVAPS xmm9, [byte rsp + 32]
	VMOVAPS xmm11, [byte rsp + 48]
	VMOVAPS xmm13, [byte rsp + 64]
	VMOVAPS xmm12, [byte rsp + 80]
	VMOVAPS xmm6, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm8, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Multiply_V32fV32f_V32f_Nehalem
_yepCore_Multiply_V32fV32f_V32f_Nehalem:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm13
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm11
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm6
	MOVAPS [byte rsp + 80], xmm10
	MOVAPS [byte rsp + 96], xmm9
	MOVAPS [byte rsp + 112], xmm8
	MOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSS xmm13, [rcx]
	ADD rcx, 4
	MOVSS xmm14, [rdx]
	ADD rdx, 4
	MULSS xmm13, xmm14
	MOVSS [r8], xmm13
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 28
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPS xmm13, [rcx]
	MOVUPS xmm4, [byte rcx + 16]
	MOVUPS xmm11, [rdx]
	MOVUPS xmm5, [byte rcx + 32]
	MOVUPS xmm12, [byte rdx + 16]
	MOVUPS xmm1, [byte rcx + 48]
	MOVUPS xmm0, [byte rdx + 32]
	MOVUPS xmm6, [byte rcx + 64]
	MOVUPS xmm3, [byte rdx + 48]
	MOVUPS xmm2, [byte rcx + 80]
	MOVUPS xmm10, [byte rdx + 64]
	MULPS xmm13, xmm11
	MOVUPS xmm9, [byte rcx + 96]
	MOVUPS xmm8, [byte rdx + 80]
	MULPS xmm4, xmm12
	MOVAPS [r8], xmm13
	ADD rcx, 112
	MOVUPS xmm7, [byte rdx + 96]
	MULPS xmm5, xmm0
	MOVAPS [byte r8 + 16], xmm4
	SUB r9, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPS xmm13, [rcx]
	ADD rdx, 112
	MULPS xmm1, xmm3
	MOVAPS [byte r8 + 32], xmm5
	MOVUPS xmm4, [byte rcx + 16]
	MOVUPS xmm11, [rdx]
	MULPS xmm6, xmm10
	MOVAPS [byte r8 + 48], xmm1
	MOVUPS xmm5, [byte rcx + 32]
	MOVUPS xmm12, [byte rdx + 16]
	MULPS xmm2, xmm8
	MOVAPS [byte r8 + 64], xmm6
	MOVUPS xmm1, [byte rcx + 48]
	MOVUPS xmm0, [byte rdx + 32]
	MULPS xmm9, xmm7
	MOVAPS [byte r8 + 80], xmm2
	MOVUPS xmm6, [byte rcx + 64]
	MOVUPS xmm3, [byte rdx + 48]
	MOVAPS [byte r8 + 96], xmm9
	MOVUPS xmm2, [byte rcx + 80]
	MOVUPS xmm10, [byte rdx + 64]
	MULPS xmm13, xmm11
	ADD r8, 112
	MOVUPS xmm9, [byte rcx + 96]
	MOVUPS xmm8, [byte rdx + 80]
	MULPS xmm4, xmm12
	MOVAPS [r8], xmm13
	ADD rcx, 112
	MOVUPS xmm7, [byte rdx + 96]
	MULPS xmm5, xmm0
	MOVAPS [byte r8 + 16], xmm4
	SUB r9, 28
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 112
	MULPS xmm1, xmm3
	MOVAPS [byte r8 + 32], xmm5
	MULPS xmm6, xmm10
	MOVAPS [byte r8 + 48], xmm1
	MULPS xmm2, xmm8
	MOVAPS [byte r8 + 64], xmm6
	MULPS xmm9, xmm7
	MOVAPS [byte r8 + 80], xmm2
	MOVAPS [byte r8 + 96], xmm9
	ADD r8, 112
	.batch_process_finish:
	ADD r9, 28
	JZ .return_ok
	.process_single:
	MOVSS xmm4, [rcx]
	ADD rcx, 4
	MOVSS xmm5, [rdx]
	ADD rdx, 4
	MULSS xmm4, xmm5
	MOVSS [r8], xmm4
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm13, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm11, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm6, [byte rsp + 64]
	MOVAPS xmm10, [byte rsp + 80]
	MOVAPS xmm9, [byte rsp + 96]
	MOVAPS xmm8, [byte rsp + 112]
	MOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Multiply_V32fV32f_V32f_SandyBridge
_yepCore_Multiply_V32fV32f_V32f_SandyBridge:
	.ENTRY:
	SUB rsp, 152
	VMOVAPS [rsp], xmm13
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm10
	VMOVAPS [byte rsp + 112], xmm11
	VMOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	VMOVSS xmm13, [rcx]
	ADD rcx, 4
	VMOVSS xmm14, [rdx]
	ADD rdx, 4
	VMULSS xmm13, xmm13, xmm14
	VMOVSS [r8], xmm13
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 56
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS ymm13, [rcx]
	VMOVUPS ymm4, [byte rcx + 32]
	VMOVUPS ymm0, [rdx]
	VMOVUPS ymm5, [byte rcx + 64]
	VMOVUPS ymm8, [byte rdx + 32]
	VMOVUPS ymm12, [byte rcx + 96]
	VMOVUPS ymm9, [byte rdx + 64]
	VMOVUPS ymm6, [dword rcx + 128]
	VMOVUPS ymm10, [byte rdx + 96]
	VMOVUPS ymm1, [dword rcx + 160]
	VMOVUPS ymm11, [dword rdx + 128]
	VMULPS ymm13, ymm13, ymm0
	VMOVUPS ymm2, [dword rcx + 192]
	VMOVUPS ymm7, [dword rdx + 160]
	VMULPS ymm4, ymm4, ymm8
	VMOVAPS [r8], ymm13
	ADD rcx, 224
	VMOVUPS ymm3, [dword rdx + 192]
	VMULPS ymm5, ymm5, ymm9
	VMOVAPS [byte r8 + 32], ymm4
	SUB r9, 56
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS ymm13, [rcx]
	ADD rdx, 224
	VMULPS ymm12, ymm12, ymm10
	VMOVAPS [byte r8 + 64], ymm5
	VMOVUPS ymm4, [byte rcx + 32]
	VMOVUPS ymm0, [rdx]
	VMULPS ymm6, ymm6, ymm11
	VMOVAPS [byte r8 + 96], ymm12
	VMOVUPS ymm5, [byte rcx + 64]
	VMOVUPS ymm8, [byte rdx + 32]
	VMULPS ymm1, ymm1, ymm7
	VMOVAPS [dword r8 + 128], ymm6
	VMOVUPS ymm12, [byte rcx + 96]
	VMOVUPS ymm9, [byte rdx + 64]
	VMULPS ymm2, ymm2, ymm3
	VMOVAPS [dword r8 + 160], ymm1
	VMOVUPS ymm6, [dword rcx + 128]
	VMOVUPS ymm10, [byte rdx + 96]
	VMOVAPS [dword r8 + 192], ymm2
	VMOVUPS ymm1, [dword rcx + 160]
	VMOVUPS ymm11, [dword rdx + 128]
	VMULPS ymm13, ymm13, ymm0
	ADD r8, 224
	VMOVUPS ymm2, [dword rcx + 192]
	VMOVUPS ymm7, [dword rdx + 160]
	VMULPS ymm4, ymm4, ymm8
	VMOVAPS [r8], ymm13
	ADD rcx, 224
	VMOVUPS ymm3, [dword rdx + 192]
	VMULPS ymm5, ymm5, ymm9
	VMOVAPS [byte r8 + 32], ymm4
	SUB r9, 56
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 224
	VMULPS ymm12, ymm12, ymm10
	VMOVAPS [byte r8 + 64], ymm5
	VMULPS ymm6, ymm6, ymm11
	VMOVAPS [byte r8 + 96], ymm12
	VMULPS ymm1, ymm1, ymm7
	VMOVAPS [dword r8 + 128], ymm6
	VMULPS ymm2, ymm2, ymm3
	VMOVAPS [dword r8 + 160], ymm1
	VMOVAPS [dword r8 + 192], ymm2
	ADD r8, 224
	.batch_process_finish:
	ADD r9, 56
	JZ .return_ok
	.process_single:
	VMOVSS xmm4, [rcx]
	ADD rcx, 4
	VMOVSS xmm5, [rdx]
	ADD rdx, 4
	VMULSS xmm4, xmm4, xmm5
	VMOVSS [r8], xmm4
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm13, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm10, [byte rsp + 96]
	VMOVAPS xmm11, [byte rsp + 112]
	VMOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Multiply_V64fV64f_V64f_Nehalem
_yepCore_Multiply_V64fV64f_V64f_Nehalem:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm13
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm11
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm6
	MOVAPS [byte rsp + 80], xmm10
	MOVAPS [byte rsp + 96], xmm9
	MOVAPS [byte rsp + 112], xmm8
	MOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSD xmm13, [rcx]
	ADD rcx, 8
	MOVSD xmm14, [rdx]
	ADD rdx, 8
	MULSD xmm13, xmm14
	MOVSD [r8], xmm13
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 14
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPD xmm13, [rcx]
	MOVUPD xmm4, [byte rcx + 16]
	MOVUPD xmm11, [rdx]
	MOVUPD xmm5, [byte rcx + 32]
	MOVUPD xmm12, [byte rdx + 16]
	MOVUPD xmm1, [byte rcx + 48]
	MOVUPD xmm0, [byte rdx + 32]
	MOVUPD xmm6, [byte rcx + 64]
	MOVUPD xmm3, [byte rdx + 48]
	MOVUPD xmm2, [byte rcx + 80]
	MOVUPD xmm10, [byte rdx + 64]
	MULPD xmm13, xmm11
	MOVUPD xmm9, [byte rcx + 96]
	MOVUPD xmm8, [byte rdx + 80]
	MULPD xmm4, xmm12
	MOVAPD [r8], xmm13
	ADD rcx, 112
	MOVUPD xmm7, [byte rdx + 96]
	MULPD xmm5, xmm0
	MOVAPD [byte r8 + 16], xmm4
	SUB r9, 14
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPD xmm13, [rcx]
	ADD rdx, 112
	MULPD xmm1, xmm3
	MOVAPD [byte r8 + 32], xmm5
	MOVUPD xmm4, [byte rcx + 16]
	MOVUPD xmm11, [rdx]
	MULPD xmm6, xmm10
	MOVAPD [byte r8 + 48], xmm1
	MOVUPD xmm5, [byte rcx + 32]
	MOVUPD xmm12, [byte rdx + 16]
	MULPD xmm2, xmm8
	MOVAPD [byte r8 + 64], xmm6
	MOVUPD xmm1, [byte rcx + 48]
	MOVUPD xmm0, [byte rdx + 32]
	MULPD xmm9, xmm7
	MOVAPD [byte r8 + 80], xmm2
	MOVUPD xmm6, [byte rcx + 64]
	MOVUPD xmm3, [byte rdx + 48]
	MOVAPD [byte r8 + 96], xmm9
	MOVUPD xmm2, [byte rcx + 80]
	MOVUPD xmm10, [byte rdx + 64]
	MULPD xmm13, xmm11
	ADD r8, 112
	MOVUPD xmm9, [byte rcx + 96]
	MOVUPD xmm8, [byte rdx + 80]
	MULPD xmm4, xmm12
	MOVAPD [r8], xmm13
	ADD rcx, 112
	MOVUPD xmm7, [byte rdx + 96]
	MULPD xmm5, xmm0
	MOVAPD [byte r8 + 16], xmm4
	SUB r9, 14
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 112
	MULPD xmm1, xmm3
	MOVAPD [byte r8 + 32], xmm5
	MULPD xmm6, xmm10
	MOVAPD [byte r8 + 48], xmm1
	MULPD xmm2, xmm8
	MOVAPD [byte r8 + 64], xmm6
	MULPD xmm9, xmm7
	MOVAPD [byte r8 + 80], xmm2
	MOVAPD [byte r8 + 96], xmm9
	ADD r8, 112
	.batch_process_finish:
	ADD r9, 14
	JZ .return_ok
	.process_single:
	MOVSD xmm4, [rcx]
	ADD rcx, 8
	MOVSD xmm5, [rdx]
	ADD rdx, 8
	MULSD xmm4, xmm5
	MOVSD [r8], xmm4
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm13, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm11, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm6, [byte rsp + 64]
	MOVAPS xmm10, [byte rsp + 80]
	MOVAPS xmm9, [byte rsp + 96]
	MOVAPS xmm8, [byte rsp + 112]
	MOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Multiply_V64fV64f_V64f_SandyBridge
_yepCore_Multiply_V64fV64f_V64f_SandyBridge:
	.ENTRY:
	SUB rsp, 152
	VMOVAPS [rsp], xmm13
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm10
	VMOVAPS [byte rsp + 112], xmm11
	VMOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	VMOVSD xmm13, [rcx]
	ADD rcx, 8
	VMOVSD xmm14, [rdx]
	ADD rdx, 8
	VMULSD xmm13, xmm13, xmm14
	VMOVSD [r8], xmm13
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 28
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD ymm13, [rcx]
	VMOVUPD ymm4, [byte rcx + 32]
	VMOVUPD ymm0, [rdx]
	VMOVUPD ymm5, [byte rcx + 64]
	VMOVUPD ymm8, [byte rdx + 32]
	VMOVUPD ymm12, [byte rcx + 96]
	VMOVUPD ymm9, [byte rdx + 64]
	VMOVUPD ymm6, [dword rcx + 128]
	VMOVUPD ymm10, [byte rdx + 96]
	VMOVUPD ymm1, [dword rcx + 160]
	VMOVUPD ymm11, [dword rdx + 128]
	VMULPD ymm13, ymm13, ymm0
	VMOVUPD ymm2, [dword rcx + 192]
	VMOVUPD ymm7, [dword rdx + 160]
	VMULPD ymm4, ymm4, ymm8
	VMOVAPD [r8], ymm13
	ADD rcx, 224
	VMOVUPD ymm3, [dword rdx + 192]
	VMULPD ymm5, ymm5, ymm9
	VMOVAPD [byte r8 + 32], ymm4
	SUB r9, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD ymm13, [rcx]
	ADD rdx, 224
	VMULPD ymm12, ymm12, ymm10
	VMOVAPD [byte r8 + 64], ymm5
	VMOVUPD ymm4, [byte rcx + 32]
	VMOVUPD ymm0, [rdx]
	VMULPD ymm6, ymm6, ymm11
	VMOVAPD [byte r8 + 96], ymm12
	VMOVUPD ymm5, [byte rcx + 64]
	VMOVUPD ymm8, [byte rdx + 32]
	VMULPD ymm1, ymm1, ymm7
	VMOVAPD [dword r8 + 128], ymm6
	VMOVUPD ymm12, [byte rcx + 96]
	VMOVUPD ymm9, [byte rdx + 64]
	VMULPD ymm2, ymm2, ymm3
	VMOVAPD [dword r8 + 160], ymm1
	VMOVUPD ymm6, [dword rcx + 128]
	VMOVUPD ymm10, [byte rdx + 96]
	VMOVAPD [dword r8 + 192], ymm2
	VMOVUPD ymm1, [dword rcx + 160]
	VMOVUPD ymm11, [dword rdx + 128]
	VMULPD ymm13, ymm13, ymm0
	ADD r8, 224
	VMOVUPD ymm2, [dword rcx + 192]
	VMOVUPD ymm7, [dword rdx + 160]
	VMULPD ymm4, ymm4, ymm8
	VMOVAPD [r8], ymm13
	ADD rcx, 224
	VMOVUPD ymm3, [dword rdx + 192]
	VMULPD ymm5, ymm5, ymm9
	VMOVAPD [byte r8 + 32], ymm4
	SUB r9, 28
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 224
	VMULPD ymm12, ymm12, ymm10
	VMOVAPD [byte r8 + 64], ymm5
	VMULPD ymm6, ymm6, ymm11
	VMOVAPD [byte r8 + 96], ymm12
	VMULPD ymm1, ymm1, ymm7
	VMOVAPD [dword r8 + 128], ymm6
	VMULPD ymm2, ymm2, ymm3
	VMOVAPD [dword r8 + 160], ymm1
	VMOVAPD [dword r8 + 192], ymm2
	ADD r8, 224
	.batch_process_finish:
	ADD r9, 28
	JZ .return_ok
	.process_single:
	VMOVSD xmm4, [rcx]
	ADD rcx, 8
	VMOVSD xmm5, [rdx]
	ADD rdx, 8
	VMULSD xmm4, xmm4, xmm5
	VMOVSD [r8], xmm4
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm13, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm10, [byte rsp + 96]
	VMOVAPS xmm11, [byte rsp + 112]
	VMOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return
