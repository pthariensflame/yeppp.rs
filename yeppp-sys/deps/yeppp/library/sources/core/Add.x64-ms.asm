;                       Yeppp! library implementation
;                   This file is auto-generated by Peach-Py,
;        Portable Efficient Assembly Code-generator in Higher-level Python,
;                  part of the Yeppp! library infrastructure
; This file is part of Yeppp! library and licensed under the New BSD license.
; See LICENSE.txt for the full text of the license.

section .text$e code align=16
global _yepCore_Add_V8sV8s_V8s_Nehalem
_yepCore_Add_V8sV8s_V8s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], al
	ADD r8, 1
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 128
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm15, [rcx]
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PADDB xmm15, xmm14
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PADDB xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PADDB xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm15, [rcx]
	ADD rdx, 128
	PADDB xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	PADDB xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	PADDB xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	PADDB xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	PADDB xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQA [byte r8 + 112], xmm13
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PADDB xmm15, xmm14
	ADD r8, 128
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PADDB xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PADDB xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	PADDB xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PADDB xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PADDB xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PADDB xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PADDB xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], al
	ADD r8, 1
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return

section .text$f code align=16
global _yepCore_Add_V8sV8s_V8s_SandyBridge
_yepCore_Add_V8sV8s_V8s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], al
	ADD r8, 1
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm15, [rcx]
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPADDB xmm15, xmm15, xmm14
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPADDB xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPADDB xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm15, [rcx]
	ADD rdx, 128
	VPADDB xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VPADDB xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VPADDB xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VPADDB xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VPADDB xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQA [byte r8 + 112], xmm13
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPADDB xmm15, xmm15, xmm14
	ADD r8, 128
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPADDB xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPADDB xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPADDB xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPADDB xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPADDB xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPADDB xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPADDB xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], al
	ADD r8, 1
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return

section .text$h code align=16
global _yepCore_Add_V8sV8s_V8s_Haswell
_yepCore_Add_V8sV8s_V8s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], al
	ADD r8, 1
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 256
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm15, [rcx]
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPADDB ymm15, ymm15, ymm8
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPADDB ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPADDB ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 256
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm15, [rcx]
	ADD rdx, 256
	VPADDB ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VPADDB ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VPADDB ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VPADDB ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VPADDB ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQA [dword r8 + 224], ymm0
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPADDB ymm15, ymm15, ymm8
	ADD r8, 256
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPADDB ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPADDB ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 256
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 256
	VPADDB ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPADDB ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPADDB ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPADDB ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPADDB ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 256
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], al
	ADD r8, 1
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return

section .text$m code align=16
global _yepCore_Add_V8sV8s_V16s_K10
_yepCore_Add_V8sV8s_V16s_K10:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm12
	MOVAPS [byte rsp + 16], xmm13
	MOVAPS [byte rsp + 32], xmm14
	MOVAPS [byte rsp + 48], xmm15
	MOVAPS [byte rsp + 64], xmm6
	MOVAPS [byte rsp + 80], xmm8
	MOVAPS [byte rsp + 96], xmm9
	MOVAPS [byte rsp + 112], xmm10
	MOVAPS [dword rsp + 128], xmm11
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm12, [rcx]
	MOVQ xmm13, [rdx]
	PXOR xmm14, xmm14
	PXOR xmm15, xmm15
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PXOR xmm8, xmm8
	PXOR xmm2, xmm2
	PCMPGTB xmm14, xmm12
	PCMPGTB xmm15, xmm13
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PXOR xmm1, xmm1
	PXOR xmm9, xmm9
	PCMPGTB xmm8, xmm4
	PCMPGTB xmm2, xmm6
	PUNPCKLBW xmm12, xmm14
	PUNPCKLBW xmm13, xmm15
	PADDW xmm12, xmm13
	MOVQ xmm0, [byte rcx + 24]
	MOVQ xmm10, [byte rdx + 24]
	PXOR xmm11, xmm11
	PXOR xmm7, xmm7
	PCMPGTB xmm1, xmm5
	PCMPGTB xmm9, xmm3
	PUNPCKLBW xmm4, xmm8
	PUNPCKLBW xmm6, xmm2
	PADDW xmm4, xmm6
	MOVDQA [r8], xmm12
	ADD rcx, 32
	ADD rdx, 32
	PCMPGTB xmm11, xmm0
	PCMPGTB xmm7, xmm10
	PUNPCKLBW xmm5, xmm1
	PUNPCKLBW xmm3, xmm9
	PADDW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm12, [rcx]
	MOVQ xmm13, [rdx]
	PXOR xmm14, xmm14
	PXOR xmm15, xmm15
	PUNPCKLBW xmm0, xmm11
	PUNPCKLBW xmm10, xmm7
	PADDW xmm0, xmm10
	MOVDQA [byte r8 + 32], xmm5
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PXOR xmm8, xmm8
	PXOR xmm2, xmm2
	PCMPGTB xmm14, xmm12
	PCMPGTB xmm15, xmm13
	MOVDQA [byte r8 + 48], xmm0
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PXOR xmm1, xmm1
	PXOR xmm9, xmm9
	PCMPGTB xmm8, xmm4
	PCMPGTB xmm2, xmm6
	PUNPCKLBW xmm12, xmm14
	PUNPCKLBW xmm13, xmm15
	PADDW xmm12, xmm13
	ADD r8, 64
	MOVQ xmm0, [byte rcx + 24]
	MOVQ xmm10, [byte rdx + 24]
	PXOR xmm11, xmm11
	PXOR xmm7, xmm7
	PCMPGTB xmm1, xmm5
	PCMPGTB xmm9, xmm3
	PUNPCKLBW xmm4, xmm8
	PUNPCKLBW xmm6, xmm2
	PADDW xmm4, xmm6
	MOVDQA [r8], xmm12
	ADD rcx, 32
	ADD rdx, 32
	PCMPGTB xmm11, xmm0
	PCMPGTB xmm7, xmm10
	PUNPCKLBW xmm5, xmm1
	PUNPCKLBW xmm3, xmm9
	PADDW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLBW xmm0, xmm11
	PUNPCKLBW xmm10, xmm7
	PADDW xmm0, xmm10
	MOVDQA [byte r8 + 32], xmm5
	MOVDQA [byte r8 + 48], xmm0
	ADD r8, 64
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm12, [rsp]
	MOVAPS xmm13, [byte rsp + 16]
	MOVAPS xmm14, [byte rsp + 32]
	MOVAPS xmm15, [byte rsp + 48]
	MOVAPS xmm6, [byte rsp + 64]
	MOVAPS xmm8, [byte rsp + 80]
	MOVAPS xmm9, [byte rsp + 96]
	MOVAPS xmm10, [byte rsp + 112]
	MOVAPS xmm11, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V8sV8s_V16s_Nehalem
_yepCore_Add_V8sV8s_V16s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVSXBW xmm15, [rcx]
	PMOVSXBW xmm4, [byte rcx + 8]
	PMOVSXBW xmm14, [rdx]
	PMOVSXBW xmm5, [byte rcx + 16]
	PMOVSXBW xmm0, [byte rdx + 8]
	PMOVSXBW xmm1, [byte rcx + 24]
	PMOVSXBW xmm3, [byte rdx + 16]
	PMOVSXBW xmm6, [byte rcx + 32]
	PMOVSXBW xmm12, [byte rdx + 24]
	PMOVSXBW xmm2, [byte rcx + 40]
	PMOVSXBW xmm9, [byte rdx + 32]
	PMOVSXBW xmm11, [byte rcx + 48]
	PMOVSXBW xmm8, [byte rdx + 40]
	PADDW xmm15, xmm14
	PMOVSXBW xmm13, [byte rcx + 56]
	PMOVSXBW xmm10, [byte rdx + 48]
	PADDW xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVSXBW xmm7, [byte rdx + 56]
	PADDW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVSXBW xmm15, [rcx]
	ADD rdx, 64
	PADDW xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PMOVSXBW xmm4, [byte rcx + 8]
	PMOVSXBW xmm14, [rdx]
	PADDW xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PMOVSXBW xmm5, [byte rcx + 16]
	PMOVSXBW xmm0, [byte rdx + 8]
	PADDW xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PMOVSXBW xmm1, [byte rcx + 24]
	PMOVSXBW xmm3, [byte rdx + 16]
	PADDW xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PMOVSXBW xmm6, [byte rcx + 32]
	PMOVSXBW xmm12, [byte rdx + 24]
	PADDW xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	PMOVSXBW xmm2, [byte rcx + 40]
	PMOVSXBW xmm9, [byte rdx + 32]
	MOVDQA [byte r8 + 112], xmm13
	PMOVSXBW xmm11, [byte rcx + 48]
	PMOVSXBW xmm8, [byte rdx + 40]
	PADDW xmm15, xmm14
	ADD r8, 128
	PMOVSXBW xmm13, [byte rcx + 56]
	PMOVSXBW xmm10, [byte rdx + 48]
	PADDW xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVSXBW xmm7, [byte rdx + 56]
	PADDW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	PADDW xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PADDW xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PADDW xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PADDW xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PADDW xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V8sV8s_V16s_SandyBridge
_yepCore_Add_V8sV8s_V16s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXBW xmm15, [rcx]
	VPMOVSXBW xmm4, [byte rcx + 8]
	VPMOVSXBW xmm14, [rdx]
	VPMOVSXBW xmm5, [byte rcx + 16]
	VPMOVSXBW xmm0, [byte rdx + 8]
	VPMOVSXBW xmm1, [byte rcx + 24]
	VPMOVSXBW xmm3, [byte rdx + 16]
	VPMOVSXBW xmm6, [byte rcx + 32]
	VPMOVSXBW xmm12, [byte rdx + 24]
	VPMOVSXBW xmm2, [byte rcx + 40]
	VPMOVSXBW xmm9, [byte rdx + 32]
	VPMOVSXBW xmm11, [byte rcx + 48]
	VPMOVSXBW xmm8, [byte rdx + 40]
	VPADDW xmm15, xmm15, xmm14
	VPMOVSXBW xmm13, [byte rcx + 56]
	VPMOVSXBW xmm10, [byte rdx + 48]
	VPADDW xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVSXBW xmm7, [byte rdx + 56]
	VPADDW xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXBW xmm15, [rcx]
	ADD rdx, 64
	VPADDW xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPMOVSXBW xmm4, [byte rcx + 8]
	VPMOVSXBW xmm14, [rdx]
	VPADDW xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMOVSXBW xmm5, [byte rcx + 16]
	VPMOVSXBW xmm0, [byte rdx + 8]
	VPADDW xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPMOVSXBW xmm1, [byte rcx + 24]
	VPMOVSXBW xmm3, [byte rdx + 16]
	VPADDW xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPMOVSXBW xmm6, [byte rcx + 32]
	VPMOVSXBW xmm12, [byte rdx + 24]
	VPADDW xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VPMOVSXBW xmm2, [byte rcx + 40]
	VPMOVSXBW xmm9, [byte rdx + 32]
	VMOVDQA [byte r8 + 112], xmm13
	VPMOVSXBW xmm11, [byte rcx + 48]
	VPMOVSXBW xmm8, [byte rdx + 40]
	VPADDW xmm15, xmm15, xmm14
	ADD r8, 128
	VPMOVSXBW xmm13, [byte rcx + 56]
	VPMOVSXBW xmm10, [byte rdx + 48]
	VPADDW xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVSXBW xmm7, [byte rdx + 56]
	VPADDW xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	VPADDW xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPADDW xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPADDW xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPADDW xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPADDW xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Add_V8sV8s_V16s_Haswell
_yepCore_Add_V8sV8s_V16s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXBW ymm15, [rcx]
	VPMOVSXBW ymm4, [byte rcx + 16]
	VPMOVSXBW ymm8, [rdx]
	VPMOVSXBW ymm5, [byte rcx + 32]
	VPMOVSXBW ymm10, [byte rdx + 16]
	VPMOVSXBW ymm14, [byte rcx + 48]
	VPMOVSXBW ymm12, [byte rdx + 32]
	VPMOVSXBW ymm6, [byte rcx + 64]
	VPMOVSXBW ymm13, [byte rdx + 48]
	VPMOVSXBW ymm1, [byte rcx + 80]
	VPMOVSXBW ymm7, [byte rdx + 64]
	VPMOVSXBW ymm2, [byte rcx + 96]
	VPMOVSXBW ymm3, [byte rdx + 80]
	VPADDW ymm15, ymm15, ymm8
	VPMOVSXBW ymm0, [byte rcx + 112]
	VPMOVSXBW ymm11, [byte rdx + 96]
	VPADDW ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVSXBW ymm9, [byte rdx + 112]
	VPADDW ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXBW ymm15, [rcx]
	ADD rdx, 128
	VPADDW ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPMOVSXBW ymm4, [byte rcx + 16]
	VPMOVSXBW ymm8, [rdx]
	VPADDW ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPMOVSXBW ymm5, [byte rcx + 32]
	VPMOVSXBW ymm10, [byte rdx + 16]
	VPADDW ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPMOVSXBW ymm14, [byte rcx + 48]
	VPMOVSXBW ymm12, [byte rdx + 32]
	VPADDW ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPMOVSXBW ymm6, [byte rcx + 64]
	VPMOVSXBW ymm13, [byte rdx + 48]
	VPADDW ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVSXBW ymm1, [byte rcx + 80]
	VPMOVSXBW ymm7, [byte rdx + 64]
	VMOVDQA [dword r8 + 224], ymm0
	VPMOVSXBW ymm2, [byte rcx + 96]
	VPMOVSXBW ymm3, [byte rdx + 80]
	VPADDW ymm15, ymm15, ymm8
	ADD r8, 256
	VPMOVSXBW ymm0, [byte rcx + 112]
	VPMOVSXBW ymm11, [byte rdx + 96]
	VPADDW ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVSXBW ymm9, [byte rdx + 112]
	VPADDW ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPADDW ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPADDW ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPADDW ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPADDW ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPADDW ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	MOVSX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Add_V8uV8u_V16u_K10
_yepCore_Add_V8uV8u_V16u_K10:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm12
	MOVAPS [byte rsp + 16], xmm13
	MOVAPS [byte rsp + 32], xmm14
	MOVAPS [byte rsp + 48], xmm6
	MOVAPS [byte rsp + 64], xmm7
	MOVAPS [byte rsp + 80], xmm8
	MOVAPS [byte rsp + 96], xmm11
	MOVAPS [byte rsp + 112], xmm10
	MOVAPS [dword rsp + 128], xmm9
	PXOR xmm12, xmm12
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 56
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm13, [rcx]
	MOVQ xmm14, [rdx]
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	MOVQ xmm7, [byte rcx + 24]
	MOVQ xmm0, [byte rdx + 24]
	PUNPCKLBW xmm13, xmm12
	MOVQ xmm8, [byte rcx + 32]
	MOVQ xmm11, [byte rdx + 32]
	PUNPCKLBW xmm4, xmm12
	PUNPCKLBW xmm14, xmm12
	MOVQ xmm1, [byte rcx + 40]
	MOVQ xmm2, [byte rdx + 40]
	PUNPCKLBW xmm5, xmm12
	PUNPCKLBW xmm6, xmm12
	PADDW xmm13, xmm14
	MOVQ xmm10, [byte rcx + 48]
	MOVQ xmm9, [byte rdx + 48]
	PUNPCKLBW xmm7, xmm12
	PUNPCKLBW xmm3, xmm12
	PADDW xmm4, xmm6
	MOVDQA [r8], xmm13
	ADD rcx, 56
	ADD rdx, 56
	PUNPCKLBW xmm8, xmm12
	PUNPCKLBW xmm0, xmm12
	PADDW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 56
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm13, [rcx]
	MOVQ xmm14, [rdx]
	PUNPCKLBW xmm1, xmm12
	PUNPCKLBW xmm11, xmm12
	PADDW xmm7, xmm0
	MOVDQA [byte r8 + 32], xmm5
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PUNPCKLBW xmm10, xmm12
	PUNPCKLBW xmm2, xmm12
	PADDW xmm8, xmm11
	MOVDQA [byte r8 + 48], xmm7
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PUNPCKLBW xmm9, xmm12
	PADDW xmm1, xmm2
	MOVDQA [byte r8 + 64], xmm8
	MOVQ xmm7, [byte rcx + 24]
	MOVQ xmm0, [byte rdx + 24]
	PUNPCKLBW xmm13, xmm12
	PADDW xmm10, xmm9
	MOVDQA [byte r8 + 80], xmm1
	MOVQ xmm8, [byte rcx + 32]
	MOVQ xmm11, [byte rdx + 32]
	PUNPCKLBW xmm4, xmm12
	PUNPCKLBW xmm14, xmm12
	MOVDQA [byte r8 + 96], xmm10
	MOVQ xmm1, [byte rcx + 40]
	MOVQ xmm2, [byte rdx + 40]
	PUNPCKLBW xmm5, xmm12
	PUNPCKLBW xmm6, xmm12
	PADDW xmm13, xmm14
	ADD r8, 112
	MOVQ xmm10, [byte rcx + 48]
	MOVQ xmm9, [byte rdx + 48]
	PUNPCKLBW xmm7, xmm12
	PUNPCKLBW xmm3, xmm12
	PADDW xmm4, xmm6
	MOVDQA [r8], xmm13
	ADD rcx, 56
	ADD rdx, 56
	PUNPCKLBW xmm8, xmm12
	PUNPCKLBW xmm0, xmm12
	PADDW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 56
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLBW xmm1, xmm12
	PUNPCKLBW xmm11, xmm12
	PADDW xmm7, xmm0
	MOVDQA [byte r8 + 32], xmm5
	PUNPCKLBW xmm10, xmm12
	PUNPCKLBW xmm2, xmm12
	PADDW xmm8, xmm11
	MOVDQA [byte r8 + 48], xmm7
	PUNPCKLBW xmm9, xmm12
	PADDW xmm1, xmm2
	MOVDQA [byte r8 + 64], xmm8
	PADDW xmm10, xmm9
	MOVDQA [byte r8 + 80], xmm1
	MOVDQA [byte r8 + 96], xmm10
	ADD r8, 112
	.batch_process_finish:
	ADD r9, 56
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm12, [rsp]
	MOVAPS xmm13, [byte rsp + 16]
	MOVAPS xmm14, [byte rsp + 32]
	MOVAPS xmm6, [byte rsp + 48]
	MOVAPS xmm7, [byte rsp + 64]
	MOVAPS xmm8, [byte rsp + 80]
	MOVAPS xmm11, [byte rsp + 96]
	MOVAPS xmm10, [byte rsp + 112]
	MOVAPS xmm9, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V8uV8u_V16u_Nehalem
_yepCore_Add_V8uV8u_V16u_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXBW xmm15, [rcx]
	PMOVZXBW xmm4, [byte rcx + 8]
	PMOVZXBW xmm14, [rdx]
	PMOVZXBW xmm5, [byte rcx + 16]
	PMOVZXBW xmm0, [byte rdx + 8]
	PMOVZXBW xmm1, [byte rcx + 24]
	PMOVZXBW xmm3, [byte rdx + 16]
	PMOVZXBW xmm6, [byte rcx + 32]
	PMOVZXBW xmm12, [byte rdx + 24]
	PMOVZXBW xmm2, [byte rcx + 40]
	PMOVZXBW xmm9, [byte rdx + 32]
	PMOVZXBW xmm11, [byte rcx + 48]
	PMOVZXBW xmm8, [byte rdx + 40]
	PADDW xmm15, xmm14
	PMOVZXBW xmm13, [byte rcx + 56]
	PMOVZXBW xmm10, [byte rdx + 48]
	PADDW xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVZXBW xmm7, [byte rdx + 56]
	PADDW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXBW xmm15, [rcx]
	ADD rdx, 64
	PADDW xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PMOVZXBW xmm4, [byte rcx + 8]
	PMOVZXBW xmm14, [rdx]
	PADDW xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PMOVZXBW xmm5, [byte rcx + 16]
	PMOVZXBW xmm0, [byte rdx + 8]
	PADDW xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PMOVZXBW xmm1, [byte rcx + 24]
	PMOVZXBW xmm3, [byte rdx + 16]
	PADDW xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PMOVZXBW xmm6, [byte rcx + 32]
	PMOVZXBW xmm12, [byte rdx + 24]
	PADDW xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	PMOVZXBW xmm2, [byte rcx + 40]
	PMOVZXBW xmm9, [byte rdx + 32]
	MOVDQA [byte r8 + 112], xmm13
	PMOVZXBW xmm11, [byte rcx + 48]
	PMOVZXBW xmm8, [byte rdx + 40]
	PADDW xmm15, xmm14
	ADD r8, 128
	PMOVZXBW xmm13, [byte rcx + 56]
	PMOVZXBW xmm10, [byte rdx + 48]
	PADDW xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVZXBW xmm7, [byte rdx + 56]
	PADDW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	PADDW xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PADDW xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PADDW xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PADDW xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PADDW xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V8uV8u_V16u_SandyBridge
_yepCore_Add_V8uV8u_V16u_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXBW xmm15, [rcx]
	VPMOVZXBW xmm4, [byte rcx + 8]
	VPMOVZXBW xmm14, [rdx]
	VPMOVZXBW xmm5, [byte rcx + 16]
	VPMOVZXBW xmm0, [byte rdx + 8]
	VPMOVZXBW xmm1, [byte rcx + 24]
	VPMOVZXBW xmm3, [byte rdx + 16]
	VPMOVZXBW xmm6, [byte rcx + 32]
	VPMOVZXBW xmm12, [byte rdx + 24]
	VPMOVZXBW xmm2, [byte rcx + 40]
	VPMOVZXBW xmm9, [byte rdx + 32]
	VPMOVZXBW xmm11, [byte rcx + 48]
	VPMOVZXBW xmm8, [byte rdx + 40]
	VPADDW xmm15, xmm15, xmm14
	VPMOVZXBW xmm13, [byte rcx + 56]
	VPMOVZXBW xmm10, [byte rdx + 48]
	VPADDW xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVZXBW xmm7, [byte rdx + 56]
	VPADDW xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXBW xmm15, [rcx]
	ADD rdx, 64
	VPADDW xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPMOVZXBW xmm4, [byte rcx + 8]
	VPMOVZXBW xmm14, [rdx]
	VPADDW xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMOVZXBW xmm5, [byte rcx + 16]
	VPMOVZXBW xmm0, [byte rdx + 8]
	VPADDW xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPMOVZXBW xmm1, [byte rcx + 24]
	VPMOVZXBW xmm3, [byte rdx + 16]
	VPADDW xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPMOVZXBW xmm6, [byte rcx + 32]
	VPMOVZXBW xmm12, [byte rdx + 24]
	VPADDW xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VPMOVZXBW xmm2, [byte rcx + 40]
	VPMOVZXBW xmm9, [byte rdx + 32]
	VMOVDQA [byte r8 + 112], xmm13
	VPMOVZXBW xmm11, [byte rcx + 48]
	VPMOVZXBW xmm8, [byte rdx + 40]
	VPADDW xmm15, xmm15, xmm14
	ADD r8, 128
	VPMOVZXBW xmm13, [byte rcx + 56]
	VPMOVZXBW xmm10, [byte rdx + 48]
	VPADDW xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVZXBW xmm7, [byte rdx + 56]
	VPADDW xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	VPADDW xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPADDW xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPADDW xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPADDW xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPADDW xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Add_V8uV8u_V16u_Haswell
_yepCore_Add_V8uV8u_V16u_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXBW ymm15, [rcx]
	VPMOVZXBW ymm4, [byte rcx + 16]
	VPMOVZXBW ymm8, [rdx]
	VPMOVZXBW ymm5, [byte rcx + 32]
	VPMOVZXBW ymm10, [byte rdx + 16]
	VPMOVZXBW ymm14, [byte rcx + 48]
	VPMOVZXBW ymm12, [byte rdx + 32]
	VPMOVZXBW ymm6, [byte rcx + 64]
	VPMOVZXBW ymm13, [byte rdx + 48]
	VPMOVZXBW ymm1, [byte rcx + 80]
	VPMOVZXBW ymm7, [byte rdx + 64]
	VPMOVZXBW ymm2, [byte rcx + 96]
	VPMOVZXBW ymm3, [byte rdx + 80]
	VPADDW ymm15, ymm15, ymm8
	VPMOVZXBW ymm0, [byte rcx + 112]
	VPMOVZXBW ymm11, [byte rdx + 96]
	VPADDW ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVZXBW ymm9, [byte rdx + 112]
	VPADDW ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXBW ymm15, [rcx]
	ADD rdx, 128
	VPADDW ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPMOVZXBW ymm4, [byte rcx + 16]
	VPMOVZXBW ymm8, [rdx]
	VPADDW ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPMOVZXBW ymm5, [byte rcx + 32]
	VPMOVZXBW ymm10, [byte rdx + 16]
	VPADDW ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPMOVZXBW ymm14, [byte rcx + 48]
	VPMOVZXBW ymm12, [byte rdx + 32]
	VPADDW ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPMOVZXBW ymm6, [byte rcx + 64]
	VPMOVZXBW ymm13, [byte rdx + 48]
	VPADDW ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVZXBW ymm1, [byte rcx + 80]
	VPMOVZXBW ymm7, [byte rdx + 64]
	VMOVDQA [dword r8 + 224], ymm0
	VPMOVZXBW ymm2, [byte rcx + 96]
	VPMOVZXBW ymm3, [byte rdx + 80]
	VPADDW ymm15, ymm15, ymm8
	ADD r8, 256
	VPMOVZXBW ymm0, [byte rcx + 112]
	VPMOVZXBW ymm11, [byte rdx + 96]
	VPADDW ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVZXBW ymm9, [byte rdx + 112]
	VPADDW ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPADDW ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPADDW ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPADDW ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPADDW ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPADDW ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 128
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	MOVZX r10d, byte [rdx]
	ADD rdx, 1
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V16sV16s_V16s_Nehalem
_yepCore_Add_V16sV16s_V16s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm15, [rcx]
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PADDW xmm15, xmm14
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PADDW xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PADDW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm15, [rcx]
	ADD rdx, 128
	PADDW xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	PADDW xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	PADDW xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	PADDW xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	PADDW xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQA [byte r8 + 112], xmm13
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PADDW xmm15, xmm14
	ADD r8, 128
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PADDW xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PADDW xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	PADDW xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PADDW xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PADDW xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PADDW xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PADDW xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V16sV16s_V16s_SandyBridge
_yepCore_Add_V16sV16s_V16s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm15, [rcx]
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPADDW xmm15, xmm15, xmm14
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPADDW xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPADDW xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm15, [rcx]
	ADD rdx, 128
	VPADDW xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VPADDW xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VPADDW xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VPADDW xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VPADDW xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQA [byte r8 + 112], xmm13
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPADDW xmm15, xmm15, xmm14
	ADD r8, 128
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPADDW xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPADDW xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPADDW xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPADDW xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPADDW xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPADDW xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPADDW xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Add_V16sV16s_V16s_Haswell
_yepCore_Add_V16sV16s_V16s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm15, [rcx]
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPADDW ymm15, ymm15, ymm8
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPADDW ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPADDW ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm15, [rcx]
	ADD rdx, 256
	VPADDW ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VPADDW ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VPADDW ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VPADDW ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VPADDW ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQA [dword r8 + 224], ymm0
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPADDW ymm15, ymm15, ymm8
	ADD r8, 256
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPADDW ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPADDW ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 256
	VPADDW ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPADDW ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPADDW ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPADDW ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPADDW ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Add_V16sV16s_V32s_K10
_yepCore_Add_V16sV16s_V32s_K10:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm12
	MOVAPS [byte rsp + 16], xmm13
	MOVAPS [byte rsp + 32], xmm14
	MOVAPS [byte rsp + 48], xmm15
	MOVAPS [byte rsp + 64], xmm6
	MOVAPS [byte rsp + 80], xmm8
	MOVAPS [byte rsp + 96], xmm9
	MOVAPS [byte rsp + 112], xmm10
	MOVAPS [dword rsp + 128], xmm11
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm12, [rcx]
	MOVQ xmm13, [rdx]
	PXOR xmm14, xmm14
	PXOR xmm15, xmm15
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PXOR xmm8, xmm8
	PXOR xmm2, xmm2
	PCMPGTW xmm14, xmm12
	PCMPGTW xmm15, xmm13
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PXOR xmm1, xmm1
	PXOR xmm9, xmm9
	PCMPGTW xmm8, xmm4
	PCMPGTW xmm2, xmm6
	PUNPCKLWD xmm12, xmm14
	PUNPCKLWD xmm13, xmm15
	PADDD xmm12, xmm13
	MOVQ xmm0, [byte rcx + 24]
	MOVQ xmm10, [byte rdx + 24]
	PXOR xmm11, xmm11
	PXOR xmm7, xmm7
	PCMPGTW xmm1, xmm5
	PCMPGTW xmm9, xmm3
	PUNPCKLWD xmm4, xmm8
	PUNPCKLWD xmm6, xmm2
	PADDD xmm4, xmm6
	MOVDQA [r8], xmm12
	ADD rcx, 32
	ADD rdx, 32
	PCMPGTW xmm11, xmm0
	PCMPGTW xmm7, xmm10
	PUNPCKLWD xmm5, xmm1
	PUNPCKLWD xmm3, xmm9
	PADDD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm12, [rcx]
	MOVQ xmm13, [rdx]
	PXOR xmm14, xmm14
	PXOR xmm15, xmm15
	PUNPCKLWD xmm0, xmm11
	PUNPCKLWD xmm10, xmm7
	PADDD xmm0, xmm10
	MOVDQA [byte r8 + 32], xmm5
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PXOR xmm8, xmm8
	PXOR xmm2, xmm2
	PCMPGTW xmm14, xmm12
	PCMPGTW xmm15, xmm13
	MOVDQA [byte r8 + 48], xmm0
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PXOR xmm1, xmm1
	PXOR xmm9, xmm9
	PCMPGTW xmm8, xmm4
	PCMPGTW xmm2, xmm6
	PUNPCKLWD xmm12, xmm14
	PUNPCKLWD xmm13, xmm15
	PADDD xmm12, xmm13
	ADD r8, 64
	MOVQ xmm0, [byte rcx + 24]
	MOVQ xmm10, [byte rdx + 24]
	PXOR xmm11, xmm11
	PXOR xmm7, xmm7
	PCMPGTW xmm1, xmm5
	PCMPGTW xmm9, xmm3
	PUNPCKLWD xmm4, xmm8
	PUNPCKLWD xmm6, xmm2
	PADDD xmm4, xmm6
	MOVDQA [r8], xmm12
	ADD rcx, 32
	ADD rdx, 32
	PCMPGTW xmm11, xmm0
	PCMPGTW xmm7, xmm10
	PUNPCKLWD xmm5, xmm1
	PUNPCKLWD xmm3, xmm9
	PADDD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLWD xmm0, xmm11
	PUNPCKLWD xmm10, xmm7
	PADDD xmm0, xmm10
	MOVDQA [byte r8 + 32], xmm5
	MOVDQA [byte r8 + 48], xmm0
	ADD r8, 64
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm12, [rsp]
	MOVAPS xmm13, [byte rsp + 16]
	MOVAPS xmm14, [byte rsp + 32]
	MOVAPS xmm15, [byte rsp + 48]
	MOVAPS xmm6, [byte rsp + 64]
	MOVAPS xmm8, [byte rsp + 80]
	MOVAPS xmm9, [byte rsp + 96]
	MOVAPS xmm10, [byte rsp + 112]
	MOVAPS xmm11, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V16sV16s_V32s_Nehalem
_yepCore_Add_V16sV16s_V32s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVSXWD xmm15, [rcx]
	PMOVSXWD xmm4, [byte rcx + 8]
	PMOVSXWD xmm14, [rdx]
	PMOVSXWD xmm5, [byte rcx + 16]
	PMOVSXWD xmm0, [byte rdx + 8]
	PMOVSXWD xmm1, [byte rcx + 24]
	PMOVSXWD xmm3, [byte rdx + 16]
	PMOVSXWD xmm6, [byte rcx + 32]
	PMOVSXWD xmm12, [byte rdx + 24]
	PMOVSXWD xmm2, [byte rcx + 40]
	PMOVSXWD xmm9, [byte rdx + 32]
	PMOVSXWD xmm11, [byte rcx + 48]
	PMOVSXWD xmm8, [byte rdx + 40]
	PADDD xmm15, xmm14
	PMOVSXWD xmm13, [byte rcx + 56]
	PMOVSXWD xmm10, [byte rdx + 48]
	PADDD xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVSXWD xmm7, [byte rdx + 56]
	PADDD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVSXWD xmm15, [rcx]
	ADD rdx, 64
	PADDD xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PMOVSXWD xmm4, [byte rcx + 8]
	PMOVSXWD xmm14, [rdx]
	PADDD xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PMOVSXWD xmm5, [byte rcx + 16]
	PMOVSXWD xmm0, [byte rdx + 8]
	PADDD xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PMOVSXWD xmm1, [byte rcx + 24]
	PMOVSXWD xmm3, [byte rdx + 16]
	PADDD xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PMOVSXWD xmm6, [byte rcx + 32]
	PMOVSXWD xmm12, [byte rdx + 24]
	PADDD xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	PMOVSXWD xmm2, [byte rcx + 40]
	PMOVSXWD xmm9, [byte rdx + 32]
	MOVDQA [byte r8 + 112], xmm13
	PMOVSXWD xmm11, [byte rcx + 48]
	PMOVSXWD xmm8, [byte rdx + 40]
	PADDD xmm15, xmm14
	ADD r8, 128
	PMOVSXWD xmm13, [byte rcx + 56]
	PMOVSXWD xmm10, [byte rdx + 48]
	PADDD xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVSXWD xmm7, [byte rdx + 56]
	PADDD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	PADDD xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PADDD xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PADDD xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PADDD xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PADDD xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V16sV16s_V32s_SandyBridge
_yepCore_Add_V16sV16s_V32s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXWD xmm15, [rcx]
	VPMOVSXWD xmm4, [byte rcx + 8]
	VPMOVSXWD xmm14, [rdx]
	VPMOVSXWD xmm5, [byte rcx + 16]
	VPMOVSXWD xmm0, [byte rdx + 8]
	VPMOVSXWD xmm1, [byte rcx + 24]
	VPMOVSXWD xmm3, [byte rdx + 16]
	VPMOVSXWD xmm6, [byte rcx + 32]
	VPMOVSXWD xmm12, [byte rdx + 24]
	VPMOVSXWD xmm2, [byte rcx + 40]
	VPMOVSXWD xmm9, [byte rdx + 32]
	VPMOVSXWD xmm11, [byte rcx + 48]
	VPMOVSXWD xmm8, [byte rdx + 40]
	VPADDD xmm15, xmm15, xmm14
	VPMOVSXWD xmm13, [byte rcx + 56]
	VPMOVSXWD xmm10, [byte rdx + 48]
	VPADDD xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVSXWD xmm7, [byte rdx + 56]
	VPADDD xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXWD xmm15, [rcx]
	ADD rdx, 64
	VPADDD xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPMOVSXWD xmm4, [byte rcx + 8]
	VPMOVSXWD xmm14, [rdx]
	VPADDD xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMOVSXWD xmm5, [byte rcx + 16]
	VPMOVSXWD xmm0, [byte rdx + 8]
	VPADDD xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPMOVSXWD xmm1, [byte rcx + 24]
	VPMOVSXWD xmm3, [byte rdx + 16]
	VPADDD xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPMOVSXWD xmm6, [byte rcx + 32]
	VPMOVSXWD xmm12, [byte rdx + 24]
	VPADDD xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VPMOVSXWD xmm2, [byte rcx + 40]
	VPMOVSXWD xmm9, [byte rdx + 32]
	VMOVDQA [byte r8 + 112], xmm13
	VPMOVSXWD xmm11, [byte rcx + 48]
	VPMOVSXWD xmm8, [byte rdx + 40]
	VPADDD xmm15, xmm15, xmm14
	ADD r8, 128
	VPMOVSXWD xmm13, [byte rcx + 56]
	VPMOVSXWD xmm10, [byte rdx + 48]
	VPADDD xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVSXWD xmm7, [byte rdx + 56]
	VPADDD xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	VPADDD xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPADDD xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPADDD xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPADDD xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPADDD xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Add_V16sV16s_V32s_Haswell
_yepCore_Add_V16sV16s_V32s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXWD ymm15, [rcx]
	VPMOVSXWD ymm4, [byte rcx + 16]
	VPMOVSXWD ymm8, [rdx]
	VPMOVSXWD ymm5, [byte rcx + 32]
	VPMOVSXWD ymm10, [byte rdx + 16]
	VPMOVSXWD ymm14, [byte rcx + 48]
	VPMOVSXWD ymm12, [byte rdx + 32]
	VPMOVSXWD ymm6, [byte rcx + 64]
	VPMOVSXWD ymm13, [byte rdx + 48]
	VPMOVSXWD ymm1, [byte rcx + 80]
	VPMOVSXWD ymm7, [byte rdx + 64]
	VPMOVSXWD ymm2, [byte rcx + 96]
	VPMOVSXWD ymm3, [byte rdx + 80]
	VPADDD ymm15, ymm15, ymm8
	VPMOVSXWD ymm0, [byte rcx + 112]
	VPMOVSXWD ymm11, [byte rdx + 96]
	VPADDD ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVSXWD ymm9, [byte rdx + 112]
	VPADDD ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXWD ymm15, [rcx]
	ADD rdx, 128
	VPADDD ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPMOVSXWD ymm4, [byte rcx + 16]
	VPMOVSXWD ymm8, [rdx]
	VPADDD ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPMOVSXWD ymm5, [byte rcx + 32]
	VPMOVSXWD ymm10, [byte rdx + 16]
	VPADDD ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPMOVSXWD ymm14, [byte rcx + 48]
	VPMOVSXWD ymm12, [byte rdx + 32]
	VPADDD ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPMOVSXWD ymm6, [byte rcx + 64]
	VPMOVSXWD ymm13, [byte rdx + 48]
	VPADDD ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVSXWD ymm1, [byte rcx + 80]
	VPMOVSXWD ymm7, [byte rdx + 64]
	VMOVDQA [dword r8 + 224], ymm0
	VPMOVSXWD ymm2, [byte rcx + 96]
	VPMOVSXWD ymm3, [byte rdx + 80]
	VPADDD ymm15, ymm15, ymm8
	ADD r8, 256
	VPMOVSXWD ymm0, [byte rcx + 112]
	VPMOVSXWD ymm11, [byte rdx + 96]
	VPADDD ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVSXWD ymm9, [byte rdx + 112]
	VPADDD ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPADDD ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPADDD ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPADDD ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPADDD ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPADDD ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	MOVSX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Add_V16uV16u_V32u_K10
_yepCore_Add_V16uV16u_V32u_K10:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm12
	MOVAPS [byte rsp + 16], xmm13
	MOVAPS [byte rsp + 32], xmm14
	MOVAPS [byte rsp + 48], xmm6
	MOVAPS [byte rsp + 64], xmm7
	MOVAPS [byte rsp + 80], xmm8
	MOVAPS [byte rsp + 96], xmm11
	MOVAPS [byte rsp + 112], xmm10
	MOVAPS [dword rsp + 128], xmm9
	PXOR xmm12, xmm12
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 28
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm13, [rcx]
	MOVQ xmm14, [rdx]
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	MOVQ xmm7, [byte rcx + 24]
	MOVQ xmm0, [byte rdx + 24]
	PUNPCKLWD xmm13, xmm12
	MOVQ xmm8, [byte rcx + 32]
	MOVQ xmm11, [byte rdx + 32]
	PUNPCKLWD xmm4, xmm12
	PUNPCKLWD xmm14, xmm12
	MOVQ xmm1, [byte rcx + 40]
	MOVQ xmm2, [byte rdx + 40]
	PUNPCKLWD xmm5, xmm12
	PUNPCKLWD xmm6, xmm12
	PADDD xmm13, xmm14
	MOVQ xmm10, [byte rcx + 48]
	MOVQ xmm9, [byte rdx + 48]
	PUNPCKLWD xmm7, xmm12
	PUNPCKLWD xmm3, xmm12
	PADDD xmm4, xmm6
	MOVDQA [r8], xmm13
	ADD rcx, 56
	ADD rdx, 56
	PUNPCKLWD xmm8, xmm12
	PUNPCKLWD xmm0, xmm12
	PADDD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm13, [rcx]
	MOVQ xmm14, [rdx]
	PUNPCKLWD xmm1, xmm12
	PUNPCKLWD xmm11, xmm12
	PADDD xmm7, xmm0
	MOVDQA [byte r8 + 32], xmm5
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PUNPCKLWD xmm10, xmm12
	PUNPCKLWD xmm2, xmm12
	PADDD xmm8, xmm11
	MOVDQA [byte r8 + 48], xmm7
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PUNPCKLWD xmm9, xmm12
	PADDD xmm1, xmm2
	MOVDQA [byte r8 + 64], xmm8
	MOVQ xmm7, [byte rcx + 24]
	MOVQ xmm0, [byte rdx + 24]
	PUNPCKLWD xmm13, xmm12
	PADDD xmm10, xmm9
	MOVDQA [byte r8 + 80], xmm1
	MOVQ xmm8, [byte rcx + 32]
	MOVQ xmm11, [byte rdx + 32]
	PUNPCKLWD xmm4, xmm12
	PUNPCKLWD xmm14, xmm12
	MOVDQA [byte r8 + 96], xmm10
	MOVQ xmm1, [byte rcx + 40]
	MOVQ xmm2, [byte rdx + 40]
	PUNPCKLWD xmm5, xmm12
	PUNPCKLWD xmm6, xmm12
	PADDD xmm13, xmm14
	ADD r8, 112
	MOVQ xmm10, [byte rcx + 48]
	MOVQ xmm9, [byte rdx + 48]
	PUNPCKLWD xmm7, xmm12
	PUNPCKLWD xmm3, xmm12
	PADDD xmm4, xmm6
	MOVDQA [r8], xmm13
	ADD rcx, 56
	ADD rdx, 56
	PUNPCKLWD xmm8, xmm12
	PUNPCKLWD xmm0, xmm12
	PADDD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 28
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLWD xmm1, xmm12
	PUNPCKLWD xmm11, xmm12
	PADDD xmm7, xmm0
	MOVDQA [byte r8 + 32], xmm5
	PUNPCKLWD xmm10, xmm12
	PUNPCKLWD xmm2, xmm12
	PADDD xmm8, xmm11
	MOVDQA [byte r8 + 48], xmm7
	PUNPCKLWD xmm9, xmm12
	PADDD xmm1, xmm2
	MOVDQA [byte r8 + 64], xmm8
	PADDD xmm10, xmm9
	MOVDQA [byte r8 + 80], xmm1
	MOVDQA [byte r8 + 96], xmm10
	ADD r8, 112
	.batch_process_finish:
	ADD r9, 28
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm12, [rsp]
	MOVAPS xmm13, [byte rsp + 16]
	MOVAPS xmm14, [byte rsp + 32]
	MOVAPS xmm6, [byte rsp + 48]
	MOVAPS xmm7, [byte rsp + 64]
	MOVAPS xmm8, [byte rsp + 80]
	MOVAPS xmm11, [byte rsp + 96]
	MOVAPS xmm10, [byte rsp + 112]
	MOVAPS xmm9, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V16uV16u_V32u_Nehalem
_yepCore_Add_V16uV16u_V32u_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXWD xmm15, [rcx]
	PMOVZXWD xmm4, [byte rcx + 8]
	PMOVZXWD xmm14, [rdx]
	PMOVZXWD xmm5, [byte rcx + 16]
	PMOVZXWD xmm0, [byte rdx + 8]
	PMOVZXWD xmm1, [byte rcx + 24]
	PMOVZXWD xmm3, [byte rdx + 16]
	PMOVZXWD xmm6, [byte rcx + 32]
	PMOVZXWD xmm12, [byte rdx + 24]
	PMOVZXWD xmm2, [byte rcx + 40]
	PMOVZXWD xmm9, [byte rdx + 32]
	PMOVZXWD xmm11, [byte rcx + 48]
	PMOVZXWD xmm8, [byte rdx + 40]
	PADDD xmm15, xmm14
	PMOVZXWD xmm13, [byte rcx + 56]
	PMOVZXWD xmm10, [byte rdx + 48]
	PADDD xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVZXWD xmm7, [byte rdx + 56]
	PADDD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXWD xmm15, [rcx]
	ADD rdx, 64
	PADDD xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PMOVZXWD xmm4, [byte rcx + 8]
	PMOVZXWD xmm14, [rdx]
	PADDD xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PMOVZXWD xmm5, [byte rcx + 16]
	PMOVZXWD xmm0, [byte rdx + 8]
	PADDD xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PMOVZXWD xmm1, [byte rcx + 24]
	PMOVZXWD xmm3, [byte rdx + 16]
	PADDD xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PMOVZXWD xmm6, [byte rcx + 32]
	PMOVZXWD xmm12, [byte rdx + 24]
	PADDD xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	PMOVZXWD xmm2, [byte rcx + 40]
	PMOVZXWD xmm9, [byte rdx + 32]
	MOVDQA [byte r8 + 112], xmm13
	PMOVZXWD xmm11, [byte rcx + 48]
	PMOVZXWD xmm8, [byte rdx + 40]
	PADDD xmm15, xmm14
	ADD r8, 128
	PMOVZXWD xmm13, [byte rcx + 56]
	PMOVZXWD xmm10, [byte rdx + 48]
	PADDD xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVZXWD xmm7, [byte rdx + 56]
	PADDD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	PADDD xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PADDD xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PADDD xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PADDD xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PADDD xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V16uV16u_V32u_SandyBridge
_yepCore_Add_V16uV16u_V32u_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXWD xmm15, [rcx]
	VPMOVZXWD xmm4, [byte rcx + 8]
	VPMOVZXWD xmm14, [rdx]
	VPMOVZXWD xmm5, [byte rcx + 16]
	VPMOVZXWD xmm0, [byte rdx + 8]
	VPMOVZXWD xmm1, [byte rcx + 24]
	VPMOVZXWD xmm3, [byte rdx + 16]
	VPMOVZXWD xmm6, [byte rcx + 32]
	VPMOVZXWD xmm12, [byte rdx + 24]
	VPMOVZXWD xmm2, [byte rcx + 40]
	VPMOVZXWD xmm9, [byte rdx + 32]
	VPMOVZXWD xmm11, [byte rcx + 48]
	VPMOVZXWD xmm8, [byte rdx + 40]
	VPADDD xmm15, xmm15, xmm14
	VPMOVZXWD xmm13, [byte rcx + 56]
	VPMOVZXWD xmm10, [byte rdx + 48]
	VPADDD xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVZXWD xmm7, [byte rdx + 56]
	VPADDD xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXWD xmm15, [rcx]
	ADD rdx, 64
	VPADDD xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPMOVZXWD xmm4, [byte rcx + 8]
	VPMOVZXWD xmm14, [rdx]
	VPADDD xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMOVZXWD xmm5, [byte rcx + 16]
	VPMOVZXWD xmm0, [byte rdx + 8]
	VPADDD xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPMOVZXWD xmm1, [byte rcx + 24]
	VPMOVZXWD xmm3, [byte rdx + 16]
	VPADDD xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPMOVZXWD xmm6, [byte rcx + 32]
	VPMOVZXWD xmm12, [byte rdx + 24]
	VPADDD xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VPMOVZXWD xmm2, [byte rcx + 40]
	VPMOVZXWD xmm9, [byte rdx + 32]
	VMOVDQA [byte r8 + 112], xmm13
	VPMOVZXWD xmm11, [byte rcx + 48]
	VPMOVZXWD xmm8, [byte rdx + 40]
	VPADDD xmm15, xmm15, xmm14
	ADD r8, 128
	VPMOVZXWD xmm13, [byte rcx + 56]
	VPMOVZXWD xmm10, [byte rdx + 48]
	VPADDD xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVZXWD xmm7, [byte rdx + 56]
	VPADDD xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	VPADDD xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPADDD xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPADDD xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPADDD xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPADDD xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Add_V16uV16u_V32u_Haswell
_yepCore_Add_V16uV16u_V32u_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXWD ymm15, [rcx]
	VPMOVZXWD ymm4, [byte rcx + 16]
	VPMOVZXWD ymm8, [rdx]
	VPMOVZXWD ymm5, [byte rcx + 32]
	VPMOVZXWD ymm10, [byte rdx + 16]
	VPMOVZXWD ymm14, [byte rcx + 48]
	VPMOVZXWD ymm12, [byte rdx + 32]
	VPMOVZXWD ymm6, [byte rcx + 64]
	VPMOVZXWD ymm13, [byte rdx + 48]
	VPMOVZXWD ymm1, [byte rcx + 80]
	VPMOVZXWD ymm7, [byte rdx + 64]
	VPMOVZXWD ymm2, [byte rcx + 96]
	VPMOVZXWD ymm3, [byte rdx + 80]
	VPADDD ymm15, ymm15, ymm8
	VPMOVZXWD ymm0, [byte rcx + 112]
	VPMOVZXWD ymm11, [byte rdx + 96]
	VPADDD ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVZXWD ymm9, [byte rdx + 112]
	VPADDD ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXWD ymm15, [rcx]
	ADD rdx, 128
	VPADDD ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPMOVZXWD ymm4, [byte rcx + 16]
	VPMOVZXWD ymm8, [rdx]
	VPADDD ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPMOVZXWD ymm5, [byte rcx + 32]
	VPMOVZXWD ymm10, [byte rdx + 16]
	VPADDD ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPMOVZXWD ymm14, [byte rcx + 48]
	VPMOVZXWD ymm12, [byte rdx + 32]
	VPADDD ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPMOVZXWD ymm6, [byte rcx + 64]
	VPMOVZXWD ymm13, [byte rdx + 48]
	VPADDD ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVZXWD ymm1, [byte rcx + 80]
	VPMOVZXWD ymm7, [byte rdx + 64]
	VMOVDQA [dword r8 + 224], ymm0
	VPMOVZXWD ymm2, [byte rcx + 96]
	VPMOVZXWD ymm3, [byte rdx + 80]
	VPADDD ymm15, ymm15, ymm8
	ADD r8, 256
	VPMOVZXWD ymm0, [byte rcx + 112]
	VPMOVZXWD ymm11, [byte rdx + 96]
	VPADDD ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVZXWD ymm9, [byte rdx + 112]
	VPADDD ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPADDD ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPADDD ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPADDD ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPADDD ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPADDD ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	MOVZX r10d, word [rdx]
	ADD rdx, 2
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V32sV32s_V32s_Nehalem
_yepCore_Add_V32sV32s_V32s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm15, [rcx]
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PADDD xmm15, xmm14
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PADDD xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PADDD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm15, [rcx]
	ADD rdx, 128
	PADDD xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	PADDD xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	PADDD xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	PADDD xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	PADDD xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQA [byte r8 + 112], xmm13
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PADDD xmm15, xmm14
	ADD r8, 128
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PADDD xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PADDD xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	PADDD xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PADDD xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PADDD xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PADDD xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PADDD xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V32sV32s_V32s_SandyBridge
_yepCore_Add_V32sV32s_V32s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm15, [rcx]
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPADDD xmm15, xmm15, xmm14
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPADDD xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPADDD xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm15, [rcx]
	ADD rdx, 128
	VPADDD xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VPADDD xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VPADDD xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VPADDD xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VPADDD xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQA [byte r8 + 112], xmm13
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPADDD xmm15, xmm15, xmm14
	ADD r8, 128
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPADDD xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPADDD xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPADDD xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPADDD xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPADDD xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPADDD xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPADDD xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Add_V32sV32s_V32s_Haswell
_yepCore_Add_V32sV32s_V32s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm15, [rcx]
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPADDD ymm15, ymm15, ymm8
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPADDD ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPADDD ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm15, [rcx]
	ADD rdx, 256
	VPADDD ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VPADDD ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VPADDD ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VPADDD ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VPADDD ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQA [dword r8 + 224], ymm0
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPADDD ymm15, ymm15, ymm8
	ADD r8, 256
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPADDD ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPADDD ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 256
	VPADDD ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPADDD ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPADDD ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPADDD ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPADDD ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	ADD eax, r10d
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Add_V32sV32s_V64s_K10
_yepCore_Add_V32sV32s_V64s_K10:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm12
	MOVAPS [byte rsp + 16], xmm13
	MOVAPS [byte rsp + 32], xmm14
	MOVAPS [byte rsp + 48], xmm15
	MOVAPS [byte rsp + 64], xmm6
	MOVAPS [byte rsp + 80], xmm8
	MOVAPS [byte rsp + 96], xmm9
	MOVAPS [byte rsp + 112], xmm10
	MOVAPS [dword rsp + 128], xmm11
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 8
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm12, [rcx]
	MOVQ xmm13, [rdx]
	PXOR xmm14, xmm14
	PXOR xmm15, xmm15
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PXOR xmm8, xmm8
	PXOR xmm2, xmm2
	PCMPGTD xmm14, xmm12
	PCMPGTD xmm15, xmm13
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PXOR xmm1, xmm1
	PXOR xmm9, xmm9
	PCMPGTD xmm8, xmm4
	PCMPGTD xmm2, xmm6
	PUNPCKLDQ xmm12, xmm14
	PUNPCKLDQ xmm13, xmm15
	PADDQ xmm12, xmm13
	MOVQ xmm0, [byte rcx + 24]
	MOVQ xmm10, [byte rdx + 24]
	PXOR xmm11, xmm11
	PXOR xmm7, xmm7
	PCMPGTD xmm1, xmm5
	PCMPGTD xmm9, xmm3
	PUNPCKLDQ xmm4, xmm8
	PUNPCKLDQ xmm6, xmm2
	PADDQ xmm4, xmm6
	MOVDQA [r8], xmm12
	ADD rcx, 32
	ADD rdx, 32
	PCMPGTD xmm11, xmm0
	PCMPGTD xmm7, xmm10
	PUNPCKLDQ xmm5, xmm1
	PUNPCKLDQ xmm3, xmm9
	PADDQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 8
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm12, [rcx]
	MOVQ xmm13, [rdx]
	PXOR xmm14, xmm14
	PXOR xmm15, xmm15
	PUNPCKLDQ xmm0, xmm11
	PUNPCKLDQ xmm10, xmm7
	PADDQ xmm0, xmm10
	MOVDQA [byte r8 + 32], xmm5
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PXOR xmm8, xmm8
	PXOR xmm2, xmm2
	PCMPGTD xmm14, xmm12
	PCMPGTD xmm15, xmm13
	MOVDQA [byte r8 + 48], xmm0
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PXOR xmm1, xmm1
	PXOR xmm9, xmm9
	PCMPGTD xmm8, xmm4
	PCMPGTD xmm2, xmm6
	PUNPCKLDQ xmm12, xmm14
	PUNPCKLDQ xmm13, xmm15
	PADDQ xmm12, xmm13
	ADD r8, 64
	MOVQ xmm0, [byte rcx + 24]
	MOVQ xmm10, [byte rdx + 24]
	PXOR xmm11, xmm11
	PXOR xmm7, xmm7
	PCMPGTD xmm1, xmm5
	PCMPGTD xmm9, xmm3
	PUNPCKLDQ xmm4, xmm8
	PUNPCKLDQ xmm6, xmm2
	PADDQ xmm4, xmm6
	MOVDQA [r8], xmm12
	ADD rcx, 32
	ADD rdx, 32
	PCMPGTD xmm11, xmm0
	PCMPGTD xmm7, xmm10
	PUNPCKLDQ xmm5, xmm1
	PUNPCKLDQ xmm3, xmm9
	PADDQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 8
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLDQ xmm0, xmm11
	PUNPCKLDQ xmm10, xmm7
	PADDQ xmm0, xmm10
	MOVDQA [byte r8 + 32], xmm5
	MOVDQA [byte r8 + 48], xmm0
	ADD r8, 64
	.batch_process_finish:
	ADD r9, 8
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm12, [rsp]
	MOVAPS xmm13, [byte rsp + 16]
	MOVAPS xmm14, [byte rsp + 32]
	MOVAPS xmm15, [byte rsp + 48]
	MOVAPS xmm6, [byte rsp + 64]
	MOVAPS xmm8, [byte rsp + 80]
	MOVAPS xmm9, [byte rsp + 96]
	MOVAPS xmm10, [byte rsp + 112]
	MOVAPS xmm11, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V32sV32s_V64s_Nehalem
_yepCore_Add_V32sV32s_V64s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVSXDQ xmm15, [rcx]
	PMOVSXDQ xmm4, [byte rcx + 8]
	PMOVSXDQ xmm14, [rdx]
	PMOVSXDQ xmm5, [byte rcx + 16]
	PMOVSXDQ xmm0, [byte rdx + 8]
	PMOVSXDQ xmm1, [byte rcx + 24]
	PMOVSXDQ xmm3, [byte rdx + 16]
	PMOVSXDQ xmm6, [byte rcx + 32]
	PMOVSXDQ xmm12, [byte rdx + 24]
	PMOVSXDQ xmm2, [byte rcx + 40]
	PMOVSXDQ xmm9, [byte rdx + 32]
	PMOVSXDQ xmm11, [byte rcx + 48]
	PMOVSXDQ xmm8, [byte rdx + 40]
	PADDQ xmm15, xmm14
	PMOVSXDQ xmm13, [byte rcx + 56]
	PMOVSXDQ xmm10, [byte rdx + 48]
	PADDQ xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVSXDQ xmm7, [byte rdx + 56]
	PADDQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVSXDQ xmm15, [rcx]
	ADD rdx, 64
	PADDQ xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PMOVSXDQ xmm4, [byte rcx + 8]
	PMOVSXDQ xmm14, [rdx]
	PADDQ xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PMOVSXDQ xmm5, [byte rcx + 16]
	PMOVSXDQ xmm0, [byte rdx + 8]
	PADDQ xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PMOVSXDQ xmm1, [byte rcx + 24]
	PMOVSXDQ xmm3, [byte rdx + 16]
	PADDQ xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PMOVSXDQ xmm6, [byte rcx + 32]
	PMOVSXDQ xmm12, [byte rdx + 24]
	PADDQ xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	PMOVSXDQ xmm2, [byte rcx + 40]
	PMOVSXDQ xmm9, [byte rdx + 32]
	MOVDQA [byte r8 + 112], xmm13
	PMOVSXDQ xmm11, [byte rcx + 48]
	PMOVSXDQ xmm8, [byte rdx + 40]
	PADDQ xmm15, xmm14
	ADD r8, 128
	PMOVSXDQ xmm13, [byte rcx + 56]
	PMOVSXDQ xmm10, [byte rdx + 48]
	PADDQ xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVSXDQ xmm7, [byte rdx + 56]
	PADDQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	PADDQ xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PADDQ xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PADDQ xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PADDQ xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PADDQ xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V32sV32s_V64s_SandyBridge
_yepCore_Add_V32sV32s_V64s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXDQ xmm15, [rcx]
	VPMOVSXDQ xmm4, [byte rcx + 8]
	VPMOVSXDQ xmm14, [rdx]
	VPMOVSXDQ xmm5, [byte rcx + 16]
	VPMOVSXDQ xmm0, [byte rdx + 8]
	VPMOVSXDQ xmm1, [byte rcx + 24]
	VPMOVSXDQ xmm3, [byte rdx + 16]
	VPMOVSXDQ xmm6, [byte rcx + 32]
	VPMOVSXDQ xmm12, [byte rdx + 24]
	VPMOVSXDQ xmm2, [byte rcx + 40]
	VPMOVSXDQ xmm9, [byte rdx + 32]
	VPMOVSXDQ xmm11, [byte rcx + 48]
	VPMOVSXDQ xmm8, [byte rdx + 40]
	VPADDQ xmm15, xmm15, xmm14
	VPMOVSXDQ xmm13, [byte rcx + 56]
	VPMOVSXDQ xmm10, [byte rdx + 48]
	VPADDQ xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVSXDQ xmm7, [byte rdx + 56]
	VPADDQ xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXDQ xmm15, [rcx]
	ADD rdx, 64
	VPADDQ xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPMOVSXDQ xmm4, [byte rcx + 8]
	VPMOVSXDQ xmm14, [rdx]
	VPADDQ xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMOVSXDQ xmm5, [byte rcx + 16]
	VPMOVSXDQ xmm0, [byte rdx + 8]
	VPADDQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPMOVSXDQ xmm1, [byte rcx + 24]
	VPMOVSXDQ xmm3, [byte rdx + 16]
	VPADDQ xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPMOVSXDQ xmm6, [byte rcx + 32]
	VPMOVSXDQ xmm12, [byte rdx + 24]
	VPADDQ xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VPMOVSXDQ xmm2, [byte rcx + 40]
	VPMOVSXDQ xmm9, [byte rdx + 32]
	VMOVDQA [byte r8 + 112], xmm13
	VPMOVSXDQ xmm11, [byte rcx + 48]
	VPMOVSXDQ xmm8, [byte rdx + 40]
	VPADDQ xmm15, xmm15, xmm14
	ADD r8, 128
	VPMOVSXDQ xmm13, [byte rcx + 56]
	VPMOVSXDQ xmm10, [byte rdx + 48]
	VPADDQ xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVSXDQ xmm7, [byte rdx + 56]
	VPADDQ xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	VPADDQ xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPADDQ xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPADDQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPADDQ xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPADDQ xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Add_V32sV32s_V64s_Haswell
_yepCore_Add_V32sV32s_V64s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXDQ ymm15, [rcx]
	VPMOVSXDQ ymm4, [byte rcx + 16]
	VPMOVSXDQ ymm8, [rdx]
	VPMOVSXDQ ymm5, [byte rcx + 32]
	VPMOVSXDQ ymm10, [byte rdx + 16]
	VPMOVSXDQ ymm14, [byte rcx + 48]
	VPMOVSXDQ ymm12, [byte rdx + 32]
	VPMOVSXDQ ymm6, [byte rcx + 64]
	VPMOVSXDQ ymm13, [byte rdx + 48]
	VPMOVSXDQ ymm1, [byte rcx + 80]
	VPMOVSXDQ ymm7, [byte rdx + 64]
	VPMOVSXDQ ymm2, [byte rcx + 96]
	VPMOVSXDQ ymm3, [byte rdx + 80]
	VPADDQ ymm15, ymm15, ymm8
	VPMOVSXDQ ymm0, [byte rcx + 112]
	VPMOVSXDQ ymm11, [byte rdx + 96]
	VPADDQ ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVSXDQ ymm9, [byte rdx + 112]
	VPADDQ ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXDQ ymm15, [rcx]
	ADD rdx, 128
	VPADDQ ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPMOVSXDQ ymm4, [byte rcx + 16]
	VPMOVSXDQ ymm8, [rdx]
	VPADDQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPMOVSXDQ ymm5, [byte rcx + 32]
	VPMOVSXDQ ymm10, [byte rdx + 16]
	VPADDQ ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPMOVSXDQ ymm14, [byte rcx + 48]
	VPMOVSXDQ ymm12, [byte rdx + 32]
	VPADDQ ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPMOVSXDQ ymm6, [byte rcx + 64]
	VPMOVSXDQ ymm13, [byte rdx + 48]
	VPADDQ ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVSXDQ ymm1, [byte rcx + 80]
	VPMOVSXDQ ymm7, [byte rdx + 64]
	VMOVDQA [dword r8 + 224], ymm0
	VPMOVSXDQ ymm2, [byte rcx + 96]
	VPMOVSXDQ ymm3, [byte rdx + 80]
	VPADDQ ymm15, ymm15, ymm8
	ADD r8, 256
	VPMOVSXDQ ymm0, [byte rcx + 112]
	VPMOVSXDQ ymm11, [byte rdx + 96]
	VPADDQ ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVSXDQ ymm9, [byte rdx + 112]
	VPADDQ ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPADDQ ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPADDQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPADDQ ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPADDQ ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPADDQ ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	MOVSX r10, dword [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Add_V32uV32u_V64u_K10
_yepCore_Add_V32uV32u_V64u_K10:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm12
	MOVAPS [byte rsp + 16], xmm13
	MOVAPS [byte rsp + 32], xmm14
	MOVAPS [byte rsp + 48], xmm6
	MOVAPS [byte rsp + 64], xmm7
	MOVAPS [byte rsp + 80], xmm8
	MOVAPS [byte rsp + 96], xmm11
	MOVAPS [byte rsp + 112], xmm10
	MOVAPS [dword rsp + 128], xmm9
	PXOR xmm12, xmm12
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 14
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm13, [rcx]
	MOVQ xmm14, [rdx]
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	MOVQ xmm7, [byte rcx + 24]
	MOVQ xmm0, [byte rdx + 24]
	PUNPCKLDQ xmm13, xmm12
	MOVQ xmm8, [byte rcx + 32]
	MOVQ xmm11, [byte rdx + 32]
	PUNPCKLDQ xmm4, xmm12
	PUNPCKLDQ xmm14, xmm12
	MOVQ xmm1, [byte rcx + 40]
	MOVQ xmm2, [byte rdx + 40]
	PUNPCKLDQ xmm5, xmm12
	PUNPCKLDQ xmm6, xmm12
	PADDQ xmm13, xmm14
	MOVQ xmm10, [byte rcx + 48]
	MOVQ xmm9, [byte rdx + 48]
	PUNPCKLDQ xmm7, xmm12
	PUNPCKLDQ xmm3, xmm12
	PADDQ xmm4, xmm6
	MOVDQA [r8], xmm13
	ADD rcx, 56
	ADD rdx, 56
	PUNPCKLDQ xmm8, xmm12
	PUNPCKLDQ xmm0, xmm12
	PADDQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 14
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm13, [rcx]
	MOVQ xmm14, [rdx]
	PUNPCKLDQ xmm1, xmm12
	PUNPCKLDQ xmm11, xmm12
	PADDQ xmm7, xmm0
	MOVDQA [byte r8 + 32], xmm5
	MOVQ xmm4, [byte rcx + 8]
	MOVQ xmm6, [byte rdx + 8]
	PUNPCKLDQ xmm10, xmm12
	PUNPCKLDQ xmm2, xmm12
	PADDQ xmm8, xmm11
	MOVDQA [byte r8 + 48], xmm7
	MOVQ xmm5, [byte rcx + 16]
	MOVQ xmm3, [byte rdx + 16]
	PUNPCKLDQ xmm9, xmm12
	PADDQ xmm1, xmm2
	MOVDQA [byte r8 + 64], xmm8
	MOVQ xmm7, [byte rcx + 24]
	MOVQ xmm0, [byte rdx + 24]
	PUNPCKLDQ xmm13, xmm12
	PADDQ xmm10, xmm9
	MOVDQA [byte r8 + 80], xmm1
	MOVQ xmm8, [byte rcx + 32]
	MOVQ xmm11, [byte rdx + 32]
	PUNPCKLDQ xmm4, xmm12
	PUNPCKLDQ xmm14, xmm12
	MOVDQA [byte r8 + 96], xmm10
	MOVQ xmm1, [byte rcx + 40]
	MOVQ xmm2, [byte rdx + 40]
	PUNPCKLDQ xmm5, xmm12
	PUNPCKLDQ xmm6, xmm12
	PADDQ xmm13, xmm14
	ADD r8, 112
	MOVQ xmm10, [byte rcx + 48]
	MOVQ xmm9, [byte rdx + 48]
	PUNPCKLDQ xmm7, xmm12
	PUNPCKLDQ xmm3, xmm12
	PADDQ xmm4, xmm6
	MOVDQA [r8], xmm13
	ADD rcx, 56
	ADD rdx, 56
	PUNPCKLDQ xmm8, xmm12
	PUNPCKLDQ xmm0, xmm12
	PADDQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 14
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLDQ xmm1, xmm12
	PUNPCKLDQ xmm11, xmm12
	PADDQ xmm7, xmm0
	MOVDQA [byte r8 + 32], xmm5
	PUNPCKLDQ xmm10, xmm12
	PUNPCKLDQ xmm2, xmm12
	PADDQ xmm8, xmm11
	MOVDQA [byte r8 + 48], xmm7
	PUNPCKLDQ xmm9, xmm12
	PADDQ xmm1, xmm2
	MOVDQA [byte r8 + 64], xmm8
	PADDQ xmm10, xmm9
	MOVDQA [byte r8 + 80], xmm1
	MOVDQA [byte r8 + 96], xmm10
	ADD r8, 112
	.batch_process_finish:
	ADD r9, 14
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm12, [rsp]
	MOVAPS xmm13, [byte rsp + 16]
	MOVAPS xmm14, [byte rsp + 32]
	MOVAPS xmm6, [byte rsp + 48]
	MOVAPS xmm7, [byte rsp + 64]
	MOVAPS xmm8, [byte rsp + 80]
	MOVAPS xmm11, [byte rsp + 96]
	MOVAPS xmm10, [byte rsp + 112]
	MOVAPS xmm9, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V32uV32u_V64u_Nehalem
_yepCore_Add_V32uV32u_V64u_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXDQ xmm15, [rcx]
	PMOVZXDQ xmm4, [byte rcx + 8]
	PMOVZXDQ xmm14, [rdx]
	PMOVZXDQ xmm5, [byte rcx + 16]
	PMOVZXDQ xmm0, [byte rdx + 8]
	PMOVZXDQ xmm1, [byte rcx + 24]
	PMOVZXDQ xmm3, [byte rdx + 16]
	PMOVZXDQ xmm6, [byte rcx + 32]
	PMOVZXDQ xmm12, [byte rdx + 24]
	PMOVZXDQ xmm2, [byte rcx + 40]
	PMOVZXDQ xmm9, [byte rdx + 32]
	PMOVZXDQ xmm11, [byte rcx + 48]
	PMOVZXDQ xmm8, [byte rdx + 40]
	PADDQ xmm15, xmm14
	PMOVZXDQ xmm13, [byte rcx + 56]
	PMOVZXDQ xmm10, [byte rdx + 48]
	PADDQ xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVZXDQ xmm7, [byte rdx + 56]
	PADDQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXDQ xmm15, [rcx]
	ADD rdx, 64
	PADDQ xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PMOVZXDQ xmm4, [byte rcx + 8]
	PMOVZXDQ xmm14, [rdx]
	PADDQ xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PMOVZXDQ xmm5, [byte rcx + 16]
	PMOVZXDQ xmm0, [byte rdx + 8]
	PADDQ xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PMOVZXDQ xmm1, [byte rcx + 24]
	PMOVZXDQ xmm3, [byte rdx + 16]
	PADDQ xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PMOVZXDQ xmm6, [byte rcx + 32]
	PMOVZXDQ xmm12, [byte rdx + 24]
	PADDQ xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	PMOVZXDQ xmm2, [byte rcx + 40]
	PMOVZXDQ xmm9, [byte rdx + 32]
	MOVDQA [byte r8 + 112], xmm13
	PMOVZXDQ xmm11, [byte rcx + 48]
	PMOVZXDQ xmm8, [byte rdx + 40]
	PADDQ xmm15, xmm14
	ADD r8, 128
	PMOVZXDQ xmm13, [byte rcx + 56]
	PMOVZXDQ xmm10, [byte rdx + 48]
	PADDQ xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 64
	PMOVZXDQ xmm7, [byte rdx + 56]
	PADDQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	PADDQ xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PADDQ xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PADDQ xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PADDQ xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PADDQ xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V32uV32u_V64u_SandyBridge
_yepCore_Add_V32uV32u_V64u_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ xmm15, [rcx]
	VPMOVZXDQ xmm4, [byte rcx + 8]
	VPMOVZXDQ xmm14, [rdx]
	VPMOVZXDQ xmm5, [byte rcx + 16]
	VPMOVZXDQ xmm0, [byte rdx + 8]
	VPMOVZXDQ xmm1, [byte rcx + 24]
	VPMOVZXDQ xmm3, [byte rdx + 16]
	VPMOVZXDQ xmm6, [byte rcx + 32]
	VPMOVZXDQ xmm12, [byte rdx + 24]
	VPMOVZXDQ xmm2, [byte rcx + 40]
	VPMOVZXDQ xmm9, [byte rdx + 32]
	VPMOVZXDQ xmm11, [byte rcx + 48]
	VPMOVZXDQ xmm8, [byte rdx + 40]
	VPADDQ xmm15, xmm15, xmm14
	VPMOVZXDQ xmm13, [byte rcx + 56]
	VPMOVZXDQ xmm10, [byte rdx + 48]
	VPADDQ xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVZXDQ xmm7, [byte rdx + 56]
	VPADDQ xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ xmm15, [rcx]
	ADD rdx, 64
	VPADDQ xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPMOVZXDQ xmm4, [byte rcx + 8]
	VPMOVZXDQ xmm14, [rdx]
	VPADDQ xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPMOVZXDQ xmm5, [byte rcx + 16]
	VPMOVZXDQ xmm0, [byte rdx + 8]
	VPADDQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPMOVZXDQ xmm1, [byte rcx + 24]
	VPMOVZXDQ xmm3, [byte rdx + 16]
	VPADDQ xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPMOVZXDQ xmm6, [byte rcx + 32]
	VPMOVZXDQ xmm12, [byte rdx + 24]
	VPADDQ xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VPMOVZXDQ xmm2, [byte rcx + 40]
	VPMOVZXDQ xmm9, [byte rdx + 32]
	VMOVDQA [byte r8 + 112], xmm13
	VPMOVZXDQ xmm11, [byte rcx + 48]
	VPMOVZXDQ xmm8, [byte rdx + 40]
	VPADDQ xmm15, xmm15, xmm14
	ADD r8, 128
	VPMOVZXDQ xmm13, [byte rcx + 56]
	VPMOVZXDQ xmm10, [byte rdx + 48]
	VPADDQ xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 64
	VPMOVZXDQ xmm7, [byte rdx + 56]
	VPADDQ xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	VPADDQ xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPADDQ xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPADDQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPADDQ xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPADDQ xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Add_V32uV32u_V64u_Haswell
_yepCore_Add_V32uV32u_V64u_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ ymm15, [rcx]
	VPMOVZXDQ ymm4, [byte rcx + 16]
	VPMOVZXDQ ymm8, [rdx]
	VPMOVZXDQ ymm5, [byte rcx + 32]
	VPMOVZXDQ ymm10, [byte rdx + 16]
	VPMOVZXDQ ymm14, [byte rcx + 48]
	VPMOVZXDQ ymm12, [byte rdx + 32]
	VPMOVZXDQ ymm6, [byte rcx + 64]
	VPMOVZXDQ ymm13, [byte rdx + 48]
	VPMOVZXDQ ymm1, [byte rcx + 80]
	VPMOVZXDQ ymm7, [byte rdx + 64]
	VPMOVZXDQ ymm2, [byte rcx + 96]
	VPMOVZXDQ ymm3, [byte rdx + 80]
	VPADDQ ymm15, ymm15, ymm8
	VPMOVZXDQ ymm0, [byte rcx + 112]
	VPMOVZXDQ ymm11, [byte rdx + 96]
	VPADDQ ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVZXDQ ymm9, [byte rdx + 112]
	VPADDQ ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ ymm15, [rcx]
	ADD rdx, 128
	VPADDQ ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPMOVZXDQ ymm4, [byte rcx + 16]
	VPMOVZXDQ ymm8, [rdx]
	VPADDQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPMOVZXDQ ymm5, [byte rcx + 32]
	VPMOVZXDQ ymm10, [byte rdx + 16]
	VPADDQ ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPMOVZXDQ ymm14, [byte rcx + 48]
	VPMOVZXDQ ymm12, [byte rdx + 32]
	VPADDQ ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPMOVZXDQ ymm6, [byte rcx + 64]
	VPMOVZXDQ ymm13, [byte rdx + 48]
	VPADDQ ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVZXDQ ymm1, [byte rcx + 80]
	VPMOVZXDQ ymm7, [byte rdx + 64]
	VMOVDQA [dword r8 + 224], ymm0
	VPMOVZXDQ ymm2, [byte rcx + 96]
	VPMOVZXDQ ymm3, [byte rdx + 80]
	VPADDQ ymm15, ymm15, ymm8
	ADD r8, 256
	VPMOVZXDQ ymm0, [byte rcx + 112]
	VPMOVZXDQ ymm11, [byte rdx + 96]
	VPADDQ ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 128
	VPMOVZXDQ ymm9, [byte rdx + 112]
	VPADDQ ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPADDQ ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPADDQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPADDQ ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPADDQ ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPADDQ ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	MOV r10d, [rdx]
	ADD rdx, 4
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V64sV64s_V64s_Nehalem
_yepCore_Add_V64sV64s_V64s_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm15
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm6
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm8
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm10
	MOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV rax, [rcx]
	ADD rcx, 8
	MOV r10, [rdx]
	ADD rdx, 8
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm15, [rcx]
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PADDQ xmm15, xmm14
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PADDQ xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PADDQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm15, [rcx]
	ADD rdx, 128
	PADDQ xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	MOVDQU xmm4, [byte rcx + 16]
	MOVDQU xmm14, [rdx]
	PADDQ xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	MOVDQU xmm5, [byte rcx + 32]
	MOVDQU xmm0, [byte rdx + 16]
	PADDQ xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	MOVDQU xmm1, [byte rcx + 48]
	MOVDQU xmm3, [byte rdx + 32]
	PADDQ xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	MOVDQU xmm6, [byte rcx + 64]
	MOVDQU xmm12, [byte rdx + 48]
	PADDQ xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQU xmm2, [byte rcx + 80]
	MOVDQU xmm9, [byte rdx + 64]
	MOVDQA [byte r8 + 112], xmm13
	MOVDQU xmm11, [byte rcx + 96]
	MOVDQU xmm8, [byte rdx + 80]
	PADDQ xmm15, xmm14
	ADD r8, 128
	MOVDQU xmm13, [byte rcx + 112]
	MOVDQU xmm10, [byte rdx + 96]
	PADDQ xmm4, xmm0
	MOVDQA [r8], xmm15
	ADD rcx, 128
	MOVDQU xmm7, [byte rdx + 112]
	PADDQ xmm5, xmm3
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	PADDQ xmm1, xmm12
	MOVDQA [byte r8 + 32], xmm5
	PADDQ xmm6, xmm9
	MOVDQA [byte r8 + 48], xmm1
	PADDQ xmm2, xmm8
	MOVDQA [byte r8 + 64], xmm6
	PADDQ xmm11, xmm10
	MOVDQA [byte r8 + 80], xmm2
	PADDQ xmm13, xmm7
	MOVDQA [byte r8 + 96], xmm11
	MOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOV rax, [rcx]
	ADD rcx, 8
	MOV r10, [rdx]
	ADD rdx, 8
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm15, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm8, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm10, [dword rsp + 128]
	MOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V64sV64s_V64s_SandyBridge
_yepCore_Add_V64sV64s_V64s_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm6
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm8
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm10
	VMOVAPS [dword rsp + 144], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV rax, [rcx]
	ADD rcx, 8
	MOV r10, [rdx]
	ADD rdx, 8
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm15, [rcx]
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPADDQ xmm15, xmm15, xmm14
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPADDQ xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPADDQ xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm15, [rcx]
	ADD rdx, 128
	VPADDQ xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VMOVDQU xmm4, [byte rcx + 16]
	VMOVDQU xmm14, [rdx]
	VPADDQ xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VMOVDQU xmm5, [byte rcx + 32]
	VMOVDQU xmm0, [byte rdx + 16]
	VPADDQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VMOVDQU xmm1, [byte rcx + 48]
	VMOVDQU xmm3, [byte rdx + 32]
	VPADDQ xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VMOVDQU xmm6, [byte rcx + 64]
	VMOVDQU xmm12, [byte rdx + 48]
	VPADDQ xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQU xmm2, [byte rcx + 80]
	VMOVDQU xmm9, [byte rdx + 64]
	VMOVDQA [byte r8 + 112], xmm13
	VMOVDQU xmm11, [byte rcx + 96]
	VMOVDQU xmm8, [byte rdx + 80]
	VPADDQ xmm15, xmm15, xmm14
	ADD r8, 128
	VMOVDQU xmm13, [byte rcx + 112]
	VMOVDQU xmm10, [byte rdx + 96]
	VPADDQ xmm4, xmm4, xmm0
	VMOVDQA [r8], xmm15
	ADD rcx, 128
	VMOVDQU xmm7, [byte rdx + 112]
	VPADDQ xmm5, xmm5, xmm3
	VMOVDQA [byte r8 + 16], xmm4
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 128
	VPADDQ xmm1, xmm1, xmm12
	VMOVDQA [byte r8 + 32], xmm5
	VPADDQ xmm6, xmm6, xmm9
	VMOVDQA [byte r8 + 48], xmm1
	VPADDQ xmm2, xmm2, xmm8
	VMOVDQA [byte r8 + 64], xmm6
	VPADDQ xmm11, xmm11, xmm10
	VMOVDQA [byte r8 + 80], xmm2
	VPADDQ xmm13, xmm13, xmm7
	VMOVDQA [byte r8 + 96], xmm11
	VMOVDQA [byte r8 + 112], xmm13
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOV rax, [rcx]
	ADD rcx, 8
	MOV r10, [rdx]
	ADD rdx, 8
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm8, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm10, [dword rsp + 128]
	VMOVAPS xmm7, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Add_V64sV64s_V64s_Haswell
_yepCore_Add_V64sV64s_V64s_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm15
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm10
	VMOVAPS [byte rsp + 48], xmm14
	VMOVAPS [byte rsp + 64], xmm12
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm7
	VMOVAPS [dword rsp + 128], xmm11
	VMOVAPS [dword rsp + 144], xmm9
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOV rax, [rcx]
	ADD rcx, 8
	MOV r10, [rdx]
	ADD rdx, 8
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm15, [rcx]
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPADDQ ymm15, ymm15, ymm8
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPADDQ ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPADDQ ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm15, [rcx]
	ADD rdx, 256
	VPADDQ ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VMOVDQU ymm4, [byte rcx + 32]
	VMOVDQU ymm8, [rdx]
	VPADDQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VMOVDQU ymm5, [byte rcx + 64]
	VMOVDQU ymm10, [byte rdx + 32]
	VPADDQ ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VMOVDQU ymm14, [byte rcx + 96]
	VMOVDQU ymm12, [byte rdx + 64]
	VPADDQ ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VMOVDQU ymm6, [dword rcx + 128]
	VMOVDQU ymm13, [byte rdx + 96]
	VPADDQ ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQU ymm1, [dword rcx + 160]
	VMOVDQU ymm7, [dword rdx + 128]
	VMOVDQA [dword r8 + 224], ymm0
	VMOVDQU ymm2, [dword rcx + 192]
	VMOVDQU ymm3, [dword rdx + 160]
	VPADDQ ymm15, ymm15, ymm8
	ADD r8, 256
	VMOVDQU ymm0, [dword rcx + 224]
	VMOVDQU ymm11, [dword rdx + 192]
	VPADDQ ymm4, ymm4, ymm10
	VMOVDQA [r8], ymm15
	ADD rcx, 256
	VMOVDQU ymm9, [dword rdx + 224]
	VPADDQ ymm5, ymm5, ymm12
	VMOVDQA [byte r8 + 32], ymm4
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 256
	VPADDQ ymm14, ymm14, ymm13
	VMOVDQA [byte r8 + 64], ymm5
	VPADDQ ymm6, ymm6, ymm7
	VMOVDQA [byte r8 + 96], ymm14
	VPADDQ ymm1, ymm1, ymm3
	VMOVDQA [dword r8 + 128], ymm6
	VPADDQ ymm2, ymm2, ymm11
	VMOVDQA [dword r8 + 160], ymm1
	VPADDQ ymm0, ymm0, ymm9
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm0
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOV rax, [rcx]
	ADD rcx, 8
	MOV r10, [rdx]
	ADD rdx, 8
	ADD rax, r10
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm15, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm10, [byte rsp + 32]
	VMOVAPS xmm14, [byte rsp + 48]
	VMOVAPS xmm12, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm7, [byte rsp + 112]
	VMOVAPS xmm11, [dword rsp + 128]
	VMOVAPS xmm9, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V32fV32f_V32f_Nehalem
_yepCore_Add_V32fV32f_V32f_Nehalem:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm13
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm11
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm6
	MOVAPS [byte rsp + 80], xmm10
	MOVAPS [byte rsp + 96], xmm9
	MOVAPS [byte rsp + 112], xmm8
	MOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSS xmm13, [rcx]
	ADD rcx, 4
	MOVSS xmm14, [rdx]
	ADD rdx, 4
	ADDSS xmm13, xmm14
	MOVSS [r8], xmm13
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 28
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPS xmm13, [rcx]
	MOVUPS xmm4, [byte rcx + 16]
	MOVUPS xmm11, [rdx]
	MOVUPS xmm5, [byte rcx + 32]
	MOVUPS xmm12, [byte rdx + 16]
	MOVUPS xmm1, [byte rcx + 48]
	MOVUPS xmm0, [byte rdx + 32]
	MOVUPS xmm6, [byte rcx + 64]
	MOVUPS xmm3, [byte rdx + 48]
	MOVUPS xmm2, [byte rcx + 80]
	MOVUPS xmm10, [byte rdx + 64]
	ADDPS xmm13, xmm11
	MOVUPS xmm9, [byte rcx + 96]
	MOVUPS xmm8, [byte rdx + 80]
	ADDPS xmm4, xmm12
	MOVAPS [r8], xmm13
	ADD rcx, 112
	MOVUPS xmm7, [byte rdx + 96]
	ADDPS xmm5, xmm0
	MOVAPS [byte r8 + 16], xmm4
	SUB r9, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPS xmm13, [rcx]
	ADD rdx, 112
	ADDPS xmm1, xmm3
	MOVAPS [byte r8 + 32], xmm5
	MOVUPS xmm4, [byte rcx + 16]
	MOVUPS xmm11, [rdx]
	ADDPS xmm6, xmm10
	MOVAPS [byte r8 + 48], xmm1
	MOVUPS xmm5, [byte rcx + 32]
	MOVUPS xmm12, [byte rdx + 16]
	ADDPS xmm2, xmm8
	MOVAPS [byte r8 + 64], xmm6
	MOVUPS xmm1, [byte rcx + 48]
	MOVUPS xmm0, [byte rdx + 32]
	ADDPS xmm9, xmm7
	MOVAPS [byte r8 + 80], xmm2
	MOVUPS xmm6, [byte rcx + 64]
	MOVUPS xmm3, [byte rdx + 48]
	MOVAPS [byte r8 + 96], xmm9
	MOVUPS xmm2, [byte rcx + 80]
	MOVUPS xmm10, [byte rdx + 64]
	ADDPS xmm13, xmm11
	ADD r8, 112
	MOVUPS xmm9, [byte rcx + 96]
	MOVUPS xmm8, [byte rdx + 80]
	ADDPS xmm4, xmm12
	MOVAPS [r8], xmm13
	ADD rcx, 112
	MOVUPS xmm7, [byte rdx + 96]
	ADDPS xmm5, xmm0
	MOVAPS [byte r8 + 16], xmm4
	SUB r9, 28
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 112
	ADDPS xmm1, xmm3
	MOVAPS [byte r8 + 32], xmm5
	ADDPS xmm6, xmm10
	MOVAPS [byte r8 + 48], xmm1
	ADDPS xmm2, xmm8
	MOVAPS [byte r8 + 64], xmm6
	ADDPS xmm9, xmm7
	MOVAPS [byte r8 + 80], xmm2
	MOVAPS [byte r8 + 96], xmm9
	ADD r8, 112
	.batch_process_finish:
	ADD r9, 28
	JZ .return_ok
	.process_single:
	MOVSS xmm4, [rcx]
	ADD rcx, 4
	MOVSS xmm5, [rdx]
	ADD rdx, 4
	ADDSS xmm4, xmm5
	MOVSS [r8], xmm4
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm13, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm11, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm6, [byte rsp + 64]
	MOVAPS xmm10, [byte rsp + 80]
	MOVAPS xmm9, [byte rsp + 96]
	MOVAPS xmm8, [byte rsp + 112]
	MOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V32fV32f_V32f_SandyBridge
_yepCore_Add_V32fV32f_V32f_SandyBridge:
	.ENTRY:
	SUB rsp, 152
	VMOVAPS [rsp], xmm13
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm10
	VMOVAPS [byte rsp + 112], xmm11
	VMOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	VMOVSS xmm13, [rcx]
	ADD rcx, 4
	VMOVSS xmm14, [rdx]
	ADD rdx, 4
	VADDSS xmm13, xmm13, xmm14
	VMOVSS [r8], xmm13
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 56
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS ymm13, [rcx]
	VMOVUPS ymm4, [byte rcx + 32]
	VMOVUPS ymm0, [rdx]
	VMOVUPS ymm5, [byte rcx + 64]
	VMOVUPS ymm8, [byte rdx + 32]
	VMOVUPS ymm12, [byte rcx + 96]
	VMOVUPS ymm9, [byte rdx + 64]
	VMOVUPS ymm6, [dword rcx + 128]
	VMOVUPS ymm10, [byte rdx + 96]
	VMOVUPS ymm1, [dword rcx + 160]
	VMOVUPS ymm11, [dword rdx + 128]
	VADDPS ymm13, ymm13, ymm0
	VMOVUPS ymm2, [dword rcx + 192]
	VMOVUPS ymm7, [dword rdx + 160]
	VADDPS ymm4, ymm4, ymm8
	VMOVAPS [r8], ymm13
	ADD rcx, 224
	VMOVUPS ymm3, [dword rdx + 192]
	VADDPS ymm5, ymm5, ymm9
	VMOVAPS [byte r8 + 32], ymm4
	SUB r9, 56
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS ymm13, [rcx]
	ADD rdx, 224
	VADDPS ymm12, ymm12, ymm10
	VMOVAPS [byte r8 + 64], ymm5
	VMOVUPS ymm4, [byte rcx + 32]
	VMOVUPS ymm0, [rdx]
	VADDPS ymm6, ymm6, ymm11
	VMOVAPS [byte r8 + 96], ymm12
	VMOVUPS ymm5, [byte rcx + 64]
	VMOVUPS ymm8, [byte rdx + 32]
	VADDPS ymm1, ymm1, ymm7
	VMOVAPS [dword r8 + 128], ymm6
	VMOVUPS ymm12, [byte rcx + 96]
	VMOVUPS ymm9, [byte rdx + 64]
	VADDPS ymm2, ymm2, ymm3
	VMOVAPS [dword r8 + 160], ymm1
	VMOVUPS ymm6, [dword rcx + 128]
	VMOVUPS ymm10, [byte rdx + 96]
	VMOVAPS [dword r8 + 192], ymm2
	VMOVUPS ymm1, [dword rcx + 160]
	VMOVUPS ymm11, [dword rdx + 128]
	VADDPS ymm13, ymm13, ymm0
	ADD r8, 224
	VMOVUPS ymm2, [dword rcx + 192]
	VMOVUPS ymm7, [dword rdx + 160]
	VADDPS ymm4, ymm4, ymm8
	VMOVAPS [r8], ymm13
	ADD rcx, 224
	VMOVUPS ymm3, [dword rdx + 192]
	VADDPS ymm5, ymm5, ymm9
	VMOVAPS [byte r8 + 32], ymm4
	SUB r9, 56
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 224
	VADDPS ymm12, ymm12, ymm10
	VMOVAPS [byte r8 + 64], ymm5
	VADDPS ymm6, ymm6, ymm11
	VMOVAPS [byte r8 + 96], ymm12
	VADDPS ymm1, ymm1, ymm7
	VMOVAPS [dword r8 + 128], ymm6
	VADDPS ymm2, ymm2, ymm3
	VMOVAPS [dword r8 + 160], ymm1
	VMOVAPS [dword r8 + 192], ymm2
	ADD r8, 224
	.batch_process_finish:
	ADD r9, 56
	JZ .return_ok
	.process_single:
	VMOVSS xmm4, [rcx]
	ADD rcx, 4
	VMOVSS xmm5, [rdx]
	ADD rdx, 4
	VADDSS xmm4, xmm4, xmm5
	VMOVSS [r8], xmm4
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm13, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm10, [byte rsp + 96]
	VMOVAPS xmm11, [byte rsp + 112]
	VMOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V64fV64f_V64f_Nehalem
_yepCore_Add_V64fV64f_V64f_Nehalem:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm13
	MOVAPS [byte rsp + 16], xmm14
	MOVAPS [byte rsp + 32], xmm11
	MOVAPS [byte rsp + 48], xmm12
	MOVAPS [byte rsp + 64], xmm6
	MOVAPS [byte rsp + 80], xmm10
	MOVAPS [byte rsp + 96], xmm9
	MOVAPS [byte rsp + 112], xmm8
	MOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSD xmm13, [rcx]
	ADD rcx, 8
	MOVSD xmm14, [rdx]
	ADD rdx, 8
	ADDSD xmm13, xmm14
	MOVSD [r8], xmm13
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 14
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPD xmm13, [rcx]
	MOVUPD xmm4, [byte rcx + 16]
	MOVUPD xmm11, [rdx]
	MOVUPD xmm5, [byte rcx + 32]
	MOVUPD xmm12, [byte rdx + 16]
	MOVUPD xmm1, [byte rcx + 48]
	MOVUPD xmm0, [byte rdx + 32]
	MOVUPD xmm6, [byte rcx + 64]
	MOVUPD xmm3, [byte rdx + 48]
	MOVUPD xmm2, [byte rcx + 80]
	MOVUPD xmm10, [byte rdx + 64]
	ADDPD xmm13, xmm11
	MOVUPD xmm9, [byte rcx + 96]
	MOVUPD xmm8, [byte rdx + 80]
	ADDPD xmm4, xmm12
	MOVAPD [r8], xmm13
	ADD rcx, 112
	MOVUPD xmm7, [byte rdx + 96]
	ADDPD xmm5, xmm0
	MOVAPD [byte r8 + 16], xmm4
	SUB r9, 14
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPD xmm13, [rcx]
	ADD rdx, 112
	ADDPD xmm1, xmm3
	MOVAPD [byte r8 + 32], xmm5
	MOVUPD xmm4, [byte rcx + 16]
	MOVUPD xmm11, [rdx]
	ADDPD xmm6, xmm10
	MOVAPD [byte r8 + 48], xmm1
	MOVUPD xmm5, [byte rcx + 32]
	MOVUPD xmm12, [byte rdx + 16]
	ADDPD xmm2, xmm8
	MOVAPD [byte r8 + 64], xmm6
	MOVUPD xmm1, [byte rcx + 48]
	MOVUPD xmm0, [byte rdx + 32]
	ADDPD xmm9, xmm7
	MOVAPD [byte r8 + 80], xmm2
	MOVUPD xmm6, [byte rcx + 64]
	MOVUPD xmm3, [byte rdx + 48]
	MOVAPD [byte r8 + 96], xmm9
	MOVUPD xmm2, [byte rcx + 80]
	MOVUPD xmm10, [byte rdx + 64]
	ADDPD xmm13, xmm11
	ADD r8, 112
	MOVUPD xmm9, [byte rcx + 96]
	MOVUPD xmm8, [byte rdx + 80]
	ADDPD xmm4, xmm12
	MOVAPD [r8], xmm13
	ADD rcx, 112
	MOVUPD xmm7, [byte rdx + 96]
	ADDPD xmm5, xmm0
	MOVAPD [byte r8 + 16], xmm4
	SUB r9, 14
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 112
	ADDPD xmm1, xmm3
	MOVAPD [byte r8 + 32], xmm5
	ADDPD xmm6, xmm10
	MOVAPD [byte r8 + 48], xmm1
	ADDPD xmm2, xmm8
	MOVAPD [byte r8 + 64], xmm6
	ADDPD xmm9, xmm7
	MOVAPD [byte r8 + 80], xmm2
	MOVAPD [byte r8 + 96], xmm9
	ADD r8, 112
	.batch_process_finish:
	ADD r9, 14
	JZ .return_ok
	.process_single:
	MOVSD xmm4, [rcx]
	ADD rcx, 8
	MOVSD xmm5, [rdx]
	ADD rdx, 8
	ADDSD xmm4, xmm5
	MOVSD [r8], xmm4
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm13, [rsp]
	MOVAPS xmm14, [byte rsp + 16]
	MOVAPS xmm11, [byte rsp + 32]
	MOVAPS xmm12, [byte rsp + 48]
	MOVAPS xmm6, [byte rsp + 64]
	MOVAPS xmm10, [byte rsp + 80]
	MOVAPS xmm9, [byte rsp + 96]
	MOVAPS xmm8, [byte rsp + 112]
	MOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V64fV64f_V64f_SandyBridge
_yepCore_Add_V64fV64f_V64f_SandyBridge:
	.ENTRY:
	SUB rsp, 152
	VMOVAPS [rsp], xmm13
	VMOVAPS [byte rsp + 16], xmm14
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm12
	VMOVAPS [byte rsp + 64], xmm9
	VMOVAPS [byte rsp + 80], xmm6
	VMOVAPS [byte rsp + 96], xmm10
	VMOVAPS [byte rsp + 112], xmm11
	VMOVAPS [dword rsp + 128], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	VMOVSD xmm13, [rcx]
	ADD rcx, 8
	VMOVSD xmm14, [rdx]
	ADD rdx, 8
	VADDSD xmm13, xmm13, xmm14
	VMOVSD [r8], xmm13
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 28
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD ymm13, [rcx]
	VMOVUPD ymm4, [byte rcx + 32]
	VMOVUPD ymm0, [rdx]
	VMOVUPD ymm5, [byte rcx + 64]
	VMOVUPD ymm8, [byte rdx + 32]
	VMOVUPD ymm12, [byte rcx + 96]
	VMOVUPD ymm9, [byte rdx + 64]
	VMOVUPD ymm6, [dword rcx + 128]
	VMOVUPD ymm10, [byte rdx + 96]
	VMOVUPD ymm1, [dword rcx + 160]
	VMOVUPD ymm11, [dword rdx + 128]
	VADDPD ymm13, ymm13, ymm0
	VMOVUPD ymm2, [dword rcx + 192]
	VMOVUPD ymm7, [dword rdx + 160]
	VADDPD ymm4, ymm4, ymm8
	VMOVAPD [r8], ymm13
	ADD rcx, 224
	VMOVUPD ymm3, [dword rdx + 192]
	VADDPD ymm5, ymm5, ymm9
	VMOVAPD [byte r8 + 32], ymm4
	SUB r9, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD ymm13, [rcx]
	ADD rdx, 224
	VADDPD ymm12, ymm12, ymm10
	VMOVAPD [byte r8 + 64], ymm5
	VMOVUPD ymm4, [byte rcx + 32]
	VMOVUPD ymm0, [rdx]
	VADDPD ymm6, ymm6, ymm11
	VMOVAPD [byte r8 + 96], ymm12
	VMOVUPD ymm5, [byte rcx + 64]
	VMOVUPD ymm8, [byte rdx + 32]
	VADDPD ymm1, ymm1, ymm7
	VMOVAPD [dword r8 + 128], ymm6
	VMOVUPD ymm12, [byte rcx + 96]
	VMOVUPD ymm9, [byte rdx + 64]
	VADDPD ymm2, ymm2, ymm3
	VMOVAPD [dword r8 + 160], ymm1
	VMOVUPD ymm6, [dword rcx + 128]
	VMOVUPD ymm10, [byte rdx + 96]
	VMOVAPD [dword r8 + 192], ymm2
	VMOVUPD ymm1, [dword rcx + 160]
	VMOVUPD ymm11, [dword rdx + 128]
	VADDPD ymm13, ymm13, ymm0
	ADD r8, 224
	VMOVUPD ymm2, [dword rcx + 192]
	VMOVUPD ymm7, [dword rdx + 160]
	VADDPD ymm4, ymm4, ymm8
	VMOVAPD [r8], ymm13
	ADD rcx, 224
	VMOVUPD ymm3, [dword rdx + 192]
	VADDPD ymm5, ymm5, ymm9
	VMOVAPD [byte r8 + 32], ymm4
	SUB r9, 28
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 224
	VADDPD ymm12, ymm12, ymm10
	VMOVAPD [byte r8 + 64], ymm5
	VADDPD ymm6, ymm6, ymm11
	VMOVAPD [byte r8 + 96], ymm12
	VADDPD ymm1, ymm1, ymm7
	VMOVAPD [dword r8 + 128], ymm6
	VADDPD ymm2, ymm2, ymm3
	VMOVAPD [dword r8 + 160], ymm1
	VMOVAPD [dword r8 + 192], ymm2
	ADD r8, 224
	.batch_process_finish:
	ADD r9, 28
	JZ .return_ok
	.process_single:
	VMOVSD xmm4, [rcx]
	ADD rcx, 8
	VMOVSD xmm5, [rdx]
	ADD rdx, 8
	VADDSD xmm4, xmm4, xmm5
	VMOVSD [r8], xmm4
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm13, [rsp]
	VMOVAPS xmm14, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm12, [byte rsp + 48]
	VMOVAPS xmm9, [byte rsp + 64]
	VMOVAPS xmm6, [byte rsp + 80]
	VMOVAPS xmm10, [byte rsp + 96]
	VMOVAPS xmm11, [byte rsp + 112]
	VMOVAPS xmm7, [dword rsp + 128]
	ADD rsp, 152
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Add_V8sS8s_V16s_K10
_yepCore_Add_V8sS8s_V16s_K10:
	.ENTRY:
	SUB rsp, 120
	MOVAPS [rsp], xmm10
	MOVAPS [byte rsp + 16], xmm11
	MOVAPS [byte rsp + 32], xmm12
	MOVAPS [byte rsp + 48], xmm8
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm6
	MOVAPS [byte rsp + 96], xmm7
	MOVSX ax, dl
	MOVZX eax, ax
	IMUL eax, eax, 65537
	MOVD xmm10, eax
	PSHUFD xmm10, xmm10, 0
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 48
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm11, [rcx]
	PXOR xmm12, xmm12
	MOVQ xmm4, [byte rcx + 8]
	PXOR xmm8, xmm8
	MOVQ xmm2, [byte rcx + 16]
	PXOR xmm9, xmm9
	PCMPGTB xmm12, xmm11
	MOVQ xmm5, [byte rcx + 24]
	PXOR xmm0, xmm0
	PCMPGTB xmm8, xmm4
	PUNPCKLBW xmm11, xmm12
	MOVQ xmm6, [byte rcx + 32]
	PXOR xmm3, xmm3
	PCMPGTB xmm9, xmm2
	PUNPCKLBW xmm4, xmm8
	PADDW xmm11, xmm10
	MOVQ xmm7, [byte rcx + 40]
	PXOR xmm1, xmm1
	PCMPGTB xmm0, xmm5
	PUNPCKLBW xmm2, xmm9
	PADDW xmm4, xmm10
	MOVDQA [r8], xmm11
	ADD rcx, 48
	PCMPGTB xmm3, xmm6
	PUNPCKLBW xmm5, xmm0
	PADDW xmm2, xmm10
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 48
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm11, [rcx]
	PXOR xmm12, xmm12
	PCMPGTB xmm1, xmm7
	PUNPCKLBW xmm6, xmm3
	PADDW xmm5, xmm10
	MOVDQA [byte r8 + 32], xmm2
	MOVQ xmm4, [byte rcx + 8]
	PXOR xmm8, xmm8
	PUNPCKLBW xmm7, xmm1
	PADDW xmm6, xmm10
	MOVDQA [byte r8 + 48], xmm5
	MOVQ xmm2, [byte rcx + 16]
	PXOR xmm9, xmm9
	PCMPGTB xmm12, xmm11
	PADDW xmm7, xmm10
	MOVDQA [byte r8 + 64], xmm6
	MOVQ xmm5, [byte rcx + 24]
	PXOR xmm0, xmm0
	PCMPGTB xmm8, xmm4
	PUNPCKLBW xmm11, xmm12
	MOVDQA [byte r8 + 80], xmm7
	MOVQ xmm6, [byte rcx + 32]
	PXOR xmm3, xmm3
	PCMPGTB xmm9, xmm2
	PUNPCKLBW xmm4, xmm8
	PADDW xmm11, xmm10
	ADD r8, 96
	MOVQ xmm7, [byte rcx + 40]
	PXOR xmm1, xmm1
	PCMPGTB xmm0, xmm5
	PUNPCKLBW xmm2, xmm9
	PADDW xmm4, xmm10
	MOVDQA [r8], xmm11
	ADD rcx, 48
	PCMPGTB xmm3, xmm6
	PUNPCKLBW xmm5, xmm0
	PADDW xmm2, xmm10
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 48
	JAE .process_batch
	.process_batch_epilogue:
	PCMPGTB xmm1, xmm7
	PUNPCKLBW xmm6, xmm3
	PADDW xmm5, xmm10
	MOVDQA [byte r8 + 32], xmm2
	PUNPCKLBW xmm7, xmm1
	PADDW xmm6, xmm10
	MOVDQA [byte r8 + 48], xmm5
	PADDW xmm7, xmm10
	MOVDQA [byte r8 + 64], xmm6
	MOVDQA [byte r8 + 80], xmm7
	ADD r8, 96
	.batch_process_finish:
	ADD r9, 48
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm10, [rsp]
	MOVAPS xmm11, [byte rsp + 16]
	MOVAPS xmm12, [byte rsp + 32]
	MOVAPS xmm8, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm6, [byte rsp + 80]
	MOVAPS xmm7, [byte rsp + 96]
	ADD rsp, 120
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V8sS8s_V16s_Nehalem
_yepCore_Add_V8sS8s_V16s_Nehalem:
	.ENTRY:
	SUB rsp, 56
	MOVAPS [rsp], xmm7
	MOVAPS [byte rsp + 16], xmm8
	MOVAPS [byte rsp + 32], xmm6
	MOVSX ax, dl
	MOVZX eax, ax
	IMUL eax, eax, 65537
	MOVD xmm7, eax
	PSHUFD xmm7, xmm7, 0
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVSXBW xmm8, [rcx]
	PMOVSXBW xmm4, [byte rcx + 8]
	PMOVSXBW xmm3, [byte rcx + 16]
	PMOVSXBW xmm5, [byte rcx + 24]
	PMOVSXBW xmm1, [byte rcx + 32]
	PADDW xmm8, xmm7
	PMOVSXBW xmm0, [byte rcx + 40]
	PADDW xmm4, xmm7
	MOVDQA [r8], xmm8
	PMOVSXBW xmm2, [byte rcx + 48]
	PADDW xmm3, xmm7
	MOVDQA [byte r8 + 16], xmm4
	PMOVSXBW xmm6, [byte rcx + 56]
	PADDW xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	PADDW xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVSXBW xmm8, [rcx]
	PADDW xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	PMOVSXBW xmm4, [byte rcx + 8]
	PADDW xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	PMOVSXBW xmm3, [byte rcx + 16]
	PADDW xmm6, xmm7
	MOVDQA [byte r8 + 96], xmm2
	PMOVSXBW xmm5, [byte rcx + 24]
	MOVDQA [byte r8 + 112], xmm6
	PMOVSXBW xmm1, [byte rcx + 32]
	PADDW xmm8, xmm7
	ADD r8, 128
	PMOVSXBW xmm0, [byte rcx + 40]
	PADDW xmm4, xmm7
	MOVDQA [r8], xmm8
	PMOVSXBW xmm2, [byte rcx + 48]
	PADDW xmm3, xmm7
	MOVDQA [byte r8 + 16], xmm4
	PMOVSXBW xmm6, [byte rcx + 56]
	PADDW xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	PADDW xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	PADDW xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	PADDW xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	PADDW xmm6, xmm7
	MOVDQA [byte r8 + 96], xmm2
	MOVDQA [byte r8 + 112], xmm6
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm7, [rsp]
	MOVAPS xmm8, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	ADD rsp, 56
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V8sS8s_V16s_SandyBridge
_yepCore_Add_V8sS8s_V16s_SandyBridge:
	.ENTRY:
	SUB rsp, 56
	VMOVAPS [rsp], xmm7
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm6
	MOVSX ax, dl
	MOVZX eax, ax
	IMUL eax, eax, 65537
	MOVD xmm7, eax
	PSHUFD xmm7, xmm7, 0
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXBW xmm8, [rcx]
	VPMOVSXBW xmm4, [byte rcx + 8]
	VPMOVSXBW xmm3, [byte rcx + 16]
	VPMOVSXBW xmm5, [byte rcx + 24]
	VPMOVSXBW xmm1, [byte rcx + 32]
	VPADDW xmm8, xmm8, xmm7
	VPMOVSXBW xmm0, [byte rcx + 40]
	VPADDW xmm4, xmm4, xmm7
	VMOVDQA [r8], xmm8
	VPMOVSXBW xmm2, [byte rcx + 48]
	VPADDW xmm3, xmm3, xmm7
	VMOVDQA [byte r8 + 16], xmm4
	VPMOVSXBW xmm6, [byte rcx + 56]
	VPADDW xmm5, xmm5, xmm7
	VMOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	VPADDW xmm1, xmm1, xmm7
	VMOVDQA [byte r8 + 48], xmm5
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXBW xmm8, [rcx]
	VPADDW xmm0, xmm0, xmm7
	VMOVDQA [byte r8 + 64], xmm1
	VPMOVSXBW xmm4, [byte rcx + 8]
	VPADDW xmm2, xmm2, xmm7
	VMOVDQA [byte r8 + 80], xmm0
	VPMOVSXBW xmm3, [byte rcx + 16]
	VPADDW xmm6, xmm6, xmm7
	VMOVDQA [byte r8 + 96], xmm2
	VPMOVSXBW xmm5, [byte rcx + 24]
	VMOVDQA [byte r8 + 112], xmm6
	VPMOVSXBW xmm1, [byte rcx + 32]
	VPADDW xmm8, xmm8, xmm7
	ADD r8, 128
	VPMOVSXBW xmm0, [byte rcx + 40]
	VPADDW xmm4, xmm4, xmm7
	VMOVDQA [r8], xmm8
	VPMOVSXBW xmm2, [byte rcx + 48]
	VPADDW xmm3, xmm3, xmm7
	VMOVDQA [byte r8 + 16], xmm4
	VPMOVSXBW xmm6, [byte rcx + 56]
	VPADDW xmm5, xmm5, xmm7
	VMOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	VPADDW xmm1, xmm1, xmm7
	VMOVDQA [byte r8 + 48], xmm5
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	VPADDW xmm0, xmm0, xmm7
	VMOVDQA [byte r8 + 64], xmm1
	VPADDW xmm2, xmm2, xmm7
	VMOVDQA [byte r8 + 80], xmm0
	VPADDW xmm6, xmm6, xmm7
	VMOVDQA [byte r8 + 96], xmm2
	VMOVDQA [byte r8 + 112], xmm6
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm7, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	ADD rsp, 56
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Add_V8sS8s_V16s_Haswell
_yepCore_Add_V8sS8s_V16s_Haswell:
	.ENTRY:
	SUB rsp, 40
	VMOVAPS [rsp], xmm8
	VMOVAPS [byte rsp + 16], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXBW ymm8, [rcx]
	VPMOVSXBW ymm4, [byte rcx + 16]
	VPMOVSXBW ymm7, [byte rcx + 32]
	VPMOVSXBW ymm5, [byte rcx + 48]
	VPMOVSXBW ymm0, [byte rcx + 64]
	VPADDW ymm8, ymm8, ymm6
	VPMOVSXBW ymm3, [byte rcx + 80]
	VPADDW ymm4, ymm4, ymm6
	VMOVDQA [r8], ymm8
	VPMOVSXBW ymm2, [byte rcx + 96]
	VPADDW ymm7, ymm7, ymm6
	VMOVDQA [byte r8 + 32], ymm4
	VPMOVSXBW ymm1, [byte rcx + 112]
	VPADDW ymm5, ymm5, ymm6
	VMOVDQA [byte r8 + 64], ymm7
	ADD rcx, 128
	VPADDW ymm0, ymm0, ymm6
	VMOVDQA [byte r8 + 96], ymm5
	SUB r9, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXBW ymm8, [rcx]
	VPADDW ymm3, ymm3, ymm6
	VMOVDQA [dword r8 + 128], ymm0
	VPMOVSXBW ymm4, [byte rcx + 16]
	VPADDW ymm2, ymm2, ymm6
	VMOVDQA [dword r8 + 160], ymm3
	VPMOVSXBW ymm7, [byte rcx + 32]
	VPADDW ymm1, ymm1, ymm6
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVSXBW ymm5, [byte rcx + 48]
	VMOVDQA [dword r8 + 224], ymm1
	VPMOVSXBW ymm0, [byte rcx + 64]
	VPADDW ymm8, ymm8, ymm6
	ADD r8, 256
	VPMOVSXBW ymm3, [byte rcx + 80]
	VPADDW ymm4, ymm4, ymm6
	VMOVDQA [r8], ymm8
	VPMOVSXBW ymm2, [byte rcx + 96]
	VPADDW ymm7, ymm7, ymm6
	VMOVDQA [byte r8 + 32], ymm4
	VPMOVSXBW ymm1, [byte rcx + 112]
	VPADDW ymm5, ymm5, ymm6
	VMOVDQA [byte r8 + 64], ymm7
	ADD rcx, 128
	VPADDW ymm0, ymm0, ymm6
	VMOVDQA [byte r8 + 96], ymm5
	SUB r9, 128
	JAE .process_batch
	.process_batch_epilogue:
	VPADDW ymm3, ymm3, ymm6
	VMOVDQA [dword r8 + 128], ymm0
	VPADDW ymm2, ymm2, ymm6
	VMOVDQA [dword r8 + 160], ymm3
	VPADDW ymm1, ymm1, ymm6
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm1
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm8, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	ADD rsp, 40
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Add_V8uS8u_V16u_K10
_yepCore_Add_V8uS8u_V16u_K10:
	.ENTRY:
	SUB rsp, 56
	MOVAPS [rsp], xmm6
	MOVAPS [byte rsp + 16], xmm7
	MOVAPS [byte rsp + 32], xmm8
	PXOR xmm6, xmm6
	MOVZX eax, dl
	IMUL eax, eax, 65537
	MOVD xmm7, eax
	PSHUFD xmm7, xmm7, 0
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 56
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm8, [rcx]
	MOVQ xmm3, [byte rcx + 8]
	MOVQ xmm4, [byte rcx + 16]
	MOVQ xmm5, [byte rcx + 24]
	MOVQ xmm1, [byte rcx + 32]
	PUNPCKLBW xmm8, xmm6
	MOVQ xmm0, [byte rcx + 40]
	PUNPCKLBW xmm3, xmm6
	PADDW xmm8, xmm7
	MOVQ xmm2, [byte rcx + 48]
	PUNPCKLBW xmm4, xmm6
	PADDW xmm3, xmm7
	MOVDQA [r8], xmm8
	ADD rcx, 56
	PUNPCKLBW xmm5, xmm6
	PADDW xmm4, xmm7
	MOVDQA [byte r8 + 16], xmm3
	SUB r9, 56
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm8, [rcx]
	PUNPCKLBW xmm1, xmm6
	PADDW xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm4
	MOVQ xmm3, [byte rcx + 8]
	PUNPCKLBW xmm0, xmm6
	PADDW xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	MOVQ xmm4, [byte rcx + 16]
	PUNPCKLBW xmm2, xmm6
	PADDW xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	MOVQ xmm5, [byte rcx + 24]
	PADDW xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	MOVQ xmm1, [byte rcx + 32]
	PUNPCKLBW xmm8, xmm6
	MOVDQA [byte r8 + 96], xmm2
	MOVQ xmm0, [byte rcx + 40]
	PUNPCKLBW xmm3, xmm6
	PADDW xmm8, xmm7
	ADD r8, 112
	MOVQ xmm2, [byte rcx + 48]
	PUNPCKLBW xmm4, xmm6
	PADDW xmm3, xmm7
	MOVDQA [r8], xmm8
	ADD rcx, 56
	PUNPCKLBW xmm5, xmm6
	PADDW xmm4, xmm7
	MOVDQA [byte r8 + 16], xmm3
	SUB r9, 56
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLBW xmm1, xmm6
	PADDW xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm4
	PUNPCKLBW xmm0, xmm6
	PADDW xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	PUNPCKLBW xmm2, xmm6
	PADDW xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	PADDW xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	MOVDQA [byte r8 + 96], xmm2
	ADD r8, 112
	.batch_process_finish:
	ADD r9, 56
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm6, [rsp]
	MOVAPS xmm7, [byte rsp + 16]
	MOVAPS xmm8, [byte rsp + 32]
	ADD rsp, 56
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V8uS8u_V16u_Nehalem
_yepCore_Add_V8uS8u_V16u_Nehalem:
	.ENTRY:
	SUB rsp, 56
	MOVAPS [rsp], xmm7
	MOVAPS [byte rsp + 16], xmm8
	MOVAPS [byte rsp + 32], xmm6
	MOVZX eax, dl
	IMUL eax, eax, 65537
	MOVD xmm7, eax
	PSHUFD xmm7, xmm7, 0
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXBW xmm8, [rcx]
	PMOVZXBW xmm4, [byte rcx + 8]
	PMOVZXBW xmm3, [byte rcx + 16]
	PMOVZXBW xmm5, [byte rcx + 24]
	PMOVZXBW xmm1, [byte rcx + 32]
	PADDW xmm8, xmm7
	PMOVZXBW xmm0, [byte rcx + 40]
	PADDW xmm4, xmm7
	MOVDQA [r8], xmm8
	PMOVZXBW xmm2, [byte rcx + 48]
	PADDW xmm3, xmm7
	MOVDQA [byte r8 + 16], xmm4
	PMOVZXBW xmm6, [byte rcx + 56]
	PADDW xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	PADDW xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXBW xmm8, [rcx]
	PADDW xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	PMOVZXBW xmm4, [byte rcx + 8]
	PADDW xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	PMOVZXBW xmm3, [byte rcx + 16]
	PADDW xmm6, xmm7
	MOVDQA [byte r8 + 96], xmm2
	PMOVZXBW xmm5, [byte rcx + 24]
	MOVDQA [byte r8 + 112], xmm6
	PMOVZXBW xmm1, [byte rcx + 32]
	PADDW xmm8, xmm7
	ADD r8, 128
	PMOVZXBW xmm0, [byte rcx + 40]
	PADDW xmm4, xmm7
	MOVDQA [r8], xmm8
	PMOVZXBW xmm2, [byte rcx + 48]
	PADDW xmm3, xmm7
	MOVDQA [byte r8 + 16], xmm4
	PMOVZXBW xmm6, [byte rcx + 56]
	PADDW xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	PADDW xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	PADDW xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	PADDW xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	PADDW xmm6, xmm7
	MOVDQA [byte r8 + 96], xmm2
	MOVDQA [byte r8 + 112], xmm6
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm7, [rsp]
	MOVAPS xmm8, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	ADD rsp, 56
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V8uS8u_V16u_SandyBridge
_yepCore_Add_V8uS8u_V16u_SandyBridge:
	.ENTRY:
	SUB rsp, 56
	VMOVAPS [rsp], xmm7
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm6
	MOVZX eax, dl
	IMUL eax, eax, 65537
	MOVD xmm7, eax
	PSHUFD xmm7, xmm7, 0
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXBW xmm8, [rcx]
	VPMOVZXBW xmm4, [byte rcx + 8]
	VPMOVZXBW xmm3, [byte rcx + 16]
	VPMOVZXBW xmm5, [byte rcx + 24]
	VPMOVZXBW xmm1, [byte rcx + 32]
	VPADDW xmm8, xmm8, xmm7
	VPMOVZXBW xmm0, [byte rcx + 40]
	VPADDW xmm4, xmm4, xmm7
	VMOVDQA [r8], xmm8
	VPMOVZXBW xmm2, [byte rcx + 48]
	VPADDW xmm3, xmm3, xmm7
	VMOVDQA [byte r8 + 16], xmm4
	VPMOVZXBW xmm6, [byte rcx + 56]
	VPADDW xmm5, xmm5, xmm7
	VMOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	VPADDW xmm1, xmm1, xmm7
	VMOVDQA [byte r8 + 48], xmm5
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXBW xmm8, [rcx]
	VPADDW xmm0, xmm0, xmm7
	VMOVDQA [byte r8 + 64], xmm1
	VPMOVZXBW xmm4, [byte rcx + 8]
	VPADDW xmm2, xmm2, xmm7
	VMOVDQA [byte r8 + 80], xmm0
	VPMOVZXBW xmm3, [byte rcx + 16]
	VPADDW xmm6, xmm6, xmm7
	VMOVDQA [byte r8 + 96], xmm2
	VPMOVZXBW xmm5, [byte rcx + 24]
	VMOVDQA [byte r8 + 112], xmm6
	VPMOVZXBW xmm1, [byte rcx + 32]
	VPADDW xmm8, xmm8, xmm7
	ADD r8, 128
	VPMOVZXBW xmm0, [byte rcx + 40]
	VPADDW xmm4, xmm4, xmm7
	VMOVDQA [r8], xmm8
	VPMOVZXBW xmm2, [byte rcx + 48]
	VPADDW xmm3, xmm3, xmm7
	VMOVDQA [byte r8 + 16], xmm4
	VPMOVZXBW xmm6, [byte rcx + 56]
	VPADDW xmm5, xmm5, xmm7
	VMOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	VPADDW xmm1, xmm1, xmm7
	VMOVDQA [byte r8 + 48], xmm5
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	VPADDW xmm0, xmm0, xmm7
	VMOVDQA [byte r8 + 64], xmm1
	VPADDW xmm2, xmm2, xmm7
	VMOVDQA [byte r8 + 80], xmm0
	VPADDW xmm6, xmm6, xmm7
	VMOVDQA [byte r8 + 96], xmm2
	VMOVDQA [byte r8 + 112], xmm6
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm7, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	ADD rsp, 56
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Add_V8uS8u_V16u_Haswell
_yepCore_Add_V8uS8u_V16u_Haswell:
	.ENTRY:
	SUB rsp, 40
	VMOVAPS [rsp], xmm8
	VMOVAPS [byte rsp + 16], xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 1
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXBW ymm8, [rcx]
	VPMOVZXBW ymm4, [byte rcx + 16]
	VPMOVZXBW ymm7, [byte rcx + 32]
	VPMOVZXBW ymm5, [byte rcx + 48]
	VPMOVZXBW ymm0, [byte rcx + 64]
	VPADDW ymm8, ymm8, ymm6
	VPMOVZXBW ymm3, [byte rcx + 80]
	VPADDW ymm4, ymm4, ymm6
	VMOVDQA [r8], ymm8
	VPMOVZXBW ymm2, [byte rcx + 96]
	VPADDW ymm7, ymm7, ymm6
	VMOVDQA [byte r8 + 32], ymm4
	VPMOVZXBW ymm1, [byte rcx + 112]
	VPADDW ymm5, ymm5, ymm6
	VMOVDQA [byte r8 + 64], ymm7
	ADD rcx, 128
	VPADDW ymm0, ymm0, ymm6
	VMOVDQA [byte r8 + 96], ymm5
	SUB r9, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXBW ymm8, [rcx]
	VPADDW ymm3, ymm3, ymm6
	VMOVDQA [dword r8 + 128], ymm0
	VPMOVZXBW ymm4, [byte rcx + 16]
	VPADDW ymm2, ymm2, ymm6
	VMOVDQA [dword r8 + 160], ymm3
	VPMOVZXBW ymm7, [byte rcx + 32]
	VPADDW ymm1, ymm1, ymm6
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVZXBW ymm5, [byte rcx + 48]
	VMOVDQA [dword r8 + 224], ymm1
	VPMOVZXBW ymm0, [byte rcx + 64]
	VPADDW ymm8, ymm8, ymm6
	ADD r8, 256
	VPMOVZXBW ymm3, [byte rcx + 80]
	VPADDW ymm4, ymm4, ymm6
	VMOVDQA [r8], ymm8
	VPMOVZXBW ymm2, [byte rcx + 96]
	VPADDW ymm7, ymm7, ymm6
	VMOVDQA [byte r8 + 32], ymm4
	VPMOVZXBW ymm1, [byte rcx + 112]
	VPADDW ymm5, ymm5, ymm6
	VMOVDQA [byte r8 + 64], ymm7
	ADD rcx, 128
	VPADDW ymm0, ymm0, ymm6
	VMOVDQA [byte r8 + 96], ymm5
	SUB r9, 128
	JAE .process_batch
	.process_batch_epilogue:
	VPADDW ymm3, ymm3, ymm6
	VMOVDQA [dword r8 + 128], ymm0
	VPADDW ymm2, ymm2, ymm6
	VMOVDQA [dword r8 + 160], ymm3
	VPADDW ymm1, ymm1, ymm6
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm1
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 128
	JZ .return_ok
	.process_single:
	MOVZX eax, byte [rcx]
	ADD rcx, 1
	ADD eax, edx
	MOV [r8], ax
	ADD r8, 2
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm8, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	ADD rsp, 40
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Add_V16sS16s_V32s_K10
_yepCore_Add_V16sS16s_V32s_K10:
	.ENTRY:
	SUB rsp, 120
	MOVAPS [rsp], xmm10
	MOVAPS [byte rsp + 16], xmm11
	MOVAPS [byte rsp + 32], xmm12
	MOVAPS [byte rsp + 48], xmm8
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm6
	MOVAPS [byte rsp + 96], xmm7
	MOVSX rdx, dx
	MOVSX eax, dx
	MOVD xmm10, eax
	PSHUFD xmm10, xmm10, 0
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 24
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm11, [rcx]
	PXOR xmm12, xmm12
	MOVQ xmm4, [byte rcx + 8]
	PXOR xmm8, xmm8
	MOVQ xmm2, [byte rcx + 16]
	PXOR xmm9, xmm9
	PCMPGTW xmm12, xmm11
	MOVQ xmm5, [byte rcx + 24]
	PXOR xmm0, xmm0
	PCMPGTW xmm8, xmm4
	PUNPCKLWD xmm11, xmm12
	MOVQ xmm6, [byte rcx + 32]
	PXOR xmm3, xmm3
	PCMPGTW xmm9, xmm2
	PUNPCKLWD xmm4, xmm8
	PADDD xmm11, xmm10
	MOVQ xmm7, [byte rcx + 40]
	PXOR xmm1, xmm1
	PCMPGTW xmm0, xmm5
	PUNPCKLWD xmm2, xmm9
	PADDD xmm4, xmm10
	MOVDQA [r8], xmm11
	ADD rcx, 48
	PCMPGTW xmm3, xmm6
	PUNPCKLWD xmm5, xmm0
	PADDD xmm2, xmm10
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 24
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm11, [rcx]
	PXOR xmm12, xmm12
	PCMPGTW xmm1, xmm7
	PUNPCKLWD xmm6, xmm3
	PADDD xmm5, xmm10
	MOVDQA [byte r8 + 32], xmm2
	MOVQ xmm4, [byte rcx + 8]
	PXOR xmm8, xmm8
	PUNPCKLWD xmm7, xmm1
	PADDD xmm6, xmm10
	MOVDQA [byte r8 + 48], xmm5
	MOVQ xmm2, [byte rcx + 16]
	PXOR xmm9, xmm9
	PCMPGTW xmm12, xmm11
	PADDD xmm7, xmm10
	MOVDQA [byte r8 + 64], xmm6
	MOVQ xmm5, [byte rcx + 24]
	PXOR xmm0, xmm0
	PCMPGTW xmm8, xmm4
	PUNPCKLWD xmm11, xmm12
	MOVDQA [byte r8 + 80], xmm7
	MOVQ xmm6, [byte rcx + 32]
	PXOR xmm3, xmm3
	PCMPGTW xmm9, xmm2
	PUNPCKLWD xmm4, xmm8
	PADDD xmm11, xmm10
	ADD r8, 96
	MOVQ xmm7, [byte rcx + 40]
	PXOR xmm1, xmm1
	PCMPGTW xmm0, xmm5
	PUNPCKLWD xmm2, xmm9
	PADDD xmm4, xmm10
	MOVDQA [r8], xmm11
	ADD rcx, 48
	PCMPGTW xmm3, xmm6
	PUNPCKLWD xmm5, xmm0
	PADDD xmm2, xmm10
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 24
	JAE .process_batch
	.process_batch_epilogue:
	PCMPGTW xmm1, xmm7
	PUNPCKLWD xmm6, xmm3
	PADDD xmm5, xmm10
	MOVDQA [byte r8 + 32], xmm2
	PUNPCKLWD xmm7, xmm1
	PADDD xmm6, xmm10
	MOVDQA [byte r8 + 48], xmm5
	PADDD xmm7, xmm10
	MOVDQA [byte r8 + 64], xmm6
	MOVDQA [byte r8 + 80], xmm7
	ADD r8, 96
	.batch_process_finish:
	ADD r9, 24
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm10, [rsp]
	MOVAPS xmm11, [byte rsp + 16]
	MOVAPS xmm12, [byte rsp + 32]
	MOVAPS xmm8, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm6, [byte rsp + 80]
	MOVAPS xmm7, [byte rsp + 96]
	ADD rsp, 120
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V16sS16s_V32s_Nehalem
_yepCore_Add_V16sS16s_V32s_Nehalem:
	.ENTRY:
	SUB rsp, 56
	MOVAPS [rsp], xmm7
	MOVAPS [byte rsp + 16], xmm8
	MOVAPS [byte rsp + 32], xmm6
	MOVSX rdx, dx
	MOVSX eax, dx
	MOVD xmm7, eax
	PSHUFD xmm7, xmm7, 0
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVSXWD xmm8, [rcx]
	PMOVSXWD xmm4, [byte rcx + 8]
	PMOVSXWD xmm3, [byte rcx + 16]
	PMOVSXWD xmm5, [byte rcx + 24]
	PMOVSXWD xmm1, [byte rcx + 32]
	PADDD xmm8, xmm7
	PMOVSXWD xmm0, [byte rcx + 40]
	PADDD xmm4, xmm7
	MOVDQA [r8], xmm8
	PMOVSXWD xmm2, [byte rcx + 48]
	PADDD xmm3, xmm7
	MOVDQA [byte r8 + 16], xmm4
	PMOVSXWD xmm6, [byte rcx + 56]
	PADDD xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	PADDD xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVSXWD xmm8, [rcx]
	PADDD xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	PMOVSXWD xmm4, [byte rcx + 8]
	PADDD xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	PMOVSXWD xmm3, [byte rcx + 16]
	PADDD xmm6, xmm7
	MOVDQA [byte r8 + 96], xmm2
	PMOVSXWD xmm5, [byte rcx + 24]
	MOVDQA [byte r8 + 112], xmm6
	PMOVSXWD xmm1, [byte rcx + 32]
	PADDD xmm8, xmm7
	ADD r8, 128
	PMOVSXWD xmm0, [byte rcx + 40]
	PADDD xmm4, xmm7
	MOVDQA [r8], xmm8
	PMOVSXWD xmm2, [byte rcx + 48]
	PADDD xmm3, xmm7
	MOVDQA [byte r8 + 16], xmm4
	PMOVSXWD xmm6, [byte rcx + 56]
	PADDD xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	PADDD xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	PADDD xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	PADDD xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	PADDD xmm6, xmm7
	MOVDQA [byte r8 + 96], xmm2
	MOVDQA [byte r8 + 112], xmm6
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm7, [rsp]
	MOVAPS xmm8, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	ADD rsp, 56
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V16sS16s_V32s_SandyBridge
_yepCore_Add_V16sS16s_V32s_SandyBridge:
	.ENTRY:
	SUB rsp, 56
	VMOVAPS [rsp], xmm7
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm6
	MOVSX rdx, dx
	MOVSX eax, dx
	MOVD xmm7, eax
	PSHUFD xmm7, xmm7, 0
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXWD xmm8, [rcx]
	VPMOVSXWD xmm4, [byte rcx + 8]
	VPMOVSXWD xmm3, [byte rcx + 16]
	VPMOVSXWD xmm5, [byte rcx + 24]
	VPMOVSXWD xmm1, [byte rcx + 32]
	VPADDD xmm8, xmm8, xmm7
	VPMOVSXWD xmm0, [byte rcx + 40]
	VPADDD xmm4, xmm4, xmm7
	VMOVDQA [r8], xmm8
	VPMOVSXWD xmm2, [byte rcx + 48]
	VPADDD xmm3, xmm3, xmm7
	VMOVDQA [byte r8 + 16], xmm4
	VPMOVSXWD xmm6, [byte rcx + 56]
	VPADDD xmm5, xmm5, xmm7
	VMOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	VPADDD xmm1, xmm1, xmm7
	VMOVDQA [byte r8 + 48], xmm5
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXWD xmm8, [rcx]
	VPADDD xmm0, xmm0, xmm7
	VMOVDQA [byte r8 + 64], xmm1
	VPMOVSXWD xmm4, [byte rcx + 8]
	VPADDD xmm2, xmm2, xmm7
	VMOVDQA [byte r8 + 80], xmm0
	VPMOVSXWD xmm3, [byte rcx + 16]
	VPADDD xmm6, xmm6, xmm7
	VMOVDQA [byte r8 + 96], xmm2
	VPMOVSXWD xmm5, [byte rcx + 24]
	VMOVDQA [byte r8 + 112], xmm6
	VPMOVSXWD xmm1, [byte rcx + 32]
	VPADDD xmm8, xmm8, xmm7
	ADD r8, 128
	VPMOVSXWD xmm0, [byte rcx + 40]
	VPADDD xmm4, xmm4, xmm7
	VMOVDQA [r8], xmm8
	VPMOVSXWD xmm2, [byte rcx + 48]
	VPADDD xmm3, xmm3, xmm7
	VMOVDQA [byte r8 + 16], xmm4
	VPMOVSXWD xmm6, [byte rcx + 56]
	VPADDD xmm5, xmm5, xmm7
	VMOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	VPADDD xmm1, xmm1, xmm7
	VMOVDQA [byte r8 + 48], xmm5
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	VPADDD xmm0, xmm0, xmm7
	VMOVDQA [byte r8 + 64], xmm1
	VPADDD xmm2, xmm2, xmm7
	VMOVDQA [byte r8 + 80], xmm0
	VPADDD xmm6, xmm6, xmm7
	VMOVDQA [byte r8 + 96], xmm2
	VMOVDQA [byte r8 + 112], xmm6
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm7, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	ADD rsp, 56
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Add_V16sS16s_V32s_Haswell
_yepCore_Add_V16sS16s_V32s_Haswell:
	.ENTRY:
	SUB rsp, 40
	VMOVAPS [rsp], xmm8
	VMOVAPS [byte rsp + 16], xmm7
	MOVSX rdx, dx
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXWD ymm8, [rcx]
	VPMOVSXWD ymm4, [byte rcx + 16]
	VPMOVSXWD ymm7, [byte rcx + 32]
	VPMOVSXWD ymm5, [byte rcx + 48]
	VPMOVSXWD ymm0, [byte rcx + 64]
	VPADDD ymm8, ymm8, ymm6
	VPMOVSXWD ymm3, [byte rcx + 80]
	VPADDD ymm4, ymm4, ymm6
	VMOVDQA [r8], ymm8
	VPMOVSXWD ymm2, [byte rcx + 96]
	VPADDD ymm7, ymm7, ymm6
	VMOVDQA [byte r8 + 32], ymm4
	VPMOVSXWD ymm1, [byte rcx + 112]
	VPADDD ymm5, ymm5, ymm6
	VMOVDQA [byte r8 + 64], ymm7
	ADD rcx, 128
	VPADDD ymm0, ymm0, ymm6
	VMOVDQA [byte r8 + 96], ymm5
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXWD ymm8, [rcx]
	VPADDD ymm3, ymm3, ymm6
	VMOVDQA [dword r8 + 128], ymm0
	VPMOVSXWD ymm4, [byte rcx + 16]
	VPADDD ymm2, ymm2, ymm6
	VMOVDQA [dword r8 + 160], ymm3
	VPMOVSXWD ymm7, [byte rcx + 32]
	VPADDD ymm1, ymm1, ymm6
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVSXWD ymm5, [byte rcx + 48]
	VMOVDQA [dword r8 + 224], ymm1
	VPMOVSXWD ymm0, [byte rcx + 64]
	VPADDD ymm8, ymm8, ymm6
	ADD r8, 256
	VPMOVSXWD ymm3, [byte rcx + 80]
	VPADDD ymm4, ymm4, ymm6
	VMOVDQA [r8], ymm8
	VPMOVSXWD ymm2, [byte rcx + 96]
	VPADDD ymm7, ymm7, ymm6
	VMOVDQA [byte r8 + 32], ymm4
	VPMOVSXWD ymm1, [byte rcx + 112]
	VPADDD ymm5, ymm5, ymm6
	VMOVDQA [byte r8 + 64], ymm7
	ADD rcx, 128
	VPADDD ymm0, ymm0, ymm6
	VMOVDQA [byte r8 + 96], ymm5
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	VPADDD ymm3, ymm3, ymm6
	VMOVDQA [dword r8 + 128], ymm0
	VPADDD ymm2, ymm2, ymm6
	VMOVDQA [dword r8 + 160], ymm3
	VPADDD ymm1, ymm1, ymm6
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm1
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm8, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	ADD rsp, 40
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Add_V16uS16u_V32u_K10
_yepCore_Add_V16uS16u_V32u_K10:
	.ENTRY:
	SUB rsp, 56
	MOVAPS [rsp], xmm6
	MOVAPS [byte rsp + 16], xmm7
	MOVAPS [byte rsp + 32], xmm8
	MOVZX edx, dx
	PXOR xmm6, xmm6
	MOVZX eax, dx
	MOVD xmm7, eax
	PSHUFD xmm7, xmm7, 0
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 28
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm8, [rcx]
	MOVQ xmm3, [byte rcx + 8]
	MOVQ xmm4, [byte rcx + 16]
	MOVQ xmm5, [byte rcx + 24]
	MOVQ xmm1, [byte rcx + 32]
	PUNPCKLWD xmm8, xmm6
	MOVQ xmm0, [byte rcx + 40]
	PUNPCKLWD xmm3, xmm6
	PADDD xmm8, xmm7
	MOVQ xmm2, [byte rcx + 48]
	PUNPCKLWD xmm4, xmm6
	PADDD xmm3, xmm7
	MOVDQA [r8], xmm8
	ADD rcx, 56
	PUNPCKLWD xmm5, xmm6
	PADDD xmm4, xmm7
	MOVDQA [byte r8 + 16], xmm3
	SUB r9, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm8, [rcx]
	PUNPCKLWD xmm1, xmm6
	PADDD xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm4
	MOVQ xmm3, [byte rcx + 8]
	PUNPCKLWD xmm0, xmm6
	PADDD xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	MOVQ xmm4, [byte rcx + 16]
	PUNPCKLWD xmm2, xmm6
	PADDD xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	MOVQ xmm5, [byte rcx + 24]
	PADDD xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	MOVQ xmm1, [byte rcx + 32]
	PUNPCKLWD xmm8, xmm6
	MOVDQA [byte r8 + 96], xmm2
	MOVQ xmm0, [byte rcx + 40]
	PUNPCKLWD xmm3, xmm6
	PADDD xmm8, xmm7
	ADD r8, 112
	MOVQ xmm2, [byte rcx + 48]
	PUNPCKLWD xmm4, xmm6
	PADDD xmm3, xmm7
	MOVDQA [r8], xmm8
	ADD rcx, 56
	PUNPCKLWD xmm5, xmm6
	PADDD xmm4, xmm7
	MOVDQA [byte r8 + 16], xmm3
	SUB r9, 28
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLWD xmm1, xmm6
	PADDD xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm4
	PUNPCKLWD xmm0, xmm6
	PADDD xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	PUNPCKLWD xmm2, xmm6
	PADDD xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	PADDD xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	MOVDQA [byte r8 + 96], xmm2
	ADD r8, 112
	.batch_process_finish:
	ADD r9, 28
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm6, [rsp]
	MOVAPS xmm7, [byte rsp + 16]
	MOVAPS xmm8, [byte rsp + 32]
	ADD rsp, 56
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V16uS16u_V32u_Nehalem
_yepCore_Add_V16uS16u_V32u_Nehalem:
	.ENTRY:
	SUB rsp, 56
	MOVAPS [rsp], xmm7
	MOVAPS [byte rsp + 16], xmm8
	MOVAPS [byte rsp + 32], xmm6
	MOVZX edx, dx
	MOVZX eax, dx
	MOVD xmm7, eax
	PSHUFD xmm7, xmm7, 0
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXWD xmm8, [rcx]
	PMOVZXWD xmm4, [byte rcx + 8]
	PMOVZXWD xmm3, [byte rcx + 16]
	PMOVZXWD xmm5, [byte rcx + 24]
	PMOVZXWD xmm1, [byte rcx + 32]
	PADDD xmm8, xmm7
	PMOVZXWD xmm0, [byte rcx + 40]
	PADDD xmm4, xmm7
	MOVDQA [r8], xmm8
	PMOVZXWD xmm2, [byte rcx + 48]
	PADDD xmm3, xmm7
	MOVDQA [byte r8 + 16], xmm4
	PMOVZXWD xmm6, [byte rcx + 56]
	PADDD xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	PADDD xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXWD xmm8, [rcx]
	PADDD xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	PMOVZXWD xmm4, [byte rcx + 8]
	PADDD xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	PMOVZXWD xmm3, [byte rcx + 16]
	PADDD xmm6, xmm7
	MOVDQA [byte r8 + 96], xmm2
	PMOVZXWD xmm5, [byte rcx + 24]
	MOVDQA [byte r8 + 112], xmm6
	PMOVZXWD xmm1, [byte rcx + 32]
	PADDD xmm8, xmm7
	ADD r8, 128
	PMOVZXWD xmm0, [byte rcx + 40]
	PADDD xmm4, xmm7
	MOVDQA [r8], xmm8
	PMOVZXWD xmm2, [byte rcx + 48]
	PADDD xmm3, xmm7
	MOVDQA [byte r8 + 16], xmm4
	PMOVZXWD xmm6, [byte rcx + 56]
	PADDD xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	PADDD xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	PADDD xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	PADDD xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	PADDD xmm6, xmm7
	MOVDQA [byte r8 + 96], xmm2
	MOVDQA [byte r8 + 112], xmm6
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm7, [rsp]
	MOVAPS xmm8, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	ADD rsp, 56
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V16uS16u_V32u_SandyBridge
_yepCore_Add_V16uS16u_V32u_SandyBridge:
	.ENTRY:
	SUB rsp, 56
	VMOVAPS [rsp], xmm7
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm6
	MOVZX edx, dx
	MOVZX eax, dx
	MOVD xmm7, eax
	PSHUFD xmm7, xmm7, 0
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXWD xmm8, [rcx]
	VPMOVZXWD xmm4, [byte rcx + 8]
	VPMOVZXWD xmm3, [byte rcx + 16]
	VPMOVZXWD xmm5, [byte rcx + 24]
	VPMOVZXWD xmm1, [byte rcx + 32]
	VPADDD xmm8, xmm8, xmm7
	VPMOVZXWD xmm0, [byte rcx + 40]
	VPADDD xmm4, xmm4, xmm7
	VMOVDQA [r8], xmm8
	VPMOVZXWD xmm2, [byte rcx + 48]
	VPADDD xmm3, xmm3, xmm7
	VMOVDQA [byte r8 + 16], xmm4
	VPMOVZXWD xmm6, [byte rcx + 56]
	VPADDD xmm5, xmm5, xmm7
	VMOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	VPADDD xmm1, xmm1, xmm7
	VMOVDQA [byte r8 + 48], xmm5
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXWD xmm8, [rcx]
	VPADDD xmm0, xmm0, xmm7
	VMOVDQA [byte r8 + 64], xmm1
	VPMOVZXWD xmm4, [byte rcx + 8]
	VPADDD xmm2, xmm2, xmm7
	VMOVDQA [byte r8 + 80], xmm0
	VPMOVZXWD xmm3, [byte rcx + 16]
	VPADDD xmm6, xmm6, xmm7
	VMOVDQA [byte r8 + 96], xmm2
	VPMOVZXWD xmm5, [byte rcx + 24]
	VMOVDQA [byte r8 + 112], xmm6
	VPMOVZXWD xmm1, [byte rcx + 32]
	VPADDD xmm8, xmm8, xmm7
	ADD r8, 128
	VPMOVZXWD xmm0, [byte rcx + 40]
	VPADDD xmm4, xmm4, xmm7
	VMOVDQA [r8], xmm8
	VPMOVZXWD xmm2, [byte rcx + 48]
	VPADDD xmm3, xmm3, xmm7
	VMOVDQA [byte r8 + 16], xmm4
	VPMOVZXWD xmm6, [byte rcx + 56]
	VPADDD xmm5, xmm5, xmm7
	VMOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	VPADDD xmm1, xmm1, xmm7
	VMOVDQA [byte r8 + 48], xmm5
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	VPADDD xmm0, xmm0, xmm7
	VMOVDQA [byte r8 + 64], xmm1
	VPADDD xmm2, xmm2, xmm7
	VMOVDQA [byte r8 + 80], xmm0
	VPADDD xmm6, xmm6, xmm7
	VMOVDQA [byte r8 + 96], xmm2
	VMOVDQA [byte r8 + 112], xmm6
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm7, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	ADD rsp, 56
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Add_V16uS16u_V32u_Haswell
_yepCore_Add_V16uS16u_V32u_Haswell:
	.ENTRY:
	SUB rsp, 40
	VMOVAPS [rsp], xmm8
	VMOVAPS [byte rsp + 16], xmm7
	MOVZX edx, dx
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 1
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXWD ymm8, [rcx]
	VPMOVZXWD ymm4, [byte rcx + 16]
	VPMOVZXWD ymm7, [byte rcx + 32]
	VPMOVZXWD ymm5, [byte rcx + 48]
	VPMOVZXWD ymm0, [byte rcx + 64]
	VPADDD ymm8, ymm8, ymm6
	VPMOVZXWD ymm3, [byte rcx + 80]
	VPADDD ymm4, ymm4, ymm6
	VMOVDQA [r8], ymm8
	VPMOVZXWD ymm2, [byte rcx + 96]
	VPADDD ymm7, ymm7, ymm6
	VMOVDQA [byte r8 + 32], ymm4
	VPMOVZXWD ymm1, [byte rcx + 112]
	VPADDD ymm5, ymm5, ymm6
	VMOVDQA [byte r8 + 64], ymm7
	ADD rcx, 128
	VPADDD ymm0, ymm0, ymm6
	VMOVDQA [byte r8 + 96], ymm5
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXWD ymm8, [rcx]
	VPADDD ymm3, ymm3, ymm6
	VMOVDQA [dword r8 + 128], ymm0
	VPMOVZXWD ymm4, [byte rcx + 16]
	VPADDD ymm2, ymm2, ymm6
	VMOVDQA [dword r8 + 160], ymm3
	VPMOVZXWD ymm7, [byte rcx + 32]
	VPADDD ymm1, ymm1, ymm6
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVZXWD ymm5, [byte rcx + 48]
	VMOVDQA [dword r8 + 224], ymm1
	VPMOVZXWD ymm0, [byte rcx + 64]
	VPADDD ymm8, ymm8, ymm6
	ADD r8, 256
	VPMOVZXWD ymm3, [byte rcx + 80]
	VPADDD ymm4, ymm4, ymm6
	VMOVDQA [r8], ymm8
	VPMOVZXWD ymm2, [byte rcx + 96]
	VPADDD ymm7, ymm7, ymm6
	VMOVDQA [byte r8 + 32], ymm4
	VPMOVZXWD ymm1, [byte rcx + 112]
	VPADDD ymm5, ymm5, ymm6
	VMOVDQA [byte r8 + 64], ymm7
	ADD rcx, 128
	VPADDD ymm0, ymm0, ymm6
	VMOVDQA [byte r8 + 96], ymm5
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	VPADDD ymm3, ymm3, ymm6
	VMOVDQA [dword r8 + 128], ymm0
	VPADDD ymm2, ymm2, ymm6
	VMOVDQA [dword r8 + 160], ymm3
	VPADDD ymm1, ymm1, ymm6
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm1
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 64
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rcx]
	ADD rcx, 2
	ADD eax, edx
	MOV [r8], eax
	ADD r8, 4
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm8, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	ADD rsp, 40
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Add_V32uS32u_V64u_K10
_yepCore_Add_V32uS32u_V64u_K10:
	.ENTRY:
	SUB rsp, 56
	MOVAPS [rsp], xmm6
	MOVAPS [byte rsp + 16], xmm7
	MOVAPS [byte rsp + 32], xmm8
	MOV edx, edx
	PXOR xmm6, xmm6
	MOVD xmm7, edx
	PUNPCKLQDQ xmm7, xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 14
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm8, [rcx]
	MOVQ xmm3, [byte rcx + 8]
	MOVQ xmm4, [byte rcx + 16]
	MOVQ xmm5, [byte rcx + 24]
	MOVQ xmm1, [byte rcx + 32]
	PUNPCKLDQ xmm8, xmm6
	MOVQ xmm0, [byte rcx + 40]
	PUNPCKLDQ xmm3, xmm6
	PADDQ xmm8, xmm7
	MOVQ xmm2, [byte rcx + 48]
	PUNPCKLDQ xmm4, xmm6
	PADDQ xmm3, xmm7
	MOVDQA [r8], xmm8
	ADD rcx, 56
	PUNPCKLDQ xmm5, xmm6
	PADDQ xmm4, xmm7
	MOVDQA [byte r8 + 16], xmm3
	SUB r9, 14
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm8, [rcx]
	PUNPCKLDQ xmm1, xmm6
	PADDQ xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm4
	MOVQ xmm3, [byte rcx + 8]
	PUNPCKLDQ xmm0, xmm6
	PADDQ xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	MOVQ xmm4, [byte rcx + 16]
	PUNPCKLDQ xmm2, xmm6
	PADDQ xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	MOVQ xmm5, [byte rcx + 24]
	PADDQ xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	MOVQ xmm1, [byte rcx + 32]
	PUNPCKLDQ xmm8, xmm6
	MOVDQA [byte r8 + 96], xmm2
	MOVQ xmm0, [byte rcx + 40]
	PUNPCKLDQ xmm3, xmm6
	PADDQ xmm8, xmm7
	ADD r8, 112
	MOVQ xmm2, [byte rcx + 48]
	PUNPCKLDQ xmm4, xmm6
	PADDQ xmm3, xmm7
	MOVDQA [r8], xmm8
	ADD rcx, 56
	PUNPCKLDQ xmm5, xmm6
	PADDQ xmm4, xmm7
	MOVDQA [byte r8 + 16], xmm3
	SUB r9, 14
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLDQ xmm1, xmm6
	PADDQ xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm4
	PUNPCKLDQ xmm0, xmm6
	PADDQ xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	PUNPCKLDQ xmm2, xmm6
	PADDQ xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	PADDQ xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	MOVDQA [byte r8 + 96], xmm2
	ADD r8, 112
	.batch_process_finish:
	ADD r9, 14
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm6, [rsp]
	MOVAPS xmm7, [byte rsp + 16]
	MOVAPS xmm8, [byte rsp + 32]
	ADD rsp, 56
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V32uS32u_V64u_Nehalem
_yepCore_Add_V32uS32u_V64u_Nehalem:
	.ENTRY:
	SUB rsp, 56
	MOVAPS [rsp], xmm7
	MOVAPS [byte rsp + 16], xmm8
	MOVAPS [byte rsp + 32], xmm6
	MOV edx, edx
	MOVD xmm7, edx
	PUNPCKLQDQ xmm7, xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXDQ xmm8, [rcx]
	PMOVZXDQ xmm4, [byte rcx + 8]
	PMOVZXDQ xmm3, [byte rcx + 16]
	PMOVZXDQ xmm5, [byte rcx + 24]
	PMOVZXDQ xmm1, [byte rcx + 32]
	PADDQ xmm8, xmm7
	PMOVZXDQ xmm0, [byte rcx + 40]
	PADDQ xmm4, xmm7
	MOVDQA [r8], xmm8
	PMOVZXDQ xmm2, [byte rcx + 48]
	PADDQ xmm3, xmm7
	MOVDQA [byte r8 + 16], xmm4
	PMOVZXDQ xmm6, [byte rcx + 56]
	PADDQ xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	PADDQ xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXDQ xmm8, [rcx]
	PADDQ xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	PMOVZXDQ xmm4, [byte rcx + 8]
	PADDQ xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	PMOVZXDQ xmm3, [byte rcx + 16]
	PADDQ xmm6, xmm7
	MOVDQA [byte r8 + 96], xmm2
	PMOVZXDQ xmm5, [byte rcx + 24]
	MOVDQA [byte r8 + 112], xmm6
	PMOVZXDQ xmm1, [byte rcx + 32]
	PADDQ xmm8, xmm7
	ADD r8, 128
	PMOVZXDQ xmm0, [byte rcx + 40]
	PADDQ xmm4, xmm7
	MOVDQA [r8], xmm8
	PMOVZXDQ xmm2, [byte rcx + 48]
	PADDQ xmm3, xmm7
	MOVDQA [byte r8 + 16], xmm4
	PMOVZXDQ xmm6, [byte rcx + 56]
	PADDQ xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	PADDQ xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	PADDQ xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	PADDQ xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	PADDQ xmm6, xmm7
	MOVDQA [byte r8 + 96], xmm2
	MOVDQA [byte r8 + 112], xmm6
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm7, [rsp]
	MOVAPS xmm8, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	ADD rsp, 56
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V32uS32u_V64u_SandyBridge
_yepCore_Add_V32uS32u_V64u_SandyBridge:
	.ENTRY:
	SUB rsp, 56
	VMOVAPS [rsp], xmm7
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm6
	MOV edx, edx
	MOVD xmm7, edx
	PUNPCKLQDQ xmm7, xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ xmm8, [rcx]
	VPMOVZXDQ xmm4, [byte rcx + 8]
	VPMOVZXDQ xmm3, [byte rcx + 16]
	VPMOVZXDQ xmm5, [byte rcx + 24]
	VPMOVZXDQ xmm1, [byte rcx + 32]
	VPADDQ xmm8, xmm8, xmm7
	VPMOVZXDQ xmm0, [byte rcx + 40]
	VPADDQ xmm4, xmm4, xmm7
	VMOVDQA [r8], xmm8
	VPMOVZXDQ xmm2, [byte rcx + 48]
	VPADDQ xmm3, xmm3, xmm7
	VMOVDQA [byte r8 + 16], xmm4
	VPMOVZXDQ xmm6, [byte rcx + 56]
	VPADDQ xmm5, xmm5, xmm7
	VMOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	VPADDQ xmm1, xmm1, xmm7
	VMOVDQA [byte r8 + 48], xmm5
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ xmm8, [rcx]
	VPADDQ xmm0, xmm0, xmm7
	VMOVDQA [byte r8 + 64], xmm1
	VPMOVZXDQ xmm4, [byte rcx + 8]
	VPADDQ xmm2, xmm2, xmm7
	VMOVDQA [byte r8 + 80], xmm0
	VPMOVZXDQ xmm3, [byte rcx + 16]
	VPADDQ xmm6, xmm6, xmm7
	VMOVDQA [byte r8 + 96], xmm2
	VPMOVZXDQ xmm5, [byte rcx + 24]
	VMOVDQA [byte r8 + 112], xmm6
	VPMOVZXDQ xmm1, [byte rcx + 32]
	VPADDQ xmm8, xmm8, xmm7
	ADD r8, 128
	VPMOVZXDQ xmm0, [byte rcx + 40]
	VPADDQ xmm4, xmm4, xmm7
	VMOVDQA [r8], xmm8
	VPMOVZXDQ xmm2, [byte rcx + 48]
	VPADDQ xmm3, xmm3, xmm7
	VMOVDQA [byte r8 + 16], xmm4
	VPMOVZXDQ xmm6, [byte rcx + 56]
	VPADDQ xmm5, xmm5, xmm7
	VMOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	VPADDQ xmm1, xmm1, xmm7
	VMOVDQA [byte r8 + 48], xmm5
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	VPADDQ xmm0, xmm0, xmm7
	VMOVDQA [byte r8 + 64], xmm1
	VPADDQ xmm2, xmm2, xmm7
	VMOVDQA [byte r8 + 80], xmm0
	VPADDQ xmm6, xmm6, xmm7
	VMOVDQA [byte r8 + 96], xmm2
	VMOVDQA [byte r8 + 112], xmm6
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm7, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	ADD rsp, 56
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Add_V32uS32u_V64u_Haswell
_yepCore_Add_V32uS32u_V64u_Haswell:
	.ENTRY:
	SUB rsp, 40
	VMOVAPS [rsp], xmm8
	VMOVAPS [byte rsp + 16], xmm7
	MOV edx, edx
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOV eax, [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ ymm8, [rcx]
	VPMOVZXDQ ymm4, [byte rcx + 16]
	VPMOVZXDQ ymm7, [byte rcx + 32]
	VPMOVZXDQ ymm5, [byte rcx + 48]
	VPMOVZXDQ ymm0, [byte rcx + 64]
	VPADDQ ymm8, ymm8, ymm6
	VPMOVZXDQ ymm3, [byte rcx + 80]
	VPADDQ ymm4, ymm4, ymm6
	VMOVDQA [r8], ymm8
	VPMOVZXDQ ymm2, [byte rcx + 96]
	VPADDQ ymm7, ymm7, ymm6
	VMOVDQA [byte r8 + 32], ymm4
	VPMOVZXDQ ymm1, [byte rcx + 112]
	VPADDQ ymm5, ymm5, ymm6
	VMOVDQA [byte r8 + 64], ymm7
	ADD rcx, 128
	VPADDQ ymm0, ymm0, ymm6
	VMOVDQA [byte r8 + 96], ymm5
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ ymm8, [rcx]
	VPADDQ ymm3, ymm3, ymm6
	VMOVDQA [dword r8 + 128], ymm0
	VPMOVZXDQ ymm4, [byte rcx + 16]
	VPADDQ ymm2, ymm2, ymm6
	VMOVDQA [dword r8 + 160], ymm3
	VPMOVZXDQ ymm7, [byte rcx + 32]
	VPADDQ ymm1, ymm1, ymm6
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVZXDQ ymm5, [byte rcx + 48]
	VMOVDQA [dword r8 + 224], ymm1
	VPMOVZXDQ ymm0, [byte rcx + 64]
	VPADDQ ymm8, ymm8, ymm6
	ADD r8, 256
	VPMOVZXDQ ymm3, [byte rcx + 80]
	VPADDQ ymm4, ymm4, ymm6
	VMOVDQA [r8], ymm8
	VPMOVZXDQ ymm2, [byte rcx + 96]
	VPADDQ ymm7, ymm7, ymm6
	VMOVDQA [byte r8 + 32], ymm4
	VPMOVZXDQ ymm1, [byte rcx + 112]
	VPADDQ ymm5, ymm5, ymm6
	VMOVDQA [byte r8 + 64], ymm7
	ADD rcx, 128
	VPADDQ ymm0, ymm0, ymm6
	VMOVDQA [byte r8 + 96], ymm5
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	VPADDQ ymm3, ymm3, ymm6
	VMOVDQA [dword r8 + 128], ymm0
	VPADDQ ymm2, ymm2, ymm6
	VMOVDQA [dword r8 + 160], ymm3
	VPADDQ ymm1, ymm1, ymm6
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm1
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm8, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	ADD rsp, 40
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$m code align=16
global _yepCore_Add_V32sS32s_V64s_K10
_yepCore_Add_V32sS32s_V64s_K10:
	.ENTRY:
	SUB rsp, 120
	MOVAPS [rsp], xmm10
	MOVAPS [byte rsp + 16], xmm11
	MOVAPS [byte rsp + 32], xmm12
	MOVAPS [byte rsp + 48], xmm8
	MOVAPS [byte rsp + 64], xmm9
	MOVAPS [byte rsp + 80], xmm6
	MOVAPS [byte rsp + 96], xmm7
	MOVSX rdx, edx
	MOVSX rax, edx
	MOVQ xmm10, rax
	PUNPCKLQDQ xmm10, xmm10
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 12
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm11, [rcx]
	PXOR xmm12, xmm12
	MOVQ xmm4, [byte rcx + 8]
	PXOR xmm8, xmm8
	MOVQ xmm2, [byte rcx + 16]
	PXOR xmm9, xmm9
	PCMPGTD xmm12, xmm11
	MOVQ xmm5, [byte rcx + 24]
	PXOR xmm0, xmm0
	PCMPGTD xmm8, xmm4
	PUNPCKLDQ xmm11, xmm12
	MOVQ xmm6, [byte rcx + 32]
	PXOR xmm3, xmm3
	PCMPGTD xmm9, xmm2
	PUNPCKLDQ xmm4, xmm8
	PADDQ xmm11, xmm10
	MOVQ xmm7, [byte rcx + 40]
	PXOR xmm1, xmm1
	PCMPGTD xmm0, xmm5
	PUNPCKLDQ xmm2, xmm9
	PADDQ xmm4, xmm10
	MOVDQA [r8], xmm11
	ADD rcx, 48
	PCMPGTD xmm3, xmm6
	PUNPCKLDQ xmm5, xmm0
	PADDQ xmm2, xmm10
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 12
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm11, [rcx]
	PXOR xmm12, xmm12
	PCMPGTD xmm1, xmm7
	PUNPCKLDQ xmm6, xmm3
	PADDQ xmm5, xmm10
	MOVDQA [byte r8 + 32], xmm2
	MOVQ xmm4, [byte rcx + 8]
	PXOR xmm8, xmm8
	PUNPCKLDQ xmm7, xmm1
	PADDQ xmm6, xmm10
	MOVDQA [byte r8 + 48], xmm5
	MOVQ xmm2, [byte rcx + 16]
	PXOR xmm9, xmm9
	PCMPGTD xmm12, xmm11
	PADDQ xmm7, xmm10
	MOVDQA [byte r8 + 64], xmm6
	MOVQ xmm5, [byte rcx + 24]
	PXOR xmm0, xmm0
	PCMPGTD xmm8, xmm4
	PUNPCKLDQ xmm11, xmm12
	MOVDQA [byte r8 + 80], xmm7
	MOVQ xmm6, [byte rcx + 32]
	PXOR xmm3, xmm3
	PCMPGTD xmm9, xmm2
	PUNPCKLDQ xmm4, xmm8
	PADDQ xmm11, xmm10
	ADD r8, 96
	MOVQ xmm7, [byte rcx + 40]
	PXOR xmm1, xmm1
	PCMPGTD xmm0, xmm5
	PUNPCKLDQ xmm2, xmm9
	PADDQ xmm4, xmm10
	MOVDQA [r8], xmm11
	ADD rcx, 48
	PCMPGTD xmm3, xmm6
	PUNPCKLDQ xmm5, xmm0
	PADDQ xmm2, xmm10
	MOVDQA [byte r8 + 16], xmm4
	SUB r9, 12
	JAE .process_batch
	.process_batch_epilogue:
	PCMPGTD xmm1, xmm7
	PUNPCKLDQ xmm6, xmm3
	PADDQ xmm5, xmm10
	MOVDQA [byte r8 + 32], xmm2
	PUNPCKLDQ xmm7, xmm1
	PADDQ xmm6, xmm10
	MOVDQA [byte r8 + 48], xmm5
	PADDQ xmm7, xmm10
	MOVDQA [byte r8 + 64], xmm6
	MOVDQA [byte r8 + 80], xmm7
	ADD r8, 96
	.batch_process_finish:
	ADD r9, 12
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm10, [rsp]
	MOVAPS xmm11, [byte rsp + 16]
	MOVAPS xmm12, [byte rsp + 32]
	MOVAPS xmm8, [byte rsp + 48]
	MOVAPS xmm9, [byte rsp + 64]
	MOVAPS xmm6, [byte rsp + 80]
	MOVAPS xmm7, [byte rsp + 96]
	ADD rsp, 120
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_Add_V32sS32s_V64s_Nehalem
_yepCore_Add_V32sS32s_V64s_Nehalem:
	.ENTRY:
	SUB rsp, 56
	MOVAPS [rsp], xmm7
	MOVAPS [byte rsp + 16], xmm8
	MOVAPS [byte rsp + 32], xmm6
	MOVSX rdx, edx
	MOVSX rax, edx
	MOVQ xmm7, rax
	PUNPCKLQDQ xmm7, xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVSXDQ xmm8, [rcx]
	PMOVSXDQ xmm4, [byte rcx + 8]
	PMOVSXDQ xmm3, [byte rcx + 16]
	PMOVSXDQ xmm5, [byte rcx + 24]
	PMOVSXDQ xmm1, [byte rcx + 32]
	PADDQ xmm8, xmm7
	PMOVSXDQ xmm0, [byte rcx + 40]
	PADDQ xmm4, xmm7
	MOVDQA [r8], xmm8
	PMOVSXDQ xmm2, [byte rcx + 48]
	PADDQ xmm3, xmm7
	MOVDQA [byte r8 + 16], xmm4
	PMOVSXDQ xmm6, [byte rcx + 56]
	PADDQ xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	PADDQ xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVSXDQ xmm8, [rcx]
	PADDQ xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	PMOVSXDQ xmm4, [byte rcx + 8]
	PADDQ xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	PMOVSXDQ xmm3, [byte rcx + 16]
	PADDQ xmm6, xmm7
	MOVDQA [byte r8 + 96], xmm2
	PMOVSXDQ xmm5, [byte rcx + 24]
	MOVDQA [byte r8 + 112], xmm6
	PMOVSXDQ xmm1, [byte rcx + 32]
	PADDQ xmm8, xmm7
	ADD r8, 128
	PMOVSXDQ xmm0, [byte rcx + 40]
	PADDQ xmm4, xmm7
	MOVDQA [r8], xmm8
	PMOVSXDQ xmm2, [byte rcx + 48]
	PADDQ xmm3, xmm7
	MOVDQA [byte r8 + 16], xmm4
	PMOVSXDQ xmm6, [byte rcx + 56]
	PADDQ xmm5, xmm7
	MOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	PADDQ xmm1, xmm7
	MOVDQA [byte r8 + 48], xmm5
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	PADDQ xmm0, xmm7
	MOVDQA [byte r8 + 64], xmm1
	PADDQ xmm2, xmm7
	MOVDQA [byte r8 + 80], xmm0
	PADDQ xmm6, xmm7
	MOVDQA [byte r8 + 96], xmm2
	MOVDQA [byte r8 + 112], xmm6
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm7, [rsp]
	MOVAPS xmm8, [byte rsp + 16]
	MOVAPS xmm6, [byte rsp + 32]
	ADD rsp, 56
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_Add_V32sS32s_V64s_SandyBridge
_yepCore_Add_V32sS32s_V64s_SandyBridge:
	.ENTRY:
	SUB rsp, 56
	VMOVAPS [rsp], xmm7
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm6
	MOVSX rdx, edx
	MOVSX rax, edx
	MOVQ xmm7, rax
	PUNPCKLQDQ xmm7, xmm7
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXDQ xmm8, [rcx]
	VPMOVSXDQ xmm4, [byte rcx + 8]
	VPMOVSXDQ xmm3, [byte rcx + 16]
	VPMOVSXDQ xmm5, [byte rcx + 24]
	VPMOVSXDQ xmm1, [byte rcx + 32]
	VPADDQ xmm8, xmm8, xmm7
	VPMOVSXDQ xmm0, [byte rcx + 40]
	VPADDQ xmm4, xmm4, xmm7
	VMOVDQA [r8], xmm8
	VPMOVSXDQ xmm2, [byte rcx + 48]
	VPADDQ xmm3, xmm3, xmm7
	VMOVDQA [byte r8 + 16], xmm4
	VPMOVSXDQ xmm6, [byte rcx + 56]
	VPADDQ xmm5, xmm5, xmm7
	VMOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	VPADDQ xmm1, xmm1, xmm7
	VMOVDQA [byte r8 + 48], xmm5
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXDQ xmm8, [rcx]
	VPADDQ xmm0, xmm0, xmm7
	VMOVDQA [byte r8 + 64], xmm1
	VPMOVSXDQ xmm4, [byte rcx + 8]
	VPADDQ xmm2, xmm2, xmm7
	VMOVDQA [byte r8 + 80], xmm0
	VPMOVSXDQ xmm3, [byte rcx + 16]
	VPADDQ xmm6, xmm6, xmm7
	VMOVDQA [byte r8 + 96], xmm2
	VPMOVSXDQ xmm5, [byte rcx + 24]
	VMOVDQA [byte r8 + 112], xmm6
	VPMOVSXDQ xmm1, [byte rcx + 32]
	VPADDQ xmm8, xmm8, xmm7
	ADD r8, 128
	VPMOVSXDQ xmm0, [byte rcx + 40]
	VPADDQ xmm4, xmm4, xmm7
	VMOVDQA [r8], xmm8
	VPMOVSXDQ xmm2, [byte rcx + 48]
	VPADDQ xmm3, xmm3, xmm7
	VMOVDQA [byte r8 + 16], xmm4
	VPMOVSXDQ xmm6, [byte rcx + 56]
	VPADDQ xmm5, xmm5, xmm7
	VMOVDQA [byte r8 + 32], xmm3
	ADD rcx, 64
	VPADDQ xmm1, xmm1, xmm7
	VMOVDQA [byte r8 + 48], xmm5
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	VPADDQ xmm0, xmm0, xmm7
	VMOVDQA [byte r8 + 64], xmm1
	VPADDQ xmm2, xmm2, xmm7
	VMOVDQA [byte r8 + 80], xmm0
	VPADDQ xmm6, xmm6, xmm7
	VMOVDQA [byte r8 + 96], xmm2
	VMOVDQA [byte r8 + 112], xmm6
	ADD r8, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm7, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm6, [byte rsp + 32]
	ADD rsp, 56
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_Add_V32sS32s_V64s_Haswell
_yepCore_Add_V32sS32s_V64s_Haswell:
	.ENTRY:
	SUB rsp, 40
	VMOVAPS [rsp], xmm8
	VMOVAPS [byte rsp + 16], xmm7
	MOVSX rdx, edx
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST r9, r9
	JZ .return_ok
	TEST r8, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JZ .return_ok
	TEST r8, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXDQ ymm8, [rcx]
	VPMOVSXDQ ymm4, [byte rcx + 16]
	VPMOVSXDQ ymm7, [byte rcx + 32]
	VPMOVSXDQ ymm5, [byte rcx + 48]
	VPMOVSXDQ ymm0, [byte rcx + 64]
	VPADDQ ymm8, ymm8, ymm6
	VPMOVSXDQ ymm3, [byte rcx + 80]
	VPADDQ ymm4, ymm4, ymm6
	VMOVDQA [r8], ymm8
	VPMOVSXDQ ymm2, [byte rcx + 96]
	VPADDQ ymm7, ymm7, ymm6
	VMOVDQA [byte r8 + 32], ymm4
	VPMOVSXDQ ymm1, [byte rcx + 112]
	VPADDQ ymm5, ymm5, ymm6
	VMOVDQA [byte r8 + 64], ymm7
	ADD rcx, 128
	VPADDQ ymm0, ymm0, ymm6
	VMOVDQA [byte r8 + 96], ymm5
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXDQ ymm8, [rcx]
	VPADDQ ymm3, ymm3, ymm6
	VMOVDQA [dword r8 + 128], ymm0
	VPMOVSXDQ ymm4, [byte rcx + 16]
	VPADDQ ymm2, ymm2, ymm6
	VMOVDQA [dword r8 + 160], ymm3
	VPMOVSXDQ ymm7, [byte rcx + 32]
	VPADDQ ymm1, ymm1, ymm6
	VMOVDQA [dword r8 + 192], ymm2
	VPMOVSXDQ ymm5, [byte rcx + 48]
	VMOVDQA [dword r8 + 224], ymm1
	VPMOVSXDQ ymm0, [byte rcx + 64]
	VPADDQ ymm8, ymm8, ymm6
	ADD r8, 256
	VPMOVSXDQ ymm3, [byte rcx + 80]
	VPADDQ ymm4, ymm4, ymm6
	VMOVDQA [r8], ymm8
	VPMOVSXDQ ymm2, [byte rcx + 96]
	VPADDQ ymm7, ymm7, ymm6
	VMOVDQA [byte r8 + 32], ymm4
	VPMOVSXDQ ymm1, [byte rcx + 112]
	VPADDQ ymm5, ymm5, ymm6
	VMOVDQA [byte r8 + 64], ymm7
	ADD rcx, 128
	VPADDQ ymm0, ymm0, ymm6
	VMOVDQA [byte r8 + 96], ymm5
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	VPADDQ ymm3, ymm3, ymm6
	VMOVDQA [dword r8 + 128], ymm0
	VPADDQ ymm2, ymm2, ymm6
	VMOVDQA [dword r8 + 160], ymm3
	VPADDQ ymm1, ymm1, ymm6
	VMOVDQA [dword r8 + 192], ymm2
	VMOVDQA [dword r8 + 224], ymm1
	ADD r8, 256
	.batch_process_finish:
	ADD r9, 32
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rcx]
	ADD rcx, 4
	ADD rax, rdx
	MOV [r8], rax
	ADD r8, 8
	SUB r9, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm8, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	ADD rsp, 40
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return
