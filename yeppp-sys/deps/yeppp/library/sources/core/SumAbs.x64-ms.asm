;                       Yeppp! library implementation
;                   This file is auto-generated by Peach-Py,
;        Portable Efficient Assembly Code-generator in Higher-level Python,
;                  part of the Yeppp! library infrastructure
; This file is part of Yeppp! library and licensed under the New BSD license.
; See LICENSE.txt for the full text of the license.

section .rdata$e rdata align=32
_yepCore_SumAbs_V32f_S32f_Nehalem_constants:
	.c0: DD 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF


section .text$e code align=16
global _yepCore_SumAbs_V32f_S32f_Nehalem
_yepCore_SumAbs_V32f_S32f_Nehalem:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm6
	MOVAPS [byte rsp + 16], xmm7
	MOVAPS [byte rsp + 32], xmm8
	MOVAPS [byte rsp + 48], xmm9
	MOVAPS [byte rsp + 64], xmm10
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm12
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm14
	MOVDQA xmm0,  [rel _yepCore_SumAbs_V32f_S32f_Nehalem_constants.c0]
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	XORPS xmm6, xmm6
	TEST r8, r8
	JZ .return_ok
	XORPS xmm7, xmm7
	XORPS xmm8, xmm8
	XORPS xmm9, xmm9
	XORPS xmm10, xmm10
	XORPS xmm11, xmm11
	XORPS xmm12, xmm12
	TEST rcx, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSS xmm13, [rcx]
	ANDPS xmm13, xmm0
	ADDPS xmm6, xmm13
	ADD rcx, 4
	SUB r8, 1
	JZ .reduce_batch
	TEST rcx, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB r8, 28
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPS xmm13, [rcx]
	MOVUPS xmm4, [byte rcx + 16]
	MOVUPS xmm5, [byte rcx + 32]
	MOVUPS xmm14, [byte rcx + 48]
	ANDPS xmm13, xmm0
	MOVUPS xmm2, [byte rcx + 64]
	ANDPS xmm4, xmm0
	ADDPS xmm6, xmm13
	MOVUPS xmm1, [byte rcx + 80]
	ANDPS xmm5, xmm0
	ADDPS xmm7, xmm4
	MOVUPS xmm3, [byte rcx + 96]
	ANDPS xmm14, xmm0
	ADDPS xmm8, xmm5
	ADD rcx, 112
	ANDPS xmm2, xmm0
	ADDPS xmm9, xmm14
	SUB r8, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPS xmm13, [rcx]
	ANDPS xmm1, xmm0
	ADDPS xmm10, xmm2
	MOVUPS xmm4, [byte rcx + 16]
	ANDPS xmm3, xmm0
	ADDPS xmm11, xmm1
	MOVUPS xmm5, [byte rcx + 32]
	ADDPS xmm12, xmm3
	MOVUPS xmm14, [byte rcx + 48]
	ANDPS xmm13, xmm0
	MOVUPS xmm2, [byte rcx + 64]
	ANDPS xmm4, xmm0
	ADDPS xmm6, xmm13
	MOVUPS xmm1, [byte rcx + 80]
	ANDPS xmm5, xmm0
	ADDPS xmm7, xmm4
	MOVUPS xmm3, [byte rcx + 96]
	ANDPS xmm14, xmm0
	ADDPS xmm8, xmm5
	ADD rcx, 112
	ANDPS xmm2, xmm0
	ADDPS xmm9, xmm14
	SUB r8, 28
	JAE .process_batch
	.process_batch_epilogue:
	ANDPS xmm1, xmm0
	ADDPS xmm10, xmm2
	ANDPS xmm3, xmm0
	ADDPS xmm11, xmm1
	ADDPS xmm12, xmm3
	.batch_process_finish:
	ADD r8, 28
	JZ .reduce_batch
	.process_single:
	MOVSS xmm4, [rcx]
	ANDPS xmm4, xmm0
	ADDPS xmm6, xmm4
	ADD rcx, 4
	SUB r8, 1
	JNZ .process_single
	.reduce_batch:
	ADDPS xmm6, xmm7
	ADDPS xmm8, xmm9
	ADDPS xmm10, xmm11
	ADDPS xmm6, xmm8
	ADDPS xmm10, xmm12
	ADDPS xmm6, xmm10
	MOVHLPS xmm4, xmm6
	ADDPS xmm6, xmm4
	MOVSHDUP xmm4, xmm6
	ADDSS xmm6, xmm4
	.return_ok:
	MOVSS [rdx], xmm6
	XOR eax, eax
	.return:
	MOVAPS xmm6, [rsp]
	MOVAPS xmm7, [byte rsp + 16]
	MOVAPS xmm8, [byte rsp + 32]
	MOVAPS xmm9, [byte rsp + 48]
	MOVAPS xmm10, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm12, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm14, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .rdata$f rdata align=32
_yepCore_SumAbs_V32f_S32f_SandyBridge_constants:
	.c0: DD 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF


section .text$f code align=16
global _yepCore_SumAbs_V32f_S32f_SandyBridge
_yepCore_SumAbs_V32f_S32f_SandyBridge:
	.ENTRY:
	SUB rsp, 152
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm14
	VMOVDQA ymm6,  [rel _yepCore_SumAbs_V32f_S32f_SandyBridge_constants.c0]
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm7, xmm7, xmm7
	TEST r8, r8
	JZ .return_ok
	VXORPS xmm8, xmm8, xmm8
	VXORPS xmm9, xmm9, xmm9
	VXORPS xmm10, xmm10, xmm10
	VXORPS xmm11, xmm11, xmm11
	VXORPS xmm12, xmm12, xmm12
	VXORPS xmm13, xmm13, xmm13
	TEST rcx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm14, [rcx]
	VANDPS xmm14, xmm14, xmm6
	VADDPS ymm7, ymm7, ymm14
	ADD rcx, 4
	SUB r8, 1
	JZ .reduce_batch
	TEST rcx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 56
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS ymm14, [rcx]
	VMOVUPS ymm4, [byte rcx + 32]
	VMOVUPS ymm5, [byte rcx + 64]
	VMOVUPS ymm2, [byte rcx + 96]
	VMOVUPS ymm0, [dword rcx + 128]
	VANDPS ymm14, ymm14, ymm6
	VMOVUPS ymm1, [dword rcx + 160]
	VANDPS ymm4, ymm4, ymm6
	VADDPS ymm7, ymm7, ymm14
	VMOVUPS ymm3, [dword rcx + 192]
	VANDPS ymm5, ymm5, ymm6
	VADDPS ymm8, ymm8, ymm4
	ADD rcx, 224
	VANDPS ymm2, ymm2, ymm6
	VADDPS ymm9, ymm9, ymm5
	SUB r8, 56
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS ymm14, [rcx]
	VANDPS ymm0, ymm0, ymm6
	VADDPS ymm10, ymm10, ymm2
	VMOVUPS ymm4, [byte rcx + 32]
	VANDPS ymm1, ymm1, ymm6
	VADDPS ymm11, ymm11, ymm0
	VMOVUPS ymm5, [byte rcx + 64]
	VANDPS ymm3, ymm3, ymm6
	VADDPS ymm12, ymm12, ymm1
	VMOVUPS ymm2, [byte rcx + 96]
	VADDPS ymm13, ymm13, ymm3
	VMOVUPS ymm0, [dword rcx + 128]
	VANDPS ymm14, ymm14, ymm6
	VMOVUPS ymm1, [dword rcx + 160]
	VANDPS ymm4, ymm4, ymm6
	VADDPS ymm7, ymm7, ymm14
	VMOVUPS ymm3, [dword rcx + 192]
	VANDPS ymm5, ymm5, ymm6
	VADDPS ymm8, ymm8, ymm4
	ADD rcx, 224
	VANDPS ymm2, ymm2, ymm6
	VADDPS ymm9, ymm9, ymm5
	SUB r8, 56
	JAE .process_batch
	.process_batch_epilogue:
	VANDPS ymm0, ymm0, ymm6
	VADDPS ymm10, ymm10, ymm2
	VANDPS ymm1, ymm1, ymm6
	VADDPS ymm11, ymm11, ymm0
	VANDPS ymm3, ymm3, ymm6
	VADDPS ymm12, ymm12, ymm1
	VADDPS ymm13, ymm13, ymm3
	.batch_process_finish:
	ADD r8, 56
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm4, [rcx]
	VANDPS xmm4, xmm4, xmm6
	VADDPS ymm7, ymm7, ymm4
	ADD rcx, 4
	SUB r8, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS ymm7, ymm7, ymm8
	VADDPS ymm9, ymm9, ymm10
	VADDPS ymm11, ymm11, ymm12
	VADDPS ymm7, ymm7, ymm9
	VADDPS ymm11, ymm11, ymm13
	VADDPS ymm7, ymm7, ymm11
	VEXTRACTF128 xmm4, ymm7, 1
	VADDPS xmm7, xmm7, xmm4
	VUNPCKHPD xmm4, xmm7, xmm7
	VADDPS xmm7, xmm7, xmm4
	VMOVSHDUP xmm4, xmm7
	VADDSS xmm7, xmm7, xmm4
	.return_ok:
	VMOVSS [rdx], xmm7
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm14, [dword rsp + 128]
	ADD rsp, 152
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .rdata$n rdata align=32
_yepCore_SumAbs_V32f_S32f_Bulldozer_constants:
	.c0: DD 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF


section .text$n code align=16
global _yepCore_SumAbs_V32f_S32f_Bulldozer
_yepCore_SumAbs_V32f_S32f_Bulldozer:
	.ENTRY:
	SUB rsp, 120
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	VMOVDQA ymm0,  [rel _yepCore_SumAbs_V32f_S32f_Bulldozer_constants.c0]
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm6, xmm6, xmm6
	TEST r8, r8
	JZ .return_ok
	VXORPS xmm7, xmm7, xmm7
	VXORPS xmm8, xmm8, xmm8
	VXORPS xmm9, xmm9, xmm9
	VXORPS xmm10, xmm10, xmm10
	VXORPS xmm11, xmm11, xmm11
	TEST rcx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm12, [rcx]
	VANDPS xmm12, xmm12, xmm0
	VADDPS xmm6, xmm6, xmm12
	ADD rcx, 4
	SUB r8, 1
	JZ .reduce_batch
	TEST rcx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS xmm12, [rcx]
	VMOVUPS xmm4, [byte rcx + 16]
	VMOVUPS ymm5, [byte rcx + 32]
	VMOVUPS xmm2, [byte rcx + 64]
	VANDPS xmm12, xmm12, xmm0
	VMOVUPS xmm3, [byte rcx + 80]
	VANDPS xmm4, xmm4, xmm0
	VADDPS xmm6, xmm6, xmm12
	VMOVUPS ymm1, [byte rcx + 96]
	VANDPS ymm5, ymm5, ymm0
	VADDPS xmm7, xmm7, xmm4
	ADD rcx, 128
	VANDPS xmm2, xmm2, xmm0
	VADDPS ymm8, ymm8, ymm5
	SUB r8, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS xmm12, [rcx]
	VANDPS xmm3, xmm3, xmm0
	VADDPS xmm9, xmm9, xmm2
	VMOVUPS xmm4, [byte rcx + 16]
	VANDPS ymm1, ymm1, ymm0
	VADDPS xmm10, xmm10, xmm3
	VMOVUPS ymm5, [byte rcx + 32]
	VADDPS ymm11, ymm11, ymm1
	VMOVUPS xmm2, [byte rcx + 64]
	VANDPS xmm12, xmm12, xmm0
	VMOVUPS xmm3, [byte rcx + 80]
	VANDPS xmm4, xmm4, xmm0
	VADDPS xmm6, xmm6, xmm12
	VMOVUPS ymm1, [byte rcx + 96]
	VANDPS ymm5, ymm5, ymm0
	VADDPS xmm7, xmm7, xmm4
	ADD rcx, 128
	VANDPS xmm2, xmm2, xmm0
	VADDPS ymm8, ymm8, ymm5
	SUB r8, 32
	JAE .process_batch
	.process_batch_epilogue:
	VANDPS xmm3, xmm3, xmm0
	VADDPS xmm9, xmm9, xmm2
	VANDPS ymm1, ymm1, ymm0
	VADDPS xmm10, xmm10, xmm3
	VADDPS ymm11, ymm11, ymm1
	.batch_process_finish:
	ADD r8, 32
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm4, [rcx]
	VANDPS xmm4, xmm4, xmm0
	VADDPS xmm6, xmm6, xmm4
	ADD rcx, 4
	SUB r8, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS xmm6, xmm6, xmm7
	VADDPS ymm8, ymm8, ymm9
	VADDPS ymm10, ymm10, ymm11
	VADDPS ymm6, ymm6, ymm8
	VADDPS ymm6, ymm6, ymm10
	VEXTRACTF128 xmm4, ymm6, 1
	VADDPS xmm6, xmm6, xmm4
	VUNPCKHPD xmm4, xmm6, xmm6
	VADDPS xmm6, xmm6, xmm4
	VMOVSHDUP xmm4, xmm6
	VADDSS xmm6, xmm6, xmm4
	.return_ok:
	VMOVSS [rdx], xmm6
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	ADD rsp, 120
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .rdata$e rdata align=32
_yepCore_SumAbs_V64f_S64f_Nehalem_constants:
	.c0: DQ 0x7FFFFFFFFFFFFFFF, 0x7FFFFFFFFFFFFFFF


section .text$e code align=16
global _yepCore_SumAbs_V64f_S64f_Nehalem
_yepCore_SumAbs_V64f_S64f_Nehalem:
	.ENTRY:
	SUB rsp, 152
	MOVAPS [rsp], xmm6
	MOVAPS [byte rsp + 16], xmm7
	MOVAPS [byte rsp + 32], xmm8
	MOVAPS [byte rsp + 48], xmm9
	MOVAPS [byte rsp + 64], xmm10
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm12
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm14
	MOVDQA xmm0,  [rel _yepCore_SumAbs_V64f_S64f_Nehalem_constants.c0]
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	XORPD xmm6, xmm6
	TEST r8, r8
	JZ .return_ok
	XORPD xmm7, xmm7
	XORPD xmm8, xmm8
	XORPD xmm9, xmm9
	XORPD xmm10, xmm10
	XORPD xmm11, xmm11
	XORPD xmm12, xmm12
	TEST rcx, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSD xmm13, [rcx]
	ANDPD xmm13, xmm0
	ADDPD xmm6, xmm13
	ADD rcx, 8
	SUB r8, 1
	JZ .reduce_batch
	TEST rcx, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB r8, 14
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPD xmm13, [rcx]
	MOVUPD xmm4, [byte rcx + 16]
	MOVUPD xmm5, [byte rcx + 32]
	MOVUPD xmm14, [byte rcx + 48]
	ANDPD xmm13, xmm0
	MOVUPD xmm2, [byte rcx + 64]
	ANDPD xmm4, xmm0
	ADDPD xmm6, xmm13
	MOVUPD xmm1, [byte rcx + 80]
	ANDPD xmm5, xmm0
	ADDPD xmm7, xmm4
	MOVUPD xmm3, [byte rcx + 96]
	ANDPD xmm14, xmm0
	ADDPD xmm8, xmm5
	ADD rcx, 112
	ANDPD xmm2, xmm0
	ADDPD xmm9, xmm14
	SUB r8, 14
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPD xmm13, [rcx]
	ANDPD xmm1, xmm0
	ADDPD xmm10, xmm2
	MOVUPD xmm4, [byte rcx + 16]
	ANDPD xmm3, xmm0
	ADDPD xmm11, xmm1
	MOVUPD xmm5, [byte rcx + 32]
	ADDPD xmm12, xmm3
	MOVUPD xmm14, [byte rcx + 48]
	ANDPD xmm13, xmm0
	MOVUPD xmm2, [byte rcx + 64]
	ANDPD xmm4, xmm0
	ADDPD xmm6, xmm13
	MOVUPD xmm1, [byte rcx + 80]
	ANDPD xmm5, xmm0
	ADDPD xmm7, xmm4
	MOVUPD xmm3, [byte rcx + 96]
	ANDPD xmm14, xmm0
	ADDPD xmm8, xmm5
	ADD rcx, 112
	ANDPD xmm2, xmm0
	ADDPD xmm9, xmm14
	SUB r8, 14
	JAE .process_batch
	.process_batch_epilogue:
	ANDPD xmm1, xmm0
	ADDPD xmm10, xmm2
	ANDPD xmm3, xmm0
	ADDPD xmm11, xmm1
	ADDPD xmm12, xmm3
	.batch_process_finish:
	ADD r8, 14
	JZ .reduce_batch
	.process_single:
	MOVSD xmm4, [rcx]
	ANDPD xmm4, xmm0
	ADDPD xmm6, xmm4
	ADD rcx, 8
	SUB r8, 1
	JNZ .process_single
	.reduce_batch:
	ADDPD xmm6, xmm7
	ADDPD xmm8, xmm9
	ADDPD xmm10, xmm11
	ADDPD xmm6, xmm8
	ADDPD xmm10, xmm12
	ADDPD xmm6, xmm10
	MOVHLPS xmm4, xmm6
	ADDSD xmm6, xmm4
	.return_ok:
	MOVSD [rdx], xmm6
	XOR eax, eax
	.return:
	MOVAPS xmm6, [rsp]
	MOVAPS xmm7, [byte rsp + 16]
	MOVAPS xmm8, [byte rsp + 32]
	MOVAPS xmm9, [byte rsp + 48]
	MOVAPS xmm10, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm12, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm14, [dword rsp + 128]
	ADD rsp, 152
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .rdata$f rdata align=32
_yepCore_SumAbs_V64f_S64f_SandyBridge_constants:
	.c0: DQ 0x7FFFFFFFFFFFFFFF, 0x7FFFFFFFFFFFFFFF, 0x7FFFFFFFFFFFFFFF, 0x7FFFFFFFFFFFFFFF


section .text$f code align=16
global _yepCore_SumAbs_V64f_S64f_SandyBridge
_yepCore_SumAbs_V64f_S64f_SandyBridge:
	.ENTRY:
	SUB rsp, 152
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm14
	VMOVDQA ymm6,  [rel _yepCore_SumAbs_V64f_S64f_SandyBridge_constants.c0]
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm7, xmm7, xmm7
	TEST r8, r8
	JZ .return_ok
	VXORPD xmm8, xmm8, xmm8
	VXORPD xmm9, xmm9, xmm9
	VXORPD xmm10, xmm10, xmm10
	VXORPD xmm11, xmm11, xmm11
	VXORPD xmm12, xmm12, xmm12
	VXORPD xmm13, xmm13, xmm13
	TEST rcx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm14, [rcx]
	VANDPD xmm14, xmm14, xmm6
	VADDPD ymm7, ymm7, ymm14
	ADD rcx, 8
	SUB r8, 1
	JZ .reduce_batch
	TEST rcx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 28
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD ymm14, [rcx]
	VMOVUPD ymm4, [byte rcx + 32]
	VMOVUPD ymm5, [byte rcx + 64]
	VMOVUPD ymm2, [byte rcx + 96]
	VMOVUPD ymm0, [dword rcx + 128]
	VANDPD ymm14, ymm14, ymm6
	VMOVUPD ymm1, [dword rcx + 160]
	VANDPD ymm4, ymm4, ymm6
	VADDPD ymm7, ymm7, ymm14
	VMOVUPD ymm3, [dword rcx + 192]
	VANDPD ymm5, ymm5, ymm6
	VADDPD ymm8, ymm8, ymm4
	ADD rcx, 224
	VANDPD ymm2, ymm2, ymm6
	VADDPD ymm9, ymm9, ymm5
	SUB r8, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD ymm14, [rcx]
	VANDPD ymm0, ymm0, ymm6
	VADDPD ymm10, ymm10, ymm2
	VMOVUPD ymm4, [byte rcx + 32]
	VANDPD ymm1, ymm1, ymm6
	VADDPD ymm11, ymm11, ymm0
	VMOVUPD ymm5, [byte rcx + 64]
	VANDPD ymm3, ymm3, ymm6
	VADDPD ymm12, ymm12, ymm1
	VMOVUPD ymm2, [byte rcx + 96]
	VADDPD ymm13, ymm13, ymm3
	VMOVUPD ymm0, [dword rcx + 128]
	VANDPD ymm14, ymm14, ymm6
	VMOVUPD ymm1, [dword rcx + 160]
	VANDPD ymm4, ymm4, ymm6
	VADDPD ymm7, ymm7, ymm14
	VMOVUPD ymm3, [dword rcx + 192]
	VANDPD ymm5, ymm5, ymm6
	VADDPD ymm8, ymm8, ymm4
	ADD rcx, 224
	VANDPD ymm2, ymm2, ymm6
	VADDPD ymm9, ymm9, ymm5
	SUB r8, 28
	JAE .process_batch
	.process_batch_epilogue:
	VANDPD ymm0, ymm0, ymm6
	VADDPD ymm10, ymm10, ymm2
	VANDPD ymm1, ymm1, ymm6
	VADDPD ymm11, ymm11, ymm0
	VANDPD ymm3, ymm3, ymm6
	VADDPD ymm12, ymm12, ymm1
	VADDPD ymm13, ymm13, ymm3
	.batch_process_finish:
	ADD r8, 28
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm4, [rcx]
	VANDPD xmm4, xmm4, xmm6
	VADDPD ymm7, ymm7, ymm4
	ADD rcx, 8
	SUB r8, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD ymm7, ymm7, ymm8
	VADDPD ymm9, ymm9, ymm10
	VADDPD ymm11, ymm11, ymm12
	VADDPD ymm7, ymm7, ymm9
	VADDPD ymm11, ymm11, ymm13
	VADDPD ymm7, ymm7, ymm11
	VEXTRACTF128 xmm4, ymm7, 1
	VADDPD xmm7, xmm7, xmm4
	VUNPCKHPD xmm4, xmm7, xmm7
	VADDSD xmm7, xmm7, xmm4
	.return_ok:
	VMOVSD [rdx], xmm7
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm14, [dword rsp + 128]
	ADD rsp, 152
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .rdata$n rdata align=32
_yepCore_SumAbs_V64f_S64f_Bulldozer_constants:
	.c0: DQ 0x7FFFFFFFFFFFFFFF, 0x7FFFFFFFFFFFFFFF, 0x7FFFFFFFFFFFFFFF, 0x7FFFFFFFFFFFFFFF


section .text$n code align=16
global _yepCore_SumAbs_V64f_S64f_Bulldozer
_yepCore_SumAbs_V64f_S64f_Bulldozer:
	.ENTRY:
	SUB rsp, 120
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	VMOVDQA ymm0,  [rel _yepCore_SumAbs_V64f_S64f_Bulldozer_constants.c0]
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm6, xmm6, xmm6
	TEST r8, r8
	JZ .return_ok
	VXORPD xmm7, xmm7, xmm7
	VXORPD xmm8, xmm8, xmm8
	VXORPD xmm9, xmm9, xmm9
	VXORPD xmm10, xmm10, xmm10
	VXORPD xmm11, xmm11, xmm11
	TEST rcx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm12, [rcx]
	VANDPD xmm12, xmm12, xmm0
	VADDPD xmm6, xmm6, xmm12
	ADD rcx, 8
	SUB r8, 1
	JZ .reduce_batch
	TEST rcx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD xmm12, [rcx]
	VMOVUPD xmm4, [byte rcx + 16]
	VMOVUPD ymm5, [byte rcx + 32]
	VMOVUPD xmm2, [byte rcx + 64]
	VANDPD xmm12, xmm12, xmm0
	VMOVUPD xmm3, [byte rcx + 80]
	VANDPD xmm4, xmm4, xmm0
	VADDPD xmm6, xmm6, xmm12
	VMOVUPD ymm1, [byte rcx + 96]
	VANDPD ymm5, ymm5, ymm0
	VADDPD xmm7, xmm7, xmm4
	ADD rcx, 128
	VANDPD xmm2, xmm2, xmm0
	VADDPD ymm8, ymm8, ymm5
	SUB r8, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD xmm12, [rcx]
	VANDPD xmm3, xmm3, xmm0
	VADDPD xmm9, xmm9, xmm2
	VMOVUPD xmm4, [byte rcx + 16]
	VANDPD ymm1, ymm1, ymm0
	VADDPD xmm10, xmm10, xmm3
	VMOVUPD ymm5, [byte rcx + 32]
	VADDPD ymm11, ymm11, ymm1
	VMOVUPD xmm2, [byte rcx + 64]
	VANDPD xmm12, xmm12, xmm0
	VMOVUPD xmm3, [byte rcx + 80]
	VANDPD xmm4, xmm4, xmm0
	VADDPD xmm6, xmm6, xmm12
	VMOVUPD ymm1, [byte rcx + 96]
	VANDPD ymm5, ymm5, ymm0
	VADDPD xmm7, xmm7, xmm4
	ADD rcx, 128
	VANDPD xmm2, xmm2, xmm0
	VADDPD ymm8, ymm8, ymm5
	SUB r8, 16
	JAE .process_batch
	.process_batch_epilogue:
	VANDPD xmm3, xmm3, xmm0
	VADDPD xmm9, xmm9, xmm2
	VANDPD ymm1, ymm1, ymm0
	VADDPD xmm10, xmm10, xmm3
	VADDPD ymm11, ymm11, ymm1
	.batch_process_finish:
	ADD r8, 16
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm4, [rcx]
	VANDPD xmm4, xmm4, xmm0
	VADDPD xmm6, xmm6, xmm4
	ADD rcx, 8
	SUB r8, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD xmm6, xmm6, xmm7
	VADDPD ymm8, ymm8, ymm9
	VADDPD ymm10, ymm10, ymm11
	VADDPD ymm6, ymm6, ymm8
	VADDPD ymm6, ymm6, ymm10
	VEXTRACTF128 xmm4, ymm6, 1
	VADDPD xmm6, xmm6, xmm4
	VUNPCKHPD xmm4, xmm6, xmm6
	VADDSD xmm6, xmm6, xmm4
	.return_ok:
	VMOVSD [rdx], xmm6
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	ADD rsp, 120
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return
