;                       Yeppp! library implementation
;                   This file is auto-generated by Peach-Py,
;        Portable Efficient Assembly Code-generator in Higher-level Python,
;                  part of the Yeppp! library infrastructure
; This file is part of Yeppp! library and licensed under the New BSD license.
; See LICENSE.txt for the full text of the license.

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_DotProduct_V32fV32f_S32f_Nehalem
_yepCore_DotProduct_V32fV32f_S32f_Nehalem:
%else
section .text
global __yepCore_DotProduct_V32fV32f_S32f_Nehalem
__yepCore_DotProduct_V32fV32f_S32f_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	XORPS xmm15, xmm15
	TEST rcx, rcx
	JZ .return_ok
	XORPS xmm7, xmm7
	XORPS xmm6, xmm6
	XORPS xmm5, xmm5
	XORPS xmm4, xmm4
	XORPS xmm3, xmm3
	XORPS xmm2, xmm2
	XORPS xmm1, xmm1
	TEST rsi, 15
	JZ .source_y_16b_aligned
	.source_y_16b_misaligned:
	MOVSS xmm0, [rdi]
	MULSS xmm0, [rsi]
	ADDPS xmm15, xmm0
	ADD rdi, 4
	ADD rsi, 4
	SUB rcx, 1
	JZ .reduce_batch
	TEST rsi, 15
	JNZ .source_y_16b_misaligned
	.source_y_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPS xmm0, [rdi]
	MOVUPS xmm8, [byte rdi + 16]
	MOVUPS xmm9, [byte rdi + 32]
	MULPS xmm0, [rsi]
	MOVUPS xmm11, [byte rdi + 48]
	MULPS xmm8, [byte rsi + 16]
	MOVUPS xmm12, [byte rdi + 64]
	MULPS xmm9, [byte rsi + 32]
	MOVUPS xmm10, [byte rdi + 80]
	MULPS xmm11, [byte rsi + 48]
	ADDPS xmm15, xmm0
	MOVUPS xmm13, [byte rdi + 96]
	MULPS xmm12, [byte rsi + 64]
	ADDPS xmm7, xmm8
	MOVUPS xmm14, [byte rdi + 112]
	MULPS xmm10, [byte rsi + 80]
	ADDPS xmm6, xmm9
	ADD rdi, 128
	MULPS xmm13, [byte rsi + 96]
	ADDPS xmm5, xmm11
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPS xmm0, [rdi]
	MULPS xmm14, [byte rsi + 112]
	ADDPS xmm4, xmm12
	MOVUPS xmm8, [byte rdi + 16]
	ADD rsi, 128
	ADDPS xmm3, xmm10
	MOVUPS xmm9, [byte rdi + 32]
	MULPS xmm0, [rsi]
	ADDPS xmm2, xmm13
	MOVUPS xmm11, [byte rdi + 48]
	MULPS xmm8, [byte rsi + 16]
	ADDPS xmm1, xmm14
	MOVUPS xmm12, [byte rdi + 64]
	MULPS xmm9, [byte rsi + 32]
	MOVUPS xmm10, [byte rdi + 80]
	MULPS xmm11, [byte rsi + 48]
	ADDPS xmm15, xmm0
	MOVUPS xmm13, [byte rdi + 96]
	MULPS xmm12, [byte rsi + 64]
	ADDPS xmm7, xmm8
	MOVUPS xmm14, [byte rdi + 112]
	MULPS xmm10, [byte rsi + 80]
	ADDPS xmm6, xmm9
	ADD rdi, 128
	MULPS xmm13, [byte rsi + 96]
	ADDPS xmm5, xmm11
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	MULPS xmm14, [byte rsi + 112]
	ADDPS xmm4, xmm12
	ADD rsi, 128
	ADDPS xmm3, xmm10
	ADDPS xmm2, xmm13
	ADDPS xmm1, xmm14
	.batch_process_finish:
	ADD rcx, 32
	JZ .reduce_batch
	.process_single:
	MOVSS xmm8, [rdi]
	MULSS xmm8, [rsi]
	ADDPS xmm15, xmm8
	ADD rdi, 4
	ADD rsi, 4
	SUB rcx, 1
	JNZ .process_single
	.reduce_batch:
	ADDPS xmm15, xmm7
	ADDPS xmm6, xmm5
	ADDPS xmm4, xmm3
	ADDPS xmm2, xmm1
	ADDPS xmm15, xmm6
	ADDPS xmm4, xmm2
	ADDPS xmm15, xmm4
	MOVHLPS xmm8, xmm15
	ADDPS xmm15, xmm8
	MOVSHDUP xmm8, xmm15
	ADDSS xmm15, xmm8
	.return_ok:
	MOVSS [rdx], xmm15
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_DotProduct_V32fV32f_S32f_SandyBridge
_yepCore_DotProduct_V32fV32f_S32f_SandyBridge:
%else
section .text
global __yepCore_DotProduct_V32fV32f_S32f_SandyBridge
__yepCore_DotProduct_V32fV32f_S32f_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm15, xmm15, xmm15
	TEST rcx, rcx
	JZ .return_ok
	VXORPS xmm7, xmm7, xmm7
	VXORPS xmm6, xmm6, xmm6
	VXORPS xmm5, xmm5, xmm5
	VXORPS xmm4, xmm4, xmm4
	VXORPS xmm3, xmm3, xmm3
	VXORPS xmm2, xmm2, xmm2
	VXORPS xmm1, xmm1, xmm1
	TEST rsi, 31
	JZ .source_y_32b_aligned
	.source_y_32b_misaligned:
	VMOVSS xmm0, [rdi]
	VMULSS xmm0, xmm0, [rsi]
	VADDPS ymm15, ymm15, ymm0
	ADD rdi, 4
	ADD rsi, 4
	SUB rcx, 1
	JZ .reduce_batch
	TEST rsi, 31
	JNZ .source_y_32b_misaligned
	.source_y_32b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS ymm0, [rdi]
	VMOVUPS ymm8, [byte rdi + 32]
	VMOVUPS ymm9, [byte rdi + 64]
	VMULPS ymm0, ymm0, [rsi]
	VMOVUPS ymm12, [byte rdi + 96]
	VMULPS ymm8, ymm8, [byte rsi + 32]
	VMOVUPS ymm14, [dword rdi + 128]
	VMULPS ymm9, ymm9, [byte rsi + 64]
	VMOVUPS ymm10, [dword rdi + 160]
	VMULPS ymm12, ymm12, [byte rsi + 96]
	VADDPS ymm15, ymm15, ymm0
	VMOVUPS ymm11, [dword rdi + 192]
	VMULPS ymm14, ymm14, [dword rsi + 128]
	VADDPS ymm7, ymm7, ymm8
	VMOVUPS ymm13, [dword rdi + 224]
	VMULPS ymm10, ymm10, [dword rsi + 160]
	VADDPS ymm6, ymm6, ymm9
	ADD rdi, 256
	VMULPS ymm11, ymm11, [dword rsi + 192]
	VADDPS ymm5, ymm5, ymm12
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS ymm0, [rdi]
	VMULPS ymm13, ymm13, [dword rsi + 224]
	VADDPS ymm4, ymm4, ymm14
	VMOVUPS ymm8, [byte rdi + 32]
	ADD rsi, 256
	VADDPS ymm3, ymm3, ymm10
	VMOVUPS ymm9, [byte rdi + 64]
	VMULPS ymm0, ymm0, [rsi]
	VADDPS ymm2, ymm2, ymm11
	VMOVUPS ymm12, [byte rdi + 96]
	VMULPS ymm8, ymm8, [byte rsi + 32]
	VADDPS ymm1, ymm1, ymm13
	VMOVUPS ymm14, [dword rdi + 128]
	VMULPS ymm9, ymm9, [byte rsi + 64]
	VMOVUPS ymm10, [dword rdi + 160]
	VMULPS ymm12, ymm12, [byte rsi + 96]
	VADDPS ymm15, ymm15, ymm0
	VMOVUPS ymm11, [dword rdi + 192]
	VMULPS ymm14, ymm14, [dword rsi + 128]
	VADDPS ymm7, ymm7, ymm8
	VMOVUPS ymm13, [dword rdi + 224]
	VMULPS ymm10, ymm10, [dword rsi + 160]
	VADDPS ymm6, ymm6, ymm9
	ADD rdi, 256
	VMULPS ymm11, ymm11, [dword rsi + 192]
	VADDPS ymm5, ymm5, ymm12
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	VMULPS ymm13, ymm13, [dword rsi + 224]
	VADDPS ymm4, ymm4, ymm14
	ADD rsi, 256
	VADDPS ymm3, ymm3, ymm10
	VADDPS ymm2, ymm2, ymm11
	VADDPS ymm1, ymm1, ymm13
	.batch_process_finish:
	ADD rcx, 64
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm8, [rdi]
	VMULSS xmm8, xmm8, [rsi]
	VADDPS ymm15, ymm15, ymm8
	ADD rdi, 4
	ADD rsi, 4
	SUB rcx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS ymm15, ymm15, ymm7
	VADDPS ymm6, ymm6, ymm5
	VADDPS ymm4, ymm4, ymm3
	VADDPS ymm2, ymm2, ymm1
	VADDPS ymm15, ymm15, ymm6
	VADDPS ymm4, ymm4, ymm2
	VADDPS ymm15, ymm15, ymm4
	VEXTRACTF128 xmm8, ymm15, 1
	VADDPS xmm15, xmm15, xmm8
	VUNPCKHPD xmm8, xmm15, xmm15
	VADDPS xmm15, xmm15, xmm8
	VMOVSHDUP xmm8, xmm15
	VADDSS xmm15, xmm15, xmm8
	.return_ok:
	VMOVSS [rdx], xmm15
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Bulldozer progbits alloc exec nowrite align=16
global _yepCore_DotProduct_V32fV32f_S32f_Bulldozer
_yepCore_DotProduct_V32fV32f_S32f_Bulldozer:
%else
section .text
global __yepCore_DotProduct_V32fV32f_S32f_Bulldozer
__yepCore_DotProduct_V32fV32f_S32f_Bulldozer:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm12, xmm12, xmm12
	TEST rcx, rcx
	JZ .return_ok
	VXORPS xmm13, xmm13, xmm13
	VXORPS xmm14, xmm14, xmm14
	VXORPS xmm15, xmm15, xmm15
	VXORPS xmm7, xmm7, xmm7
	VXORPS xmm6, xmm6, xmm6
	TEST rsi, 31
	JZ .source_y_32b_aligned
	.source_y_32b_misaligned:
	VMOVSS xmm5, [rdi]
	VMOVSS xmm4, [rsi]
	VFMADDPS xmm12, xmm5, xmm4, xmm12
	ADD rdi, 4
	ADD rsi, 4
	SUB rcx, 1
	JZ .reduce_batch
	TEST rsi, 31
	JNZ .source_y_32b_misaligned
	.source_y_32b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS xmm5, [rdi]
	VMOVUPS xmm8, [byte rdi + 16]
	VMOVUPS ymm9, [byte rdi + 32]
	VMOVUPS xmm4, [byte rdi + 64]
	VFMADDPS xmm12, xmm5, [rsi], xmm12
	VMOVUPS xmm11, [byte rdi + 80]
	VFMADDPS xmm13, xmm8, [byte rsi + 16], xmm13
	VMOVUPS ymm10, [byte rdi + 96]
	VFMADDPS ymm14, ymm9, [byte rsi + 32], ymm14
	ADD rdi, 128
	VFMADDPS xmm15, xmm4, [byte rsi + 64], xmm15
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS xmm5, [rdi]
	VFMADDPS xmm7, xmm11, [byte rsi + 80], xmm7
	VMOVUPS xmm8, [byte rdi + 16]
	VFMADDPS ymm6, ymm10, [byte rsi + 96], ymm6
	VMOVUPS ymm9, [byte rdi + 32]
	ADD rsi, 128
	VMOVUPS xmm4, [byte rdi + 64]
	VFMADDPS xmm12, xmm5, [rsi], xmm12
	VMOVUPS xmm11, [byte rdi + 80]
	VFMADDPS xmm13, xmm8, [byte rsi + 16], xmm13
	VMOVUPS ymm10, [byte rdi + 96]
	VFMADDPS ymm14, ymm9, [byte rsi + 32], ymm14
	ADD rdi, 128
	VFMADDPS xmm15, xmm4, [byte rsi + 64], xmm15
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	VFMADDPS xmm7, xmm11, [byte rsi + 80], xmm7
	VFMADDPS ymm6, ymm10, [byte rsi + 96], ymm6
	ADD rsi, 128
	.batch_process_finish:
	ADD rcx, 32
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm8, [rdi]
	VMOVSS xmm9, [rsi]
	VFMADDPS xmm12, xmm8, xmm9, xmm12
	ADD rdi, 4
	ADD rsi, 4
	SUB rcx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS xmm12, xmm12, xmm13
	VADDPS ymm14, ymm14, ymm15
	VADDPS ymm7, ymm7, ymm6
	VADDPS ymm12, ymm12, ymm14
	VADDPS ymm12, ymm12, ymm7
	VEXTRACTF128 xmm8, ymm12, 1
	VADDPS xmm12, xmm12, xmm8
	VUNPCKHPD xmm8, xmm12, xmm12
	VADDPS xmm12, xmm12, xmm8
	VMOVSHDUP xmm8, xmm12
	VADDSS xmm12, xmm12, xmm8
	.return_ok:
	VMOVSS [rdx], xmm12
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_DotProduct_V32fV32f_S32f_Haswell
_yepCore_DotProduct_V32fV32f_S32f_Haswell:
%else
section .text
global __yepCore_DotProduct_V32fV32f_S32f_Haswell
__yepCore_DotProduct_V32fV32f_S32f_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm12, xmm12, xmm12
	TEST rcx, rcx
	JZ .return_ok
	VXORPS xmm13, xmm13, xmm13
	VXORPS xmm14, xmm14, xmm14
	VXORPS xmm15, xmm15, xmm15
	VXORPS xmm7, xmm7, xmm7
	VXORPS xmm6, xmm6, xmm6
	VXORPS xmm5, xmm5, xmm5
	VXORPS xmm4, xmm4, xmm4
	TEST rsi, 31
	JZ .source_y_32b_aligned
	.source_y_32b_misaligned:
	VMOVSS xmm3, [rdi]
	VMOVSS xmm2, [rsi]
	VFMADD231PS ymm12, ymm3, ymm2
	ADD rdi, 4
	ADD rsi, 4
	SUB rcx, 1
	JZ .reduce_batch
	TEST rsi, 31
	JNZ .source_y_32b_misaligned
	.source_y_32b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS ymm3, [rdi]
	VMOVUPS ymm8, [byte rdi + 32]
	VMOVUPS ymm9, [byte rdi + 64]
	VMOVUPS ymm11, [byte rdi + 96]
	VFMADD231PS ymm12, ymm3, [rsi]
	VMOVUPS ymm2, [dword rdi + 128]
	VFMADD231PS ymm13, ymm8, [byte rsi + 32]
	VMOVUPS ymm1, [dword rdi + 160]
	VFMADD231PS ymm14, ymm9, [byte rsi + 64]
	VMOVUPS ymm10, [dword rdi + 192]
	VFMADD231PS ymm15, ymm11, [byte rsi + 96]
	VMOVUPS ymm11, [dword rdi + 224]
	VFMADD231PS ymm7, ymm2, [dword rsi + 128]
	ADD rdi, 256
	VFMADD231PS ymm6, ymm1, [dword rsi + 160]
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS ymm3, [rdi]
	VFMADD231PS ymm5, ymm10, [dword rsi + 192]
	VMOVUPS ymm8, [byte rdi + 32]
	VFMADD231PS ymm4, ymm11, [dword rsi + 224]
	VMOVUPS ymm9, [byte rdi + 64]
	ADD rsi, 256
	VMOVUPS ymm11, [byte rdi + 96]
	VFMADD231PS ymm12, ymm3, [rsi]
	VMOVUPS ymm2, [dword rdi + 128]
	VFMADD231PS ymm13, ymm8, [byte rsi + 32]
	VMOVUPS ymm1, [dword rdi + 160]
	VFMADD231PS ymm14, ymm9, [byte rsi + 64]
	VMOVUPS ymm10, [dword rdi + 192]
	VFMADD231PS ymm15, ymm11, [byte rsi + 96]
	VMOVUPS ymm11, [dword rdi + 224]
	VFMADD231PS ymm7, ymm2, [dword rsi + 128]
	ADD rdi, 256
	VFMADD231PS ymm6, ymm1, [dword rsi + 160]
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	VFMADD231PS ymm5, ymm10, [dword rsi + 192]
	VFMADD231PS ymm4, ymm11, [dword rsi + 224]
	ADD rsi, 256
	.batch_process_finish:
	ADD rcx, 64
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm8, [rdi]
	VMOVSS xmm9, [rsi]
	VFMADD231PS ymm12, ymm8, ymm9
	ADD rdi, 4
	ADD rsi, 4
	SUB rcx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS ymm12, ymm12, ymm13
	VADDPS ymm14, ymm14, ymm15
	VADDPS ymm7, ymm7, ymm6
	VADDPS ymm5, ymm5, ymm4
	VADDPS ymm12, ymm12, ymm14
	VADDPS ymm7, ymm7, ymm5
	VADDPS ymm12, ymm12, ymm7
	VEXTRACTF128 xmm8, ymm12, 1
	VADDPS xmm12, xmm12, xmm8
	VUNPCKHPD xmm8, xmm12, xmm12
	VADDPS xmm12, xmm12, xmm8
	VMOVSHDUP xmm8, xmm12
	VADDSS xmm12, xmm12, xmm8
	.return_ok:
	VMOVSS [rdx], xmm12
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_DotProduct_V64fV64f_S64f_Nehalem
_yepCore_DotProduct_V64fV64f_S64f_Nehalem:
%else
section .text
global __yepCore_DotProduct_V64fV64f_S64f_Nehalem
__yepCore_DotProduct_V64fV64f_S64f_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	XORPD xmm15, xmm15
	TEST rcx, rcx
	JZ .return_ok
	XORPD xmm7, xmm7
	XORPD xmm6, xmm6
	XORPD xmm5, xmm5
	XORPD xmm4, xmm4
	XORPD xmm3, xmm3
	XORPD xmm2, xmm2
	XORPD xmm1, xmm1
	TEST rsi, 15
	JZ .source_y_16b_aligned
	.source_y_16b_misaligned:
	MOVSD xmm0, [rdi]
	MULSD xmm0, [rsi]
	ADDPD xmm15, xmm0
	ADD rdi, 8
	ADD rsi, 8
	SUB rcx, 1
	JZ .reduce_batch
	TEST rsi, 15
	JNZ .source_y_16b_misaligned
	.source_y_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPD xmm0, [rdi]
	MOVUPD xmm8, [byte rdi + 16]
	MOVUPD xmm9, [byte rdi + 32]
	MULPD xmm0, [rsi]
	MOVUPD xmm11, [byte rdi + 48]
	MULPD xmm8, [byte rsi + 16]
	MOVUPD xmm12, [byte rdi + 64]
	MULPD xmm9, [byte rsi + 32]
	MOVUPD xmm10, [byte rdi + 80]
	MULPD xmm11, [byte rsi + 48]
	ADDPD xmm15, xmm0
	MOVUPD xmm13, [byte rdi + 96]
	MULPD xmm12, [byte rsi + 64]
	ADDPD xmm7, xmm8
	MOVUPD xmm14, [byte rdi + 112]
	MULPD xmm10, [byte rsi + 80]
	ADDPD xmm6, xmm9
	ADD rdi, 128
	MULPD xmm13, [byte rsi + 96]
	ADDPD xmm5, xmm11
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPD xmm0, [rdi]
	MULPD xmm14, [byte rsi + 112]
	ADDPD xmm4, xmm12
	MOVUPD xmm8, [byte rdi + 16]
	ADD rsi, 128
	ADDPD xmm3, xmm10
	MOVUPD xmm9, [byte rdi + 32]
	MULPD xmm0, [rsi]
	ADDPD xmm2, xmm13
	MOVUPD xmm11, [byte rdi + 48]
	MULPD xmm8, [byte rsi + 16]
	ADDPD xmm1, xmm14
	MOVUPD xmm12, [byte rdi + 64]
	MULPD xmm9, [byte rsi + 32]
	MOVUPD xmm10, [byte rdi + 80]
	MULPD xmm11, [byte rsi + 48]
	ADDPD xmm15, xmm0
	MOVUPD xmm13, [byte rdi + 96]
	MULPD xmm12, [byte rsi + 64]
	ADDPD xmm7, xmm8
	MOVUPD xmm14, [byte rdi + 112]
	MULPD xmm10, [byte rsi + 80]
	ADDPD xmm6, xmm9
	ADD rdi, 128
	MULPD xmm13, [byte rsi + 96]
	ADDPD xmm5, xmm11
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	MULPD xmm14, [byte rsi + 112]
	ADDPD xmm4, xmm12
	ADD rsi, 128
	ADDPD xmm3, xmm10
	ADDPD xmm2, xmm13
	ADDPD xmm1, xmm14
	.batch_process_finish:
	ADD rcx, 16
	JZ .reduce_batch
	.process_single:
	MOVSD xmm8, [rdi]
	MULSD xmm8, [rsi]
	ADDPD xmm15, xmm8
	ADD rdi, 8
	ADD rsi, 8
	SUB rcx, 1
	JNZ .process_single
	.reduce_batch:
	ADDPD xmm15, xmm7
	ADDPD xmm6, xmm5
	ADDPD xmm4, xmm3
	ADDPD xmm2, xmm1
	ADDPD xmm15, xmm6
	ADDPD xmm4, xmm2
	ADDPD xmm15, xmm4
	MOVHLPS xmm8, xmm15
	ADDSD xmm15, xmm8
	.return_ok:
	MOVSD [rdx], xmm15
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Bonnell progbits alloc exec nowrite align=16
global _yepCore_DotProduct_V64fV64f_S64f_Bonnell
_yepCore_DotProduct_V64fV64f_S64f_Bonnell:
%else
section .text
global __yepCore_DotProduct_V64fV64f_S64f_Bonnell
__yepCore_DotProduct_V64fV64f_S64f_Bonnell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	XORPD xmm15, xmm15
	TEST rcx, rcx
	JZ .return_ok
	XORPD xmm7, xmm7
	XORPD xmm6, xmm6
	XORPD xmm5, xmm5
	XORPD xmm4, xmm4
	XORPD xmm3, xmm3
	XORPD xmm2, xmm2
	XORPD xmm1, xmm1
	SUB rcx, 8
	JB .batch_process_finish
	.process_batch_prologue:
	MOVSD xmm0, [rdi]
	MOVSD xmm8, [byte rdi + 8]
	MULSD xmm0, [rsi]
	MOVSD xmm9, [byte rdi + 16]
	MULSD xmm8, [byte rsi + 8]
	MOVSD xmm11, [byte rdi + 24]
	MULSD xmm9, [byte rsi + 16]
	MOVSD xmm12, [byte rdi + 32]
	MULSD xmm11, [byte rsi + 24]
	MOVSD xmm10, [byte rdi + 40]
	MULSD xmm12, [byte rsi + 32]
	ADDSD xmm15, xmm0
	MOVSD xmm13, [byte rdi + 48]
	MULSD xmm10, [byte rsi + 40]
	ADDSD xmm7, xmm8
	MOVSD xmm14, [byte rdi + 56]
	MULSD xmm13, [byte rsi + 48]
	ADDSD xmm6, xmm9
	ADD rdi, 64
	MULSD xmm14, [byte rsi + 56]
	ADDSD xmm5, xmm11
	SUB rcx, 8
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVSD xmm0, [rdi]
	ADD rsi, 64
	ADDSD xmm4, xmm12
	MOVSD xmm8, [byte rdi + 8]
	MULSD xmm0, [rsi]
	ADDSD xmm3, xmm10
	MOVSD xmm9, [byte rdi + 16]
	MULSD xmm8, [byte rsi + 8]
	ADDSD xmm2, xmm13
	MOVSD xmm11, [byte rdi + 24]
	MULSD xmm9, [byte rsi + 16]
	ADDSD xmm1, xmm14
	MOVSD xmm12, [byte rdi + 32]
	MULSD xmm11, [byte rsi + 24]
	MOVSD xmm10, [byte rdi + 40]
	MULSD xmm12, [byte rsi + 32]
	ADDSD xmm15, xmm0
	MOVSD xmm13, [byte rdi + 48]
	MULSD xmm10, [byte rsi + 40]
	ADDSD xmm7, xmm8
	MOVSD xmm14, [byte rdi + 56]
	MULSD xmm13, [byte rsi + 48]
	ADDSD xmm6, xmm9
	ADD rdi, 64
	MULSD xmm14, [byte rsi + 56]
	ADDSD xmm5, xmm11
	SUB rcx, 8
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 64
	ADDSD xmm4, xmm12
	ADDSD xmm3, xmm10
	ADDSD xmm2, xmm13
	ADDSD xmm1, xmm14
	.batch_process_finish:
	ADD rcx, 8
	JZ .reduce_batch
	.process_single:
	MOVSD xmm8, [rdi]
	MULSD xmm8, [rsi]
	ADDPD xmm15, xmm8
	ADD rdi, 8
	ADD rsi, 8
	SUB rcx, 1
	JNZ .process_single
	.reduce_batch:
	ADDPD xmm15, xmm7
	ADDPD xmm6, xmm5
	ADDPD xmm4, xmm3
	ADDPD xmm2, xmm1
	ADDPD xmm15, xmm6
	ADDPD xmm4, xmm2
	ADDPD xmm15, xmm4
	MOVHLPS xmm8, xmm15
	ADDSD xmm15, xmm8
	.return_ok:
	MOVSD [rdx], xmm15
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_DotProduct_V64fV64f_S64f_SandyBridge
_yepCore_DotProduct_V64fV64f_S64f_SandyBridge:
%else
section .text
global __yepCore_DotProduct_V64fV64f_S64f_SandyBridge
__yepCore_DotProduct_V64fV64f_S64f_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm15, xmm15, xmm15
	TEST rcx, rcx
	JZ .return_ok
	VXORPD xmm7, xmm7, xmm7
	VXORPD xmm6, xmm6, xmm6
	VXORPD xmm5, xmm5, xmm5
	VXORPD xmm4, xmm4, xmm4
	VXORPD xmm3, xmm3, xmm3
	VXORPD xmm2, xmm2, xmm2
	VXORPD xmm1, xmm1, xmm1
	TEST rsi, 31
	JZ .source_y_32b_aligned
	.source_y_32b_misaligned:
	VMOVSD xmm0, [rdi]
	VMULSD xmm0, xmm0, [rsi]
	VADDPD ymm15, ymm15, ymm0
	ADD rdi, 8
	ADD rsi, 8
	SUB rcx, 1
	JZ .reduce_batch
	TEST rsi, 31
	JNZ .source_y_32b_misaligned
	.source_y_32b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD ymm0, [rdi]
	VMOVUPD ymm8, [byte rdi + 32]
	VMOVUPD ymm9, [byte rdi + 64]
	VMULPD ymm0, ymm0, [rsi]
	VMOVUPD ymm12, [byte rdi + 96]
	VMULPD ymm8, ymm8, [byte rsi + 32]
	VMOVUPD ymm14, [dword rdi + 128]
	VMULPD ymm9, ymm9, [byte rsi + 64]
	VMOVUPD ymm10, [dword rdi + 160]
	VMULPD ymm12, ymm12, [byte rsi + 96]
	VADDPD ymm15, ymm15, ymm0
	VMOVUPD ymm11, [dword rdi + 192]
	VMULPD ymm14, ymm14, [dword rsi + 128]
	VADDPD ymm7, ymm7, ymm8
	VMOVUPD ymm13, [dword rdi + 224]
	VMULPD ymm10, ymm10, [dword rsi + 160]
	VADDPD ymm6, ymm6, ymm9
	ADD rdi, 256
	VMULPD ymm11, ymm11, [dword rsi + 192]
	VADDPD ymm5, ymm5, ymm12
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD ymm0, [rdi]
	VMULPD ymm13, ymm13, [dword rsi + 224]
	VADDPD ymm4, ymm4, ymm14
	VMOVUPD ymm8, [byte rdi + 32]
	ADD rsi, 256
	VADDPD ymm3, ymm3, ymm10
	VMOVUPD ymm9, [byte rdi + 64]
	VMULPD ymm0, ymm0, [rsi]
	VADDPD ymm2, ymm2, ymm11
	VMOVUPD ymm12, [byte rdi + 96]
	VMULPD ymm8, ymm8, [byte rsi + 32]
	VADDPD ymm1, ymm1, ymm13
	VMOVUPD ymm14, [dword rdi + 128]
	VMULPD ymm9, ymm9, [byte rsi + 64]
	VMOVUPD ymm10, [dword rdi + 160]
	VMULPD ymm12, ymm12, [byte rsi + 96]
	VADDPD ymm15, ymm15, ymm0
	VMOVUPD ymm11, [dword rdi + 192]
	VMULPD ymm14, ymm14, [dword rsi + 128]
	VADDPD ymm7, ymm7, ymm8
	VMOVUPD ymm13, [dword rdi + 224]
	VMULPD ymm10, ymm10, [dword rsi + 160]
	VADDPD ymm6, ymm6, ymm9
	ADD rdi, 256
	VMULPD ymm11, ymm11, [dword rsi + 192]
	VADDPD ymm5, ymm5, ymm12
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	VMULPD ymm13, ymm13, [dword rsi + 224]
	VADDPD ymm4, ymm4, ymm14
	ADD rsi, 256
	VADDPD ymm3, ymm3, ymm10
	VADDPD ymm2, ymm2, ymm11
	VADDPD ymm1, ymm1, ymm13
	.batch_process_finish:
	ADD rcx, 32
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm8, [rdi]
	VMULSD xmm8, xmm8, [rsi]
	VADDPD ymm15, ymm15, ymm8
	ADD rdi, 8
	ADD rsi, 8
	SUB rcx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD ymm15, ymm15, ymm7
	VADDPD ymm6, ymm6, ymm5
	VADDPD ymm4, ymm4, ymm3
	VADDPD ymm2, ymm2, ymm1
	VADDPD ymm15, ymm15, ymm6
	VADDPD ymm4, ymm4, ymm2
	VADDPD ymm15, ymm15, ymm4
	VEXTRACTF128 xmm8, ymm15, 1
	VADDPD xmm15, xmm15, xmm8
	VUNPCKHPD xmm8, xmm15, xmm15
	VADDSD xmm15, xmm15, xmm8
	.return_ok:
	VMOVSD [rdx], xmm15
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Bulldozer progbits alloc exec nowrite align=16
global _yepCore_DotProduct_V64fV64f_S64f_Bulldozer
_yepCore_DotProduct_V64fV64f_S64f_Bulldozer:
%else
section .text
global __yepCore_DotProduct_V64fV64f_S64f_Bulldozer
__yepCore_DotProduct_V64fV64f_S64f_Bulldozer:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm12, xmm12, xmm12
	TEST rcx, rcx
	JZ .return_ok
	VXORPD xmm13, xmm13, xmm13
	VXORPD xmm14, xmm14, xmm14
	VXORPD xmm15, xmm15, xmm15
	VXORPD xmm7, xmm7, xmm7
	VXORPD xmm6, xmm6, xmm6
	TEST rsi, 31
	JZ .source_y_32b_aligned
	.source_y_32b_misaligned:
	VMOVSD xmm5, [rdi]
	VMOVSD xmm4, [rsi]
	VFMADDPD xmm12, xmm5, xmm4, xmm12
	ADD rdi, 8
	ADD rsi, 8
	SUB rcx, 1
	JZ .reduce_batch
	TEST rsi, 31
	JNZ .source_y_32b_misaligned
	.source_y_32b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD xmm5, [rdi]
	VMOVUPD xmm8, [byte rdi + 16]
	VMOVUPD ymm9, [byte rdi + 32]
	VMOVUPD xmm4, [byte rdi + 64]
	VFMADDPD xmm12, xmm5, [rsi], xmm12
	VMOVUPD xmm11, [byte rdi + 80]
	VFMADDPD xmm13, xmm8, [byte rsi + 16], xmm13
	VMOVUPD ymm10, [byte rdi + 96]
	VFMADDPD ymm14, ymm9, [byte rsi + 32], ymm14
	ADD rdi, 128
	VFMADDPD xmm15, xmm4, [byte rsi + 64], xmm15
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD xmm5, [rdi]
	VFMADDPD xmm7, xmm11, [byte rsi + 80], xmm7
	VMOVUPD xmm8, [byte rdi + 16]
	VFMADDPD ymm6, ymm10, [byte rsi + 96], ymm6
	VMOVUPD ymm9, [byte rdi + 32]
	ADD rsi, 128
	VMOVUPD xmm4, [byte rdi + 64]
	VFMADDPD xmm12, xmm5, [rsi], xmm12
	VMOVUPD xmm11, [byte rdi + 80]
	VFMADDPD xmm13, xmm8, [byte rsi + 16], xmm13
	VMOVUPD ymm10, [byte rdi + 96]
	VFMADDPD ymm14, ymm9, [byte rsi + 32], ymm14
	ADD rdi, 128
	VFMADDPD xmm15, xmm4, [byte rsi + 64], xmm15
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	VFMADDPD xmm7, xmm11, [byte rsi + 80], xmm7
	VFMADDPD ymm6, ymm10, [byte rsi + 96], ymm6
	ADD rsi, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm8, [rdi]
	VMOVSD xmm9, [rsi]
	VFMADDPD xmm12, xmm8, xmm9, xmm12
	ADD rdi, 8
	ADD rsi, 8
	SUB rcx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD xmm12, xmm12, xmm13
	VADDPD ymm14, ymm14, ymm15
	VADDPD ymm7, ymm7, ymm6
	VADDPD ymm12, ymm12, ymm14
	VADDPD ymm12, ymm12, ymm7
	VEXTRACTF128 xmm8, ymm12, 1
	VADDPD xmm12, xmm12, xmm8
	VUNPCKHPD xmm8, xmm12, xmm12
	VADDSD xmm12, xmm12, xmm8
	.return_ok:
	VMOVSD [rdx], xmm12
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_DotProduct_V64fV64f_S64f_Haswell
_yepCore_DotProduct_V64fV64f_S64f_Haswell:
%else
section .text
global __yepCore_DotProduct_V64fV64f_S64f_Haswell
__yepCore_DotProduct_V64fV64f_S64f_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm12, xmm12, xmm12
	TEST rcx, rcx
	JZ .return_ok
	VXORPD xmm13, xmm13, xmm13
	VXORPD xmm14, xmm14, xmm14
	VXORPD xmm15, xmm15, xmm15
	VXORPD xmm7, xmm7, xmm7
	VXORPD xmm6, xmm6, xmm6
	VXORPD xmm5, xmm5, xmm5
	VXORPD xmm4, xmm4, xmm4
	TEST rsi, 31
	JZ .source_y_32b_aligned
	.source_y_32b_misaligned:
	VMOVSD xmm3, [rdi]
	VMOVSD xmm2, [rsi]
	VFMADD231PD ymm12, ymm3, ymm2
	ADD rdi, 8
	ADD rsi, 8
	SUB rcx, 1
	JZ .reduce_batch
	TEST rsi, 31
	JNZ .source_y_32b_misaligned
	.source_y_32b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD ymm3, [rdi]
	VMOVUPD ymm8, [byte rdi + 32]
	VMOVUPD ymm9, [byte rdi + 64]
	VMOVUPD ymm11, [byte rdi + 96]
	VFMADD231PD ymm12, ymm3, [rsi]
	VMOVUPD ymm2, [dword rdi + 128]
	VFMADD231PD ymm13, ymm8, [byte rsi + 32]
	VMOVUPD ymm1, [dword rdi + 160]
	VFMADD231PD ymm14, ymm9, [byte rsi + 64]
	VMOVUPD ymm10, [dword rdi + 192]
	VFMADD231PD ymm15, ymm11, [byte rsi + 96]
	VMOVUPD ymm11, [dword rdi + 224]
	VFMADD231PD ymm7, ymm2, [dword rsi + 128]
	ADD rdi, 256
	VFMADD231PD ymm6, ymm1, [dword rsi + 160]
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD ymm3, [rdi]
	VFMADD231PD ymm5, ymm10, [dword rsi + 192]
	VMOVUPD ymm8, [byte rdi + 32]
	VFMADD231PD ymm4, ymm11, [dword rsi + 224]
	VMOVUPD ymm9, [byte rdi + 64]
	ADD rsi, 256
	VMOVUPD ymm11, [byte rdi + 96]
	VFMADD231PD ymm12, ymm3, [rsi]
	VMOVUPD ymm2, [dword rdi + 128]
	VFMADD231PD ymm13, ymm8, [byte rsi + 32]
	VMOVUPD ymm1, [dword rdi + 160]
	VFMADD231PD ymm14, ymm9, [byte rsi + 64]
	VMOVUPD ymm10, [dword rdi + 192]
	VFMADD231PD ymm15, ymm11, [byte rsi + 96]
	VMOVUPD ymm11, [dword rdi + 224]
	VFMADD231PD ymm7, ymm2, [dword rsi + 128]
	ADD rdi, 256
	VFMADD231PD ymm6, ymm1, [dword rsi + 160]
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	VFMADD231PD ymm5, ymm10, [dword rsi + 192]
	VFMADD231PD ymm4, ymm11, [dword rsi + 224]
	ADD rsi, 256
	.batch_process_finish:
	ADD rcx, 32
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm8, [rdi]
	VMOVSD xmm9, [rsi]
	VFMADD231PD ymm12, ymm8, ymm9
	ADD rdi, 8
	ADD rsi, 8
	SUB rcx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD ymm12, ymm12, ymm13
	VADDPD ymm14, ymm14, ymm15
	VADDPD ymm7, ymm7, ymm6
	VADDPD ymm5, ymm5, ymm4
	VADDPD ymm12, ymm12, ymm14
	VADDPD ymm7, ymm7, ymm5
	VADDPD ymm12, ymm12, ymm7
	VEXTRACTF128 xmm8, ymm12, 1
	VADDPD xmm12, xmm12, xmm8
	VUNPCKHPD xmm8, xmm12, xmm12
	VADDSD xmm12, xmm12, xmm8
	.return_ok:
	VMOVSD [rdx], xmm12
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return
