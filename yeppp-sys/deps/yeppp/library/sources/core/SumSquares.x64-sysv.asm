;                       Yeppp! library implementation
;                   This file is auto-generated by Peach-Py,
;        Portable Efficient Assembly Code-generator in Higher-level Python,
;                  part of the Yeppp! library infrastructure
; This file is part of Yeppp! library and licensed under the New BSD license.
; See LICENSE.txt for the full text of the license.

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_SumSquares_V32f_S32f_Nehalem
_yepCore_SumSquares_V32f_S32f_Nehalem:
%else
section .text
global __yepCore_SumSquares_V32f_S32f_Nehalem
__yepCore_SumSquares_V32f_S32f_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	XORPS xmm15, xmm15
	TEST rdx, rdx
	JZ .return_ok
	XORPS xmm7, xmm7
	XORPS xmm6, xmm6
	XORPS xmm5, xmm5
	XORPS xmm4, xmm4
	XORPS xmm3, xmm3
	XORPS xmm2, xmm2
	XORPS xmm1, xmm1
	TEST rdi, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSS xmm0, [rdi]
	MULSS xmm0, xmm0
	ADDPS xmm15, xmm0
	ADD rdi, 4
	SUB rdx, 1
	JZ .reduce_batch
	TEST rdi, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB rdx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPS xmm0, [rdi]
	MOVUPS xmm8, [byte rdi + 16]
	MOVUPS xmm9, [byte rdi + 32]
	MULPS xmm0, xmm0
	MOVUPS xmm10, [byte rdi + 48]
	MULPS xmm8, xmm8
	MOVUPS xmm12, [byte rdi + 64]
	MULPS xmm9, xmm9
	MOVUPS xmm11, [byte rdi + 80]
	MULPS xmm10, xmm10
	ADDPS xmm15, xmm0
	MOVUPS xmm13, [byte rdi + 96]
	MULPS xmm12, xmm12
	ADDPS xmm7, xmm8
	MOVUPS xmm14, [byte rdi + 112]
	MULPS xmm11, xmm11
	ADDPS xmm6, xmm9
	ADD rdi, 128
	MULPS xmm13, xmm13
	ADDPS xmm5, xmm10
	SUB rdx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPS xmm0, [rdi]
	MULPS xmm14, xmm14
	ADDPS xmm4, xmm12
	MOVUPS xmm8, [byte rdi + 16]
	ADDPS xmm3, xmm11
	MOVUPS xmm9, [byte rdi + 32]
	MULPS xmm0, xmm0
	ADDPS xmm2, xmm13
	MOVUPS xmm10, [byte rdi + 48]
	MULPS xmm8, xmm8
	ADDPS xmm1, xmm14
	MOVUPS xmm12, [byte rdi + 64]
	MULPS xmm9, xmm9
	MOVUPS xmm11, [byte rdi + 80]
	MULPS xmm10, xmm10
	ADDPS xmm15, xmm0
	MOVUPS xmm13, [byte rdi + 96]
	MULPS xmm12, xmm12
	ADDPS xmm7, xmm8
	MOVUPS xmm14, [byte rdi + 112]
	MULPS xmm11, xmm11
	ADDPS xmm6, xmm9
	ADD rdi, 128
	MULPS xmm13, xmm13
	ADDPS xmm5, xmm10
	SUB rdx, 32
	JAE .process_batch
	.process_batch_epilogue:
	MULPS xmm14, xmm14
	ADDPS xmm4, xmm12
	ADDPS xmm3, xmm11
	ADDPS xmm2, xmm13
	ADDPS xmm1, xmm14
	.batch_process_finish:
	ADD rdx, 32
	JZ .reduce_batch
	.process_single:
	MOVSS xmm8, [rdi]
	MULSS xmm8, xmm8
	ADDPS xmm15, xmm8
	ADD rdi, 4
	SUB rdx, 1
	JNZ .process_single
	.reduce_batch:
	ADDPS xmm15, xmm7
	ADDPS xmm6, xmm5
	ADDPS xmm4, xmm3
	ADDPS xmm2, xmm1
	ADDPS xmm15, xmm6
	ADDPS xmm4, xmm2
	ADDPS xmm15, xmm4
	MOVHLPS xmm8, xmm15
	ADDPS xmm15, xmm8
	MOVSHDUP xmm8, xmm15
	ADDSS xmm15, xmm8
	.return_ok:
	MOVSS [rsi], xmm15
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_SumSquares_V32f_S32f_SandyBridge
_yepCore_SumSquares_V32f_S32f_SandyBridge:
%else
section .text
global __yepCore_SumSquares_V32f_S32f_SandyBridge
__yepCore_SumSquares_V32f_S32f_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm15, xmm15, xmm15
	TEST rdx, rdx
	JZ .return_ok
	VXORPS xmm7, xmm7, xmm7
	VXORPS xmm6, xmm6, xmm6
	VXORPS xmm5, xmm5, xmm5
	VXORPS xmm4, xmm4, xmm4
	VXORPS xmm3, xmm3, xmm3
	VXORPS xmm2, xmm2, xmm2
	VXORPS xmm1, xmm1, xmm1
	TEST rdi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm0, [rdi]
	VMULSS xmm0, xmm0, xmm0
	VADDPS ymm15, ymm15, ymm0
	ADD rdi, 4
	SUB rdx, 1
	JZ .reduce_batch
	TEST rdi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rdx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS ymm0, [rdi]
	VMOVUPS ymm8, [byte rdi + 32]
	VMOVUPS ymm9, [byte rdi + 64]
	VMULPS ymm0, ymm0, ymm0
	VMOVUPS ymm12, [byte rdi + 96]
	VMULPS ymm8, ymm8, ymm8
	VMOVUPS ymm14, [dword rdi + 128]
	VMULPS ymm9, ymm9, ymm9
	VMOVUPS ymm13, [dword rdi + 160]
	VMULPS ymm12, ymm12, ymm12
	VADDPS ymm15, ymm15, ymm0
	VMOVUPS ymm10, [dword rdi + 192]
	VMULPS ymm14, ymm14, ymm14
	VADDPS ymm7, ymm7, ymm8
	VMOVUPS ymm11, [dword rdi + 224]
	VMULPS ymm13, ymm13, ymm13
	VADDPS ymm6, ymm6, ymm9
	ADD rdi, 256
	VMULPS ymm10, ymm10, ymm10
	VADDPS ymm5, ymm5, ymm12
	SUB rdx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS ymm0, [rdi]
	VMULPS ymm11, ymm11, ymm11
	VADDPS ymm4, ymm4, ymm14
	VMOVUPS ymm8, [byte rdi + 32]
	VADDPS ymm3, ymm3, ymm13
	VMOVUPS ymm9, [byte rdi + 64]
	VMULPS ymm0, ymm0, ymm0
	VADDPS ymm2, ymm2, ymm10
	VMOVUPS ymm12, [byte rdi + 96]
	VMULPS ymm8, ymm8, ymm8
	VADDPS ymm1, ymm1, ymm11
	VMOVUPS ymm14, [dword rdi + 128]
	VMULPS ymm9, ymm9, ymm9
	VMOVUPS ymm13, [dword rdi + 160]
	VMULPS ymm12, ymm12, ymm12
	VADDPS ymm15, ymm15, ymm0
	VMOVUPS ymm10, [dword rdi + 192]
	VMULPS ymm14, ymm14, ymm14
	VADDPS ymm7, ymm7, ymm8
	VMOVUPS ymm11, [dword rdi + 224]
	VMULPS ymm13, ymm13, ymm13
	VADDPS ymm6, ymm6, ymm9
	ADD rdi, 256
	VMULPS ymm10, ymm10, ymm10
	VADDPS ymm5, ymm5, ymm12
	SUB rdx, 64
	JAE .process_batch
	.process_batch_epilogue:
	VMULPS ymm11, ymm11, ymm11
	VADDPS ymm4, ymm4, ymm14
	VADDPS ymm3, ymm3, ymm13
	VADDPS ymm2, ymm2, ymm10
	VADDPS ymm1, ymm1, ymm11
	.batch_process_finish:
	ADD rdx, 64
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm8, [rdi]
	VMULSS xmm8, xmm8, xmm8
	VADDPS ymm15, ymm15, ymm8
	ADD rdi, 4
	SUB rdx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS ymm15, ymm15, ymm7
	VADDPS ymm6, ymm6, ymm5
	VADDPS ymm4, ymm4, ymm3
	VADDPS ymm2, ymm2, ymm1
	VADDPS ymm15, ymm15, ymm6
	VADDPS ymm4, ymm4, ymm2
	VADDPS ymm15, ymm15, ymm4
	VEXTRACTF128 xmm8, ymm15, 1
	VADDPS xmm15, xmm15, xmm8
	VUNPCKHPD xmm8, xmm15, xmm15
	VADDPS xmm15, xmm15, xmm8
	VMOVSHDUP xmm8, xmm15
	VADDSS xmm15, xmm15, xmm8
	.return_ok:
	VMOVSS [rsi], xmm15
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Bulldozer progbits alloc exec nowrite align=16
global _yepCore_SumSquares_V32f_S32f_Bulldozer
_yepCore_SumSquares_V32f_S32f_Bulldozer:
%else
section .text
global __yepCore_SumSquares_V32f_S32f_Bulldozer
__yepCore_SumSquares_V32f_S32f_Bulldozer:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm11, xmm11, xmm11
	TEST rdx, rdx
	JZ .return_ok
	VXORPS xmm12, xmm12, xmm12
	VXORPS xmm13, xmm13, xmm13
	VXORPS xmm14, xmm14, xmm14
	VXORPS xmm15, xmm15, xmm15
	VXORPS xmm7, xmm7, xmm7
	VXORPS xmm6, xmm6, xmm6
	VXORPS xmm5, xmm5, xmm5
	VXORPS xmm4, xmm4, xmm4
	TEST rdi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm3, [rdi]
	VFMADDPS xmm11, xmm3, xmm3, xmm11
	ADD rdi, 4
	SUB rdx, 1
	JZ .reduce_batch
	TEST rdi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rdx, 48
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS xmm3, [rdi]
	VMOVUPS xmm8, [byte rdi + 16]
	VMOVUPS ymm9, [byte rdi + 32]
	VMOVUPS xmm10, [byte rdi + 64]
	VFMADDPS xmm11, xmm3, xmm3, xmm11
	VMOVUPS xmm2, [byte rdi + 80]
	VFMADDPS xmm12, xmm8, xmm8, xmm12
	VMOVUPS ymm1, [byte rdi + 96]
	VFMADDPS ymm13, ymm9, ymm9, ymm13
	VMOVUPS xmm0, [dword rdi + 128]
	VFMADDPS xmm14, xmm10, xmm10, xmm14
	VMOVUPS xmm9, [dword rdi + 144]
	VFMADDPS xmm15, xmm2, xmm2, xmm15
	VMOVUPS ymm10, [dword rdi + 160]
	VFMADDPS ymm7, ymm1, ymm1, ymm7
	ADD rdi, 192
	VFMADDPS xmm6, xmm0, xmm0, xmm6
	SUB rdx, 48
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS xmm3, [rdi]
	VFMADDPS xmm5, xmm9, xmm9, xmm5
	VMOVUPS xmm8, [byte rdi + 16]
	VFMADDPS ymm4, ymm10, ymm10, ymm4
	VMOVUPS ymm9, [byte rdi + 32]
	VMOVUPS xmm10, [byte rdi + 64]
	VFMADDPS xmm11, xmm3, xmm3, xmm11
	VMOVUPS xmm2, [byte rdi + 80]
	VFMADDPS xmm12, xmm8, xmm8, xmm12
	VMOVUPS ymm1, [byte rdi + 96]
	VFMADDPS ymm13, ymm9, ymm9, ymm13
	VMOVUPS xmm0, [dword rdi + 128]
	VFMADDPS xmm14, xmm10, xmm10, xmm14
	VMOVUPS xmm9, [dword rdi + 144]
	VFMADDPS xmm15, xmm2, xmm2, xmm15
	VMOVUPS ymm10, [dword rdi + 160]
	VFMADDPS ymm7, ymm1, ymm1, ymm7
	ADD rdi, 192
	VFMADDPS xmm6, xmm0, xmm0, xmm6
	SUB rdx, 48
	JAE .process_batch
	.process_batch_epilogue:
	VFMADDPS xmm5, xmm9, xmm9, xmm5
	VFMADDPS ymm4, ymm10, ymm10, ymm4
	.batch_process_finish:
	ADD rdx, 48
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm8, [rdi]
	VFMADDPS xmm11, xmm8, xmm8, xmm11
	ADD rdi, 4
	SUB rdx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS xmm11, xmm11, xmm12
	VADDPS ymm13, ymm13, ymm14
	VADDPS ymm15, ymm15, ymm7
	VADDPS xmm6, xmm6, xmm5
	VADDPS ymm11, ymm11, ymm13
	VADDPS ymm15, ymm15, ymm6
	VADDPS ymm11, ymm11, ymm15
	VADDPS ymm11, ymm11, ymm4
	VEXTRACTF128 xmm8, ymm11, 1
	VADDPS xmm11, xmm11, xmm8
	VUNPCKHPD xmm8, xmm11, xmm11
	VADDPS xmm11, xmm11, xmm8
	VMOVSHDUP xmm8, xmm11
	VADDSS xmm11, xmm11, xmm8
	.return_ok:
	VMOVSS [rsi], xmm11
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_SumSquares_V32f_S32f_Haswell
_yepCore_SumSquares_V32f_S32f_Haswell:
%else
section .text
global __yepCore_SumSquares_V32f_S32f_Haswell
__yepCore_SumSquares_V32f_S32f_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm11, xmm11, xmm11
	TEST rdx, rdx
	JZ .return_ok
	VXORPS xmm12, xmm12, xmm12
	VXORPS xmm13, xmm13, xmm13
	VXORPS xmm14, xmm14, xmm14
	VXORPS xmm15, xmm15, xmm15
	VXORPS xmm7, xmm7, xmm7
	VXORPS xmm6, xmm6, xmm6
	VXORPS xmm5, xmm5, xmm5
	TEST rdi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm4, [rdi]
	VFMADD231PS ymm11, ymm4, ymm4
	ADD rdi, 4
	SUB rdx, 1
	JZ .reduce_batch
	TEST rdi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rdx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS ymm4, [rdi]
	VMOVUPS ymm8, [byte rdi + 32]
	VMOVUPS ymm9, [byte rdi + 64]
	VMOVUPS ymm10, [byte rdi + 96]
	VFMADD231PS ymm11, ymm4, ymm4
	VMOVUPS ymm3, [dword rdi + 128]
	VFMADD231PS ymm12, ymm8, ymm8
	VMOVUPS ymm2, [dword rdi + 160]
	VFMADD231PS ymm13, ymm9, ymm9
	VMOVUPS ymm9, [dword rdi + 192]
	VFMADD231PS ymm14, ymm10, ymm10
	VMOVUPS ymm10, [dword rdi + 224]
	VFMADD231PS ymm15, ymm3, ymm3
	ADD rdi, 256
	VFMADD231PS ymm7, ymm2, ymm2
	SUB rdx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS ymm4, [rdi]
	VFMADD231PS ymm6, ymm9, ymm9
	VMOVUPS ymm8, [byte rdi + 32]
	VFMADD231PS ymm5, ymm10, ymm10
	VMOVUPS ymm9, [byte rdi + 64]
	VMOVUPS ymm10, [byte rdi + 96]
	VFMADD231PS ymm11, ymm4, ymm4
	VMOVUPS ymm3, [dword rdi + 128]
	VFMADD231PS ymm12, ymm8, ymm8
	VMOVUPS ymm2, [dword rdi + 160]
	VFMADD231PS ymm13, ymm9, ymm9
	VMOVUPS ymm9, [dword rdi + 192]
	VFMADD231PS ymm14, ymm10, ymm10
	VMOVUPS ymm10, [dword rdi + 224]
	VFMADD231PS ymm15, ymm3, ymm3
	ADD rdi, 256
	VFMADD231PS ymm7, ymm2, ymm2
	SUB rdx, 64
	JAE .process_batch
	.process_batch_epilogue:
	VFMADD231PS ymm6, ymm9, ymm9
	VFMADD231PS ymm5, ymm10, ymm10
	.batch_process_finish:
	ADD rdx, 64
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm8, [rdi]
	VFMADD231PS ymm11, ymm8, ymm8
	ADD rdi, 4
	SUB rdx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS ymm11, ymm11, ymm12
	VADDPS ymm13, ymm13, ymm14
	VADDPS ymm15, ymm15, ymm7
	VADDPS ymm6, ymm6, ymm5
	VADDPS ymm11, ymm11, ymm13
	VADDPS ymm15, ymm15, ymm6
	VADDPS ymm11, ymm11, ymm15
	VEXTRACTF128 xmm8, ymm11, 1
	VADDPS xmm11, xmm11, xmm8
	VUNPCKHPD xmm8, xmm11, xmm11
	VADDPS xmm11, xmm11, xmm8
	VMOVSHDUP xmm8, xmm11
	VADDSS xmm11, xmm11, xmm8
	.return_ok:
	VMOVSS [rsi], xmm11
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_SumSquares_V64f_S64f_Nehalem
_yepCore_SumSquares_V64f_S64f_Nehalem:
%else
section .text
global __yepCore_SumSquares_V64f_S64f_Nehalem
__yepCore_SumSquares_V64f_S64f_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	XORPD xmm15, xmm15
	TEST rdx, rdx
	JZ .return_ok
	XORPD xmm7, xmm7
	XORPD xmm6, xmm6
	XORPD xmm5, xmm5
	XORPD xmm4, xmm4
	XORPD xmm3, xmm3
	XORPD xmm2, xmm2
	XORPD xmm1, xmm1
	TEST rdi, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSD xmm0, [rdi]
	MULSD xmm0, xmm0
	ADDPD xmm15, xmm0
	ADD rdi, 8
	SUB rdx, 1
	JZ .reduce_batch
	TEST rdi, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB rdx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPD xmm0, [rdi]
	MOVUPD xmm8, [byte rdi + 16]
	MOVUPD xmm9, [byte rdi + 32]
	MULPD xmm0, xmm0
	MOVUPD xmm10, [byte rdi + 48]
	MULPD xmm8, xmm8
	MOVUPD xmm12, [byte rdi + 64]
	MULPD xmm9, xmm9
	MOVUPD xmm11, [byte rdi + 80]
	MULPD xmm10, xmm10
	ADDPD xmm15, xmm0
	MOVUPD xmm13, [byte rdi + 96]
	MULPD xmm12, xmm12
	ADDPD xmm7, xmm8
	MOVUPD xmm14, [byte rdi + 112]
	MULPD xmm11, xmm11
	ADDPD xmm6, xmm9
	ADD rdi, 128
	MULPD xmm13, xmm13
	ADDPD xmm5, xmm10
	SUB rdx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPD xmm0, [rdi]
	MULPD xmm14, xmm14
	ADDPD xmm4, xmm12
	MOVUPD xmm8, [byte rdi + 16]
	ADDPD xmm3, xmm11
	MOVUPD xmm9, [byte rdi + 32]
	MULPD xmm0, xmm0
	ADDPD xmm2, xmm13
	MOVUPD xmm10, [byte rdi + 48]
	MULPD xmm8, xmm8
	ADDPD xmm1, xmm14
	MOVUPD xmm12, [byte rdi + 64]
	MULPD xmm9, xmm9
	MOVUPD xmm11, [byte rdi + 80]
	MULPD xmm10, xmm10
	ADDPD xmm15, xmm0
	MOVUPD xmm13, [byte rdi + 96]
	MULPD xmm12, xmm12
	ADDPD xmm7, xmm8
	MOVUPD xmm14, [byte rdi + 112]
	MULPD xmm11, xmm11
	ADDPD xmm6, xmm9
	ADD rdi, 128
	MULPD xmm13, xmm13
	ADDPD xmm5, xmm10
	SUB rdx, 16
	JAE .process_batch
	.process_batch_epilogue:
	MULPD xmm14, xmm14
	ADDPD xmm4, xmm12
	ADDPD xmm3, xmm11
	ADDPD xmm2, xmm13
	ADDPD xmm1, xmm14
	.batch_process_finish:
	ADD rdx, 16
	JZ .reduce_batch
	.process_single:
	MOVSD xmm8, [rdi]
	MULSD xmm8, xmm8
	ADDPD xmm15, xmm8
	ADD rdi, 8
	SUB rdx, 1
	JNZ .process_single
	.reduce_batch:
	ADDPD xmm15, xmm7
	ADDPD xmm6, xmm5
	ADDPD xmm4, xmm3
	ADDPD xmm2, xmm1
	ADDPD xmm15, xmm6
	ADDPD xmm4, xmm2
	ADDPD xmm15, xmm4
	MOVHLPS xmm8, xmm15
	ADDSD xmm15, xmm8
	.return_ok:
	MOVSD [rsi], xmm15
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_SumSquares_V64f_S64f_SandyBridge
_yepCore_SumSquares_V64f_S64f_SandyBridge:
%else
section .text
global __yepCore_SumSquares_V64f_S64f_SandyBridge
__yepCore_SumSquares_V64f_S64f_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm15, xmm15, xmm15
	TEST rdx, rdx
	JZ .return_ok
	VXORPD xmm7, xmm7, xmm7
	VXORPD xmm6, xmm6, xmm6
	VXORPD xmm5, xmm5, xmm5
	VXORPD xmm4, xmm4, xmm4
	VXORPD xmm3, xmm3, xmm3
	VXORPD xmm2, xmm2, xmm2
	VXORPD xmm1, xmm1, xmm1
	TEST rdi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm0, [rdi]
	VMULSD xmm0, xmm0, xmm0
	VADDPD ymm15, ymm15, ymm0
	ADD rdi, 8
	SUB rdx, 1
	JZ .reduce_batch
	TEST rdi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rdx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD ymm0, [rdi]
	VMOVUPD ymm8, [byte rdi + 32]
	VMOVUPD ymm9, [byte rdi + 64]
	VMULPD ymm0, ymm0, ymm0
	VMOVUPD ymm12, [byte rdi + 96]
	VMULPD ymm8, ymm8, ymm8
	VMOVUPD ymm14, [dword rdi + 128]
	VMULPD ymm9, ymm9, ymm9
	VMOVUPD ymm13, [dword rdi + 160]
	VMULPD ymm12, ymm12, ymm12
	VADDPD ymm15, ymm15, ymm0
	VMOVUPD ymm10, [dword rdi + 192]
	VMULPD ymm14, ymm14, ymm14
	VADDPD ymm7, ymm7, ymm8
	VMOVUPD ymm11, [dword rdi + 224]
	VMULPD ymm13, ymm13, ymm13
	VADDPD ymm6, ymm6, ymm9
	ADD rdi, 256
	VMULPD ymm10, ymm10, ymm10
	VADDPD ymm5, ymm5, ymm12
	SUB rdx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD ymm0, [rdi]
	VMULPD ymm11, ymm11, ymm11
	VADDPD ymm4, ymm4, ymm14
	VMOVUPD ymm8, [byte rdi + 32]
	VADDPD ymm3, ymm3, ymm13
	VMOVUPD ymm9, [byte rdi + 64]
	VMULPD ymm0, ymm0, ymm0
	VADDPD ymm2, ymm2, ymm10
	VMOVUPD ymm12, [byte rdi + 96]
	VMULPD ymm8, ymm8, ymm8
	VADDPD ymm1, ymm1, ymm11
	VMOVUPD ymm14, [dword rdi + 128]
	VMULPD ymm9, ymm9, ymm9
	VMOVUPD ymm13, [dword rdi + 160]
	VMULPD ymm12, ymm12, ymm12
	VADDPD ymm15, ymm15, ymm0
	VMOVUPD ymm10, [dword rdi + 192]
	VMULPD ymm14, ymm14, ymm14
	VADDPD ymm7, ymm7, ymm8
	VMOVUPD ymm11, [dword rdi + 224]
	VMULPD ymm13, ymm13, ymm13
	VADDPD ymm6, ymm6, ymm9
	ADD rdi, 256
	VMULPD ymm10, ymm10, ymm10
	VADDPD ymm5, ymm5, ymm12
	SUB rdx, 32
	JAE .process_batch
	.process_batch_epilogue:
	VMULPD ymm11, ymm11, ymm11
	VADDPD ymm4, ymm4, ymm14
	VADDPD ymm3, ymm3, ymm13
	VADDPD ymm2, ymm2, ymm10
	VADDPD ymm1, ymm1, ymm11
	.batch_process_finish:
	ADD rdx, 32
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm8, [rdi]
	VMULSD xmm8, xmm8, xmm8
	VADDPD ymm15, ymm15, ymm8
	ADD rdi, 8
	SUB rdx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD ymm15, ymm15, ymm7
	VADDPD ymm6, ymm6, ymm5
	VADDPD ymm4, ymm4, ymm3
	VADDPD ymm2, ymm2, ymm1
	VADDPD ymm15, ymm15, ymm6
	VADDPD ymm4, ymm4, ymm2
	VADDPD ymm15, ymm15, ymm4
	VEXTRACTF128 xmm8, ymm15, 1
	VADDPD xmm15, xmm15, xmm8
	VUNPCKHPD xmm8, xmm15, xmm15
	VADDSD xmm15, xmm15, xmm8
	.return_ok:
	VMOVSD [rsi], xmm15
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Bulldozer progbits alloc exec nowrite align=16
global _yepCore_SumSquares_V64f_S64f_Bulldozer
_yepCore_SumSquares_V64f_S64f_Bulldozer:
%else
section .text
global __yepCore_SumSquares_V64f_S64f_Bulldozer
__yepCore_SumSquares_V64f_S64f_Bulldozer:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm11, xmm11, xmm11
	TEST rdx, rdx
	JZ .return_ok
	VXORPD xmm12, xmm12, xmm12
	VXORPD xmm13, xmm13, xmm13
	VXORPD xmm14, xmm14, xmm14
	VXORPD xmm15, xmm15, xmm15
	VXORPD xmm7, xmm7, xmm7
	VXORPD xmm6, xmm6, xmm6
	VXORPD xmm5, xmm5, xmm5
	VXORPD xmm4, xmm4, xmm4
	TEST rdi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm3, [rdi]
	VFMADDPD xmm11, xmm3, xmm3, xmm11
	ADD rdi, 8
	SUB rdx, 1
	JZ .reduce_batch
	TEST rdi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rdx, 24
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD xmm3, [rdi]
	VMOVUPD xmm8, [byte rdi + 16]
	VMOVUPD ymm9, [byte rdi + 32]
	VMOVUPD xmm10, [byte rdi + 64]
	VFMADDPD xmm11, xmm3, xmm3, xmm11
	VMOVUPD xmm2, [byte rdi + 80]
	VFMADDPD xmm12, xmm8, xmm8, xmm12
	VMOVUPD ymm1, [byte rdi + 96]
	VFMADDPD ymm13, ymm9, ymm9, ymm13
	VMOVUPD xmm0, [dword rdi + 128]
	VFMADDPD xmm14, xmm10, xmm10, xmm14
	VMOVUPD xmm9, [dword rdi + 144]
	VFMADDPD xmm15, xmm2, xmm2, xmm15
	VMOVUPD ymm10, [dword rdi + 160]
	VFMADDPD ymm7, ymm1, ymm1, ymm7
	ADD rdi, 192
	VFMADDPD xmm6, xmm0, xmm0, xmm6
	SUB rdx, 24
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD xmm3, [rdi]
	VFMADDPD xmm5, xmm9, xmm9, xmm5
	VMOVUPD xmm8, [byte rdi + 16]
	VFMADDPD ymm4, ymm10, ymm10, ymm4
	VMOVUPD ymm9, [byte rdi + 32]
	VMOVUPD xmm10, [byte rdi + 64]
	VFMADDPD xmm11, xmm3, xmm3, xmm11
	VMOVUPD xmm2, [byte rdi + 80]
	VFMADDPD xmm12, xmm8, xmm8, xmm12
	VMOVUPD ymm1, [byte rdi + 96]
	VFMADDPD ymm13, ymm9, ymm9, ymm13
	VMOVUPD xmm0, [dword rdi + 128]
	VFMADDPD xmm14, xmm10, xmm10, xmm14
	VMOVUPD xmm9, [dword rdi + 144]
	VFMADDPD xmm15, xmm2, xmm2, xmm15
	VMOVUPD ymm10, [dword rdi + 160]
	VFMADDPD ymm7, ymm1, ymm1, ymm7
	ADD rdi, 192
	VFMADDPD xmm6, xmm0, xmm0, xmm6
	SUB rdx, 24
	JAE .process_batch
	.process_batch_epilogue:
	VFMADDPD xmm5, xmm9, xmm9, xmm5
	VFMADDPD ymm4, ymm10, ymm10, ymm4
	.batch_process_finish:
	ADD rdx, 24
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm8, [rdi]
	VFMADDPD xmm11, xmm8, xmm8, xmm11
	ADD rdi, 8
	SUB rdx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD xmm11, xmm11, xmm12
	VADDPD ymm13, ymm13, ymm14
	VADDPD ymm15, ymm15, ymm7
	VADDPD xmm6, xmm6, xmm5
	VADDPD ymm11, ymm11, ymm13
	VADDPD ymm15, ymm15, ymm6
	VADDPD ymm11, ymm11, ymm15
	VADDPD ymm11, ymm11, ymm4
	VEXTRACTF128 xmm8, ymm11, 1
	VADDPD xmm11, xmm11, xmm8
	VUNPCKHPD xmm8, xmm11, xmm11
	VADDSD xmm11, xmm11, xmm8
	.return_ok:
	VMOVSD [rsi], xmm11
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_SumSquares_V64f_S64f_Haswell
_yepCore_SumSquares_V64f_S64f_Haswell:
%else
section .text
global __yepCore_SumSquares_V64f_S64f_Haswell
__yepCore_SumSquares_V64f_S64f_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm11, xmm11, xmm11
	TEST rdx, rdx
	JZ .return_ok
	VXORPD xmm12, xmm12, xmm12
	VXORPD xmm13, xmm13, xmm13
	VXORPD xmm14, xmm14, xmm14
	VXORPD xmm15, xmm15, xmm15
	VXORPD xmm7, xmm7, xmm7
	VXORPD xmm6, xmm6, xmm6
	VXORPD xmm5, xmm5, xmm5
	TEST rdi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm4, [rdi]
	VFMADD231PD ymm11, ymm4, ymm4
	ADD rdi, 8
	SUB rdx, 1
	JZ .reduce_batch
	TEST rdi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rdx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD ymm4, [rdi]
	VMOVUPD ymm8, [byte rdi + 32]
	VMOVUPD ymm9, [byte rdi + 64]
	VMOVUPD ymm10, [byte rdi + 96]
	VFMADD231PD ymm11, ymm4, ymm4
	VMOVUPD ymm3, [dword rdi + 128]
	VFMADD231PD ymm12, ymm8, ymm8
	VMOVUPD ymm2, [dword rdi + 160]
	VFMADD231PD ymm13, ymm9, ymm9
	VMOVUPD ymm9, [dword rdi + 192]
	VFMADD231PD ymm14, ymm10, ymm10
	VMOVUPD ymm10, [dword rdi + 224]
	VFMADD231PD ymm15, ymm3, ymm3
	ADD rdi, 256
	VFMADD231PD ymm7, ymm2, ymm2
	SUB rdx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD ymm4, [rdi]
	VFMADD231PD ymm6, ymm9, ymm9
	VMOVUPD ymm8, [byte rdi + 32]
	VFMADD231PD ymm5, ymm10, ymm10
	VMOVUPD ymm9, [byte rdi + 64]
	VMOVUPD ymm10, [byte rdi + 96]
	VFMADD231PD ymm11, ymm4, ymm4
	VMOVUPD ymm3, [dword rdi + 128]
	VFMADD231PD ymm12, ymm8, ymm8
	VMOVUPD ymm2, [dword rdi + 160]
	VFMADD231PD ymm13, ymm9, ymm9
	VMOVUPD ymm9, [dword rdi + 192]
	VFMADD231PD ymm14, ymm10, ymm10
	VMOVUPD ymm10, [dword rdi + 224]
	VFMADD231PD ymm15, ymm3, ymm3
	ADD rdi, 256
	VFMADD231PD ymm7, ymm2, ymm2
	SUB rdx, 32
	JAE .process_batch
	.process_batch_epilogue:
	VFMADD231PD ymm6, ymm9, ymm9
	VFMADD231PD ymm5, ymm10, ymm10
	.batch_process_finish:
	ADD rdx, 32
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm8, [rdi]
	VFMADD231PD ymm11, ymm8, ymm8
	ADD rdi, 8
	SUB rdx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD ymm11, ymm11, ymm12
	VADDPD ymm13, ymm13, ymm14
	VADDPD ymm15, ymm15, ymm7
	VADDPD ymm6, ymm6, ymm5
	VADDPD ymm11, ymm11, ymm13
	VADDPD ymm15, ymm15, ymm6
	VADDPD ymm11, ymm11, ymm15
	VEXTRACTF128 xmm8, ymm11, 1
	VADDPD xmm11, xmm11, xmm8
	VUNPCKHPD xmm8, xmm11, xmm11
	VADDSD xmm11, xmm11, xmm8
	.return_ok:
	VMOVSD [rsi], xmm11
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return
