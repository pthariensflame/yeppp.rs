;                       Yeppp! library implementation
;                   This file is auto-generated by Peach-Py,
;        Portable Efficient Assembly Code-generator in Higher-level Python,
;                  part of the Yeppp! library infrastructure
; This file is part of Yeppp! library and licensed under the New BSD license.
; See LICENSE.txt for the full text of the license.

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Sum_V32f_S32f_Nehalem
_yepCore_Sum_V32f_S32f_Nehalem:
%else
section .text
global __yepCore_Sum_V32f_S32f_Nehalem
__yepCore_Sum_V32f_S32f_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	XORPS xmm15, xmm15
	TEST rdx, rdx
	JZ .return_ok
	XORPS xmm7, xmm7
	XORPS xmm6, xmm6
	XORPS xmm5, xmm5
	XORPS xmm4, xmm4
	XORPS xmm3, xmm3
	XORPS xmm2, xmm2
	XORPS xmm1, xmm1
	TEST rdi, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSS xmm0, [rdi]
	ADDPS xmm15, xmm0
	ADD rdi, 4
	SUB rdx, 1
	JZ .reduce_batch
	TEST rdi, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB rdx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPS xmm0, [rdi]
	MOVUPS xmm8, [byte rdi + 16]
	MOVUPS xmm9, [byte rdi + 32]
	MOVUPS xmm10, [byte rdi + 48]
	MOVUPS xmm12, [byte rdi + 64]
	MOVUPS xmm11, [byte rdi + 80]
	ADDPS xmm15, xmm0
	MOVUPS xmm13, [byte rdi + 96]
	ADDPS xmm7, xmm8
	MOVUPS xmm14, [byte rdi + 112]
	ADDPS xmm6, xmm9
	ADD rdi, 128
	ADDPS xmm5, xmm10
	SUB rdx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPS xmm0, [rdi]
	ADDPS xmm4, xmm12
	MOVUPS xmm8, [byte rdi + 16]
	ADDPS xmm3, xmm11
	MOVUPS xmm9, [byte rdi + 32]
	ADDPS xmm2, xmm13
	MOVUPS xmm10, [byte rdi + 48]
	ADDPS xmm1, xmm14
	MOVUPS xmm12, [byte rdi + 64]
	MOVUPS xmm11, [byte rdi + 80]
	ADDPS xmm15, xmm0
	MOVUPS xmm13, [byte rdi + 96]
	ADDPS xmm7, xmm8
	MOVUPS xmm14, [byte rdi + 112]
	ADDPS xmm6, xmm9
	ADD rdi, 128
	ADDPS xmm5, xmm10
	SUB rdx, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADDPS xmm4, xmm12
	ADDPS xmm3, xmm11
	ADDPS xmm2, xmm13
	ADDPS xmm1, xmm14
	.batch_process_finish:
	ADD rdx, 32
	JZ .reduce_batch
	.process_single:
	MOVSS xmm8, [rdi]
	ADDPS xmm15, xmm8
	ADD rdi, 4
	SUB rdx, 1
	JNZ .process_single
	.reduce_batch:
	ADDPS xmm15, xmm7
	ADDPS xmm6, xmm5
	ADDPS xmm4, xmm3
	ADDPS xmm2, xmm1
	ADDPS xmm15, xmm6
	ADDPS xmm4, xmm2
	ADDPS xmm15, xmm4
	MOVHLPS xmm8, xmm15
	ADDPS xmm15, xmm8
	MOVSHDUP xmm8, xmm15
	ADDSS xmm15, xmm8
	.return_ok:
	MOVSS [rsi], xmm15
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Sum_V32f_S32f_SandyBridge
_yepCore_Sum_V32f_S32f_SandyBridge:
%else
section .text
global __yepCore_Sum_V32f_S32f_SandyBridge
__yepCore_Sum_V32f_S32f_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm13, xmm13, xmm13
	TEST rdx, rdx
	JZ .return_ok
	VXORPS xmm14, xmm14, xmm14
	VXORPS xmm15, xmm15, xmm15
	VXORPS xmm7, xmm7, xmm7
	VXORPS xmm6, xmm6, xmm6
	VXORPS xmm5, xmm5, xmm5
	VXORPS xmm4, xmm4, xmm4
	VXORPS xmm3, xmm3, xmm3
	TEST rdi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm2, [rdi]
	VADDPS ymm13, ymm13, ymm2
	ADD rdi, 4
	SUB rdx, 1
	JZ .reduce_batch
	TEST rdi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rdx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS ymm2, [rdi]
	VMOVUPS ymm8, [byte rdi + 32]
	VMOVUPS ymm9, [byte rdi + 64]
	VMOVUPS ymm1, [byte rdi + 96]
	VMOVUPS ymm0, [dword rdi + 128]
	VADDPS ymm13, ymm13, ymm2
	VMOVUPS ymm12, [dword rdi + 160]
	VADDPS ymm14, ymm14, ymm8
	VMOVUPS ymm10, [dword rdi + 192]
	VADDPS ymm15, ymm15, ymm9
	VMOVUPS ymm11, [dword rdi + 224]
	VADDPS ymm7, ymm7, ymm1
	ADD rdi, 256
	VADDPS ymm6, ymm6, ymm0
	SUB rdx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS ymm2, [rdi]
	VADDPS ymm5, ymm5, ymm12
	VMOVUPS ymm8, [byte rdi + 32]
	VADDPS ymm4, ymm4, ymm10
	VMOVUPS ymm9, [byte rdi + 64]
	VADDPS ymm3, ymm3, ymm11
	VMOVUPS ymm1, [byte rdi + 96]
	VMOVUPS ymm0, [dword rdi + 128]
	VADDPS ymm13, ymm13, ymm2
	VMOVUPS ymm12, [dword rdi + 160]
	VADDPS ymm14, ymm14, ymm8
	VMOVUPS ymm10, [dword rdi + 192]
	VADDPS ymm15, ymm15, ymm9
	VMOVUPS ymm11, [dword rdi + 224]
	VADDPS ymm7, ymm7, ymm1
	ADD rdi, 256
	VADDPS ymm6, ymm6, ymm0
	SUB rdx, 64
	JAE .process_batch
	.process_batch_epilogue:
	VADDPS ymm5, ymm5, ymm12
	VADDPS ymm4, ymm4, ymm10
	VADDPS ymm3, ymm3, ymm11
	.batch_process_finish:
	ADD rdx, 64
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm8, [rdi]
	VADDPS ymm13, ymm13, ymm8
	ADD rdi, 4
	SUB rdx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS ymm13, ymm13, ymm14
	VADDPS ymm15, ymm15, ymm7
	VADDPS ymm6, ymm6, ymm5
	VADDPS ymm4, ymm4, ymm3
	VADDPS ymm13, ymm13, ymm15
	VADDPS ymm6, ymm6, ymm4
	VADDPS ymm13, ymm13, ymm6
	VEXTRACTF128 xmm8, ymm13, 1
	VADDPS xmm13, xmm13, xmm8
	VUNPCKHPD xmm8, xmm13, xmm13
	VADDPS xmm13, xmm13, xmm8
	VMOVSHDUP xmm8, xmm13
	VADDSS xmm13, xmm13, xmm8
	.return_ok:
	VMOVSS [rsi], xmm13
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Bulldozer progbits alloc exec nowrite align=16
global _yepCore_Sum_V32f_S32f_Bulldozer
_yepCore_Sum_V32f_S32f_Bulldozer:
%else
section .text
global __yepCore_Sum_V32f_S32f_Bulldozer
__yepCore_Sum_V32f_S32f_Bulldozer:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm11, xmm11, xmm11
	TEST rdx, rdx
	JZ .return_ok
	VXORPS xmm12, xmm12, xmm12
	VXORPS xmm13, xmm13, xmm13
	VXORPS xmm14, xmm14, xmm14
	VXORPS xmm15, xmm15, xmm15
	VXORPS xmm7, xmm7, xmm7
	TEST rdi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm6, [rdi]
	VADDPS xmm11, xmm11, xmm6
	ADD rdi, 4
	SUB rdx, 1
	JZ .reduce_batch
	TEST rdi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rdx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS xmm6, [rdi]
	VMOVUPS xmm10, [byte rdi + 16]
	VMOVUPS ymm5, [byte rdi + 32]
	VMOVUPS xmm4, [byte rdi + 64]
	VADDPS xmm11, xmm11, xmm6
	VMOVUPS xmm8, [byte rdi + 80]
	VADDPS xmm12, xmm12, xmm10
	VMOVUPS ymm9, [byte rdi + 96]
	VADDPS ymm13, ymm13, ymm5
	ADD rdi, 128
	VADDPS xmm14, xmm14, xmm4
	SUB rdx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS xmm6, [rdi]
	VADDPS xmm15, xmm15, xmm8
	VMOVUPS xmm10, [byte rdi + 16]
	VADDPS ymm7, ymm7, ymm9
	VMOVUPS ymm5, [byte rdi + 32]
	VMOVUPS xmm4, [byte rdi + 64]
	VADDPS xmm11, xmm11, xmm6
	VMOVUPS xmm8, [byte rdi + 80]
	VADDPS xmm12, xmm12, xmm10
	VMOVUPS ymm9, [byte rdi + 96]
	VADDPS ymm13, ymm13, ymm5
	ADD rdi, 128
	VADDPS xmm14, xmm14, xmm4
	SUB rdx, 32
	JAE .process_batch
	.process_batch_epilogue:
	VADDPS xmm15, xmm15, xmm8
	VADDPS ymm7, ymm7, ymm9
	.batch_process_finish:
	ADD rdx, 32
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm8, [rdi]
	VADDPS xmm11, xmm11, xmm8
	ADD rdi, 4
	SUB rdx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS xmm11, xmm11, xmm12
	VADDPS ymm13, ymm13, ymm14
	VADDPS ymm15, ymm15, ymm7
	VADDPS ymm11, ymm11, ymm13
	VADDPS ymm11, ymm11, ymm15
	VEXTRACTF128 xmm8, ymm11, 1
	VADDPS xmm11, xmm11, xmm8
	VUNPCKHPD xmm8, xmm11, xmm11
	VADDPS xmm11, xmm11, xmm8
	VMOVSHDUP xmm8, xmm11
	VADDSS xmm11, xmm11, xmm8
	.return_ok:
	VMOVSS [rsi], xmm11
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Sum_V64f_S64f_Nehalem
_yepCore_Sum_V64f_S64f_Nehalem:
%else
section .text
global __yepCore_Sum_V64f_S64f_Nehalem
__yepCore_Sum_V64f_S64f_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	XORPD xmm15, xmm15
	TEST rdx, rdx
	JZ .return_ok
	XORPD xmm7, xmm7
	XORPD xmm6, xmm6
	XORPD xmm5, xmm5
	XORPD xmm4, xmm4
	XORPD xmm3, xmm3
	XORPD xmm2, xmm2
	XORPD xmm1, xmm1
	TEST rdi, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSD xmm0, [rdi]
	ADDPD xmm15, xmm0
	ADD rdi, 8
	SUB rdx, 1
	JZ .reduce_batch
	TEST rdi, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB rdx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPD xmm0, [rdi]
	MOVUPD xmm8, [byte rdi + 16]
	MOVUPD xmm9, [byte rdi + 32]
	MOVUPD xmm10, [byte rdi + 48]
	MOVUPD xmm12, [byte rdi + 64]
	MOVUPD xmm11, [byte rdi + 80]
	ADDPD xmm15, xmm0
	MOVUPD xmm13, [byte rdi + 96]
	ADDPD xmm7, xmm8
	MOVUPD xmm14, [byte rdi + 112]
	ADDPD xmm6, xmm9
	ADD rdi, 128
	ADDPD xmm5, xmm10
	SUB rdx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPD xmm0, [rdi]
	ADDPD xmm4, xmm12
	MOVUPD xmm8, [byte rdi + 16]
	ADDPD xmm3, xmm11
	MOVUPD xmm9, [byte rdi + 32]
	ADDPD xmm2, xmm13
	MOVUPD xmm10, [byte rdi + 48]
	ADDPD xmm1, xmm14
	MOVUPD xmm12, [byte rdi + 64]
	MOVUPD xmm11, [byte rdi + 80]
	ADDPD xmm15, xmm0
	MOVUPD xmm13, [byte rdi + 96]
	ADDPD xmm7, xmm8
	MOVUPD xmm14, [byte rdi + 112]
	ADDPD xmm6, xmm9
	ADD rdi, 128
	ADDPD xmm5, xmm10
	SUB rdx, 16
	JAE .process_batch
	.process_batch_epilogue:
	ADDPD xmm4, xmm12
	ADDPD xmm3, xmm11
	ADDPD xmm2, xmm13
	ADDPD xmm1, xmm14
	.batch_process_finish:
	ADD rdx, 16
	JZ .reduce_batch
	.process_single:
	MOVSD xmm8, [rdi]
	ADDPD xmm15, xmm8
	ADD rdi, 8
	SUB rdx, 1
	JNZ .process_single
	.reduce_batch:
	ADDPD xmm15, xmm7
	ADDPD xmm6, xmm5
	ADDPD xmm4, xmm3
	ADDPD xmm2, xmm1
	ADDPD xmm15, xmm6
	ADDPD xmm4, xmm2
	ADDPD xmm15, xmm4
	MOVHLPS xmm8, xmm15
	ADDSD xmm15, xmm8
	.return_ok:
	MOVSD [rsi], xmm15
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Sum_V64f_S64f_SandyBridge
_yepCore_Sum_V64f_S64f_SandyBridge:
%else
section .text
global __yepCore_Sum_V64f_S64f_SandyBridge
__yepCore_Sum_V64f_S64f_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm13, xmm13, xmm13
	TEST rdx, rdx
	JZ .return_ok
	VXORPD xmm14, xmm14, xmm14
	VXORPD xmm15, xmm15, xmm15
	VXORPD xmm7, xmm7, xmm7
	VXORPD xmm6, xmm6, xmm6
	VXORPD xmm5, xmm5, xmm5
	VXORPD xmm4, xmm4, xmm4
	VXORPD xmm3, xmm3, xmm3
	TEST rdi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm2, [rdi]
	VADDPD ymm13, ymm13, ymm2
	ADD rdi, 8
	SUB rdx, 1
	JZ .reduce_batch
	TEST rdi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rdx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD ymm2, [rdi]
	VMOVUPD ymm8, [byte rdi + 32]
	VMOVUPD ymm9, [byte rdi + 64]
	VMOVUPD ymm1, [byte rdi + 96]
	VMOVUPD ymm0, [dword rdi + 128]
	VADDPD ymm13, ymm13, ymm2
	VMOVUPD ymm12, [dword rdi + 160]
	VADDPD ymm14, ymm14, ymm8
	VMOVUPD ymm10, [dword rdi + 192]
	VADDPD ymm15, ymm15, ymm9
	VMOVUPD ymm11, [dword rdi + 224]
	VADDPD ymm7, ymm7, ymm1
	ADD rdi, 256
	VADDPD ymm6, ymm6, ymm0
	SUB rdx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD ymm2, [rdi]
	VADDPD ymm5, ymm5, ymm12
	VMOVUPD ymm8, [byte rdi + 32]
	VADDPD ymm4, ymm4, ymm10
	VMOVUPD ymm9, [byte rdi + 64]
	VADDPD ymm3, ymm3, ymm11
	VMOVUPD ymm1, [byte rdi + 96]
	VMOVUPD ymm0, [dword rdi + 128]
	VADDPD ymm13, ymm13, ymm2
	VMOVUPD ymm12, [dword rdi + 160]
	VADDPD ymm14, ymm14, ymm8
	VMOVUPD ymm10, [dword rdi + 192]
	VADDPD ymm15, ymm15, ymm9
	VMOVUPD ymm11, [dword rdi + 224]
	VADDPD ymm7, ymm7, ymm1
	ADD rdi, 256
	VADDPD ymm6, ymm6, ymm0
	SUB rdx, 32
	JAE .process_batch
	.process_batch_epilogue:
	VADDPD ymm5, ymm5, ymm12
	VADDPD ymm4, ymm4, ymm10
	VADDPD ymm3, ymm3, ymm11
	.batch_process_finish:
	ADD rdx, 32
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm8, [rdi]
	VADDPD ymm13, ymm13, ymm8
	ADD rdi, 8
	SUB rdx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD ymm13, ymm13, ymm14
	VADDPD ymm15, ymm15, ymm7
	VADDPD ymm6, ymm6, ymm5
	VADDPD ymm4, ymm4, ymm3
	VADDPD ymm13, ymm13, ymm15
	VADDPD ymm6, ymm6, ymm4
	VADDPD ymm13, ymm13, ymm6
	VEXTRACTF128 xmm8, ymm13, 1
	VADDPD xmm13, xmm13, xmm8
	VUNPCKHPD xmm8, xmm13, xmm13
	VADDSD xmm13, xmm13, xmm8
	.return_ok:
	VMOVSD [rsi], xmm13
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Bulldozer progbits alloc exec nowrite align=16
global _yepCore_Sum_V64f_S64f_Bulldozer
_yepCore_Sum_V64f_S64f_Bulldozer:
%else
section .text
global __yepCore_Sum_V64f_S64f_Bulldozer
__yepCore_Sum_V64f_S64f_Bulldozer:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm11, xmm11, xmm11
	TEST rdx, rdx
	JZ .return_ok
	VXORPD xmm12, xmm12, xmm12
	VXORPD xmm13, xmm13, xmm13
	VXORPD xmm14, xmm14, xmm14
	VXORPD xmm15, xmm15, xmm15
	VXORPD xmm7, xmm7, xmm7
	TEST rdi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm6, [rdi]
	VADDPD xmm11, xmm11, xmm6
	ADD rdi, 8
	SUB rdx, 1
	JZ .reduce_batch
	TEST rdi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rdx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD xmm6, [rdi]
	VMOVUPD xmm10, [byte rdi + 16]
	VMOVUPD ymm5, [byte rdi + 32]
	VMOVUPD xmm4, [byte rdi + 64]
	VADDPD xmm11, xmm11, xmm6
	VMOVUPD xmm8, [byte rdi + 80]
	VADDPD xmm12, xmm12, xmm10
	VMOVUPD ymm9, [byte rdi + 96]
	VADDPD ymm13, ymm13, ymm5
	ADD rdi, 128
	VADDPD xmm14, xmm14, xmm4
	SUB rdx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD xmm6, [rdi]
	VADDPD xmm15, xmm15, xmm8
	VMOVUPD xmm10, [byte rdi + 16]
	VADDPD ymm7, ymm7, ymm9
	VMOVUPD ymm5, [byte rdi + 32]
	VMOVUPD xmm4, [byte rdi + 64]
	VADDPD xmm11, xmm11, xmm6
	VMOVUPD xmm8, [byte rdi + 80]
	VADDPD xmm12, xmm12, xmm10
	VMOVUPD ymm9, [byte rdi + 96]
	VADDPD ymm13, ymm13, ymm5
	ADD rdi, 128
	VADDPD xmm14, xmm14, xmm4
	SUB rdx, 16
	JAE .process_batch
	.process_batch_epilogue:
	VADDPD xmm15, xmm15, xmm8
	VADDPD ymm7, ymm7, ymm9
	.batch_process_finish:
	ADD rdx, 16
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm8, [rdi]
	VADDPD xmm11, xmm11, xmm8
	ADD rdi, 8
	SUB rdx, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD xmm11, xmm11, xmm12
	VADDPD ymm13, ymm13, ymm14
	VADDPD ymm15, ymm15, ymm7
	VADDPD ymm11, ymm11, ymm13
	VADDPD ymm11, ymm11, ymm15
	VEXTRACTF128 xmm8, ymm11, 1
	VADDPD xmm11, xmm11, xmm8
	VUNPCKHPD xmm8, xmm11, xmm11
	VADDSD xmm11, xmm11, xmm8
	.return_ok:
	VMOVSD [rsi], xmm11
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return
