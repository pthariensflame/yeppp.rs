;                       Yeppp! library implementation
;                   This file is auto-generated by Peach-Py,
;        Portable Efficient Assembly Code-generator in Higher-level Python,
;                  part of the Yeppp! library infrastructure
; This file is part of Yeppp! library and licensed under the New BSD license.
; See LICENSE.txt for the full text of the license.

section .text$e code align=16
global _yepCore_DotProduct_V32fV32f_S32f_Nehalem
_yepCore_DotProduct_V32fV32f_S32f_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm7
	MOVAPS [byte rsp + 16], xmm8
	MOVAPS [byte rsp + 32], xmm9
	MOVAPS [byte rsp + 48], xmm10
	MOVAPS [byte rsp + 64], xmm11
	MOVAPS [byte rsp + 80], xmm12
	MOVAPS [byte rsp + 96], xmm13
	MOVAPS [byte rsp + 112], xmm14
	MOVAPS [dword rsp + 128], xmm15
	MOVAPS [dword rsp + 144], xmm6
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	XORPS xmm7, xmm7
	TEST r9, r9
	JZ .return_ok
	XORPS xmm8, xmm8
	XORPS xmm9, xmm9
	XORPS xmm10, xmm10
	XORPS xmm11, xmm11
	XORPS xmm12, xmm12
	XORPS xmm13, xmm13
	XORPS xmm14, xmm14
	TEST rdx, 15
	JZ .source_y_16b_aligned
	.source_y_16b_misaligned:
	MOVSS xmm15, [rcx]
	MULSS xmm15, [rdx]
	ADDPS xmm7, xmm15
	ADD rcx, 4
	ADD rdx, 4
	SUB r9, 1
	JZ .reduce_batch
	TEST rdx, 15
	JNZ .source_y_16b_misaligned
	.source_y_16b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPS xmm15, [rcx]
	MOVUPS xmm4, [byte rcx + 16]
	MOVUPS xmm5, [byte rcx + 32]
	MULPS xmm15, [rdx]
	MOVUPS xmm2, [byte rcx + 48]
	MULPS xmm4, [byte rdx + 16]
	MOVUPS xmm1, [byte rcx + 64]
	MULPS xmm5, [byte rdx + 32]
	MOVUPS xmm3, [byte rcx + 80]
	MULPS xmm2, [byte rdx + 48]
	ADDPS xmm7, xmm15
	MOVUPS xmm0, [byte rcx + 96]
	MULPS xmm1, [byte rdx + 64]
	ADDPS xmm8, xmm4
	MOVUPS xmm6, [byte rcx + 112]
	MULPS xmm3, [byte rdx + 80]
	ADDPS xmm9, xmm5
	ADD rcx, 128
	MULPS xmm0, [byte rdx + 96]
	ADDPS xmm10, xmm2
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPS xmm15, [rcx]
	MULPS xmm6, [byte rdx + 112]
	ADDPS xmm11, xmm1
	MOVUPS xmm4, [byte rcx + 16]
	ADD rdx, 128
	ADDPS xmm12, xmm3
	MOVUPS xmm5, [byte rcx + 32]
	MULPS xmm15, [rdx]
	ADDPS xmm13, xmm0
	MOVUPS xmm2, [byte rcx + 48]
	MULPS xmm4, [byte rdx + 16]
	ADDPS xmm14, xmm6
	MOVUPS xmm1, [byte rcx + 64]
	MULPS xmm5, [byte rdx + 32]
	MOVUPS xmm3, [byte rcx + 80]
	MULPS xmm2, [byte rdx + 48]
	ADDPS xmm7, xmm15
	MOVUPS xmm0, [byte rcx + 96]
	MULPS xmm1, [byte rdx + 64]
	ADDPS xmm8, xmm4
	MOVUPS xmm6, [byte rcx + 112]
	MULPS xmm3, [byte rdx + 80]
	ADDPS xmm9, xmm5
	ADD rcx, 128
	MULPS xmm0, [byte rdx + 96]
	ADDPS xmm10, xmm2
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	MULPS xmm6, [byte rdx + 112]
	ADDPS xmm11, xmm1
	ADD rdx, 128
	ADDPS xmm12, xmm3
	ADDPS xmm13, xmm0
	ADDPS xmm14, xmm6
	.batch_process_finish:
	ADD r9, 32
	JZ .reduce_batch
	.process_single:
	MOVSS xmm4, [rcx]
	MULSS xmm4, [rdx]
	ADDPS xmm7, xmm4
	ADD rcx, 4
	ADD rdx, 4
	SUB r9, 1
	JNZ .process_single
	.reduce_batch:
	ADDPS xmm7, xmm8
	ADDPS xmm9, xmm10
	ADDPS xmm11, xmm12
	ADDPS xmm13, xmm14
	ADDPS xmm7, xmm9
	ADDPS xmm11, xmm13
	ADDPS xmm7, xmm11
	MOVHLPS xmm4, xmm7
	ADDPS xmm7, xmm4
	MOVSHDUP xmm4, xmm7
	ADDSS xmm7, xmm4
	.return_ok:
	MOVSS [r8], xmm7
	XOR eax, eax
	.return:
	MOVAPS xmm7, [rsp]
	MOVAPS xmm8, [byte rsp + 16]
	MOVAPS xmm9, [byte rsp + 32]
	MOVAPS xmm10, [byte rsp + 48]
	MOVAPS xmm11, [byte rsp + 64]
	MOVAPS xmm12, [byte rsp + 80]
	MOVAPS xmm13, [byte rsp + 96]
	MOVAPS xmm14, [byte rsp + 112]
	MOVAPS xmm15, [dword rsp + 128]
	MOVAPS xmm6, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_DotProduct_V32fV32f_S32f_SandyBridge
_yepCore_DotProduct_V32fV32f_S32f_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm7
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm9
	VMOVAPS [byte rsp + 48], xmm10
	VMOVAPS [byte rsp + 64], xmm11
	VMOVAPS [byte rsp + 80], xmm12
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm14
	VMOVAPS [dword rsp + 128], xmm15
	VMOVAPS [dword rsp + 144], xmm6
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm7, xmm7, xmm7
	TEST r9, r9
	JZ .return_ok
	VXORPS xmm8, xmm8, xmm8
	VXORPS xmm9, xmm9, xmm9
	VXORPS xmm10, xmm10, xmm10
	VXORPS xmm11, xmm11, xmm11
	VXORPS xmm12, xmm12, xmm12
	VXORPS xmm13, xmm13, xmm13
	VXORPS xmm14, xmm14, xmm14
	TEST rdx, 31
	JZ .source_y_32b_aligned
	.source_y_32b_misaligned:
	VMOVSS xmm15, [rcx]
	VMULSS xmm15, xmm15, [rdx]
	VADDPS ymm7, ymm7, ymm15
	ADD rcx, 4
	ADD rdx, 4
	SUB r9, 1
	JZ .reduce_batch
	TEST rdx, 31
	JNZ .source_y_32b_misaligned
	.source_y_32b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS ymm15, [rcx]
	VMOVUPS ymm4, [byte rcx + 32]
	VMOVUPS ymm5, [byte rcx + 64]
	VMULPS ymm15, ymm15, [rdx]
	VMOVUPS ymm1, [byte rcx + 96]
	VMULPS ymm4, ymm4, [byte rdx + 32]
	VMOVUPS ymm6, [dword rcx + 128]
	VMULPS ymm5, ymm5, [byte rdx + 64]
	VMOVUPS ymm3, [dword rcx + 160]
	VMULPS ymm1, ymm1, [byte rdx + 96]
	VADDPS ymm7, ymm7, ymm15
	VMOVUPS ymm2, [dword rcx + 192]
	VMULPS ymm6, ymm6, [dword rdx + 128]
	VADDPS ymm8, ymm8, ymm4
	VMOVUPS ymm0, [dword rcx + 224]
	VMULPS ymm3, ymm3, [dword rdx + 160]
	VADDPS ymm9, ymm9, ymm5
	ADD rcx, 256
	VMULPS ymm2, ymm2, [dword rdx + 192]
	VADDPS ymm10, ymm10, ymm1
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS ymm15, [rcx]
	VMULPS ymm0, ymm0, [dword rdx + 224]
	VADDPS ymm11, ymm11, ymm6
	VMOVUPS ymm4, [byte rcx + 32]
	ADD rdx, 256
	VADDPS ymm12, ymm12, ymm3
	VMOVUPS ymm5, [byte rcx + 64]
	VMULPS ymm15, ymm15, [rdx]
	VADDPS ymm13, ymm13, ymm2
	VMOVUPS ymm1, [byte rcx + 96]
	VMULPS ymm4, ymm4, [byte rdx + 32]
	VADDPS ymm14, ymm14, ymm0
	VMOVUPS ymm6, [dword rcx + 128]
	VMULPS ymm5, ymm5, [byte rdx + 64]
	VMOVUPS ymm3, [dword rcx + 160]
	VMULPS ymm1, ymm1, [byte rdx + 96]
	VADDPS ymm7, ymm7, ymm15
	VMOVUPS ymm2, [dword rcx + 192]
	VMULPS ymm6, ymm6, [dword rdx + 128]
	VADDPS ymm8, ymm8, ymm4
	VMOVUPS ymm0, [dword rcx + 224]
	VMULPS ymm3, ymm3, [dword rdx + 160]
	VADDPS ymm9, ymm9, ymm5
	ADD rcx, 256
	VMULPS ymm2, ymm2, [dword rdx + 192]
	VADDPS ymm10, ymm10, ymm1
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	VMULPS ymm0, ymm0, [dword rdx + 224]
	VADDPS ymm11, ymm11, ymm6
	ADD rdx, 256
	VADDPS ymm12, ymm12, ymm3
	VADDPS ymm13, ymm13, ymm2
	VADDPS ymm14, ymm14, ymm0
	.batch_process_finish:
	ADD r9, 64
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm4, [rcx]
	VMULSS xmm4, xmm4, [rdx]
	VADDPS ymm7, ymm7, ymm4
	ADD rcx, 4
	ADD rdx, 4
	SUB r9, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS ymm7, ymm7, ymm8
	VADDPS ymm9, ymm9, ymm10
	VADDPS ymm11, ymm11, ymm12
	VADDPS ymm13, ymm13, ymm14
	VADDPS ymm7, ymm7, ymm9
	VADDPS ymm11, ymm11, ymm13
	VADDPS ymm7, ymm7, ymm11
	VEXTRACTF128 xmm4, ymm7, 1
	VADDPS xmm7, xmm7, xmm4
	VUNPCKHPD xmm4, xmm7, xmm7
	VADDPS xmm7, xmm7, xmm4
	VMOVSHDUP xmm4, xmm7
	VADDSS xmm7, xmm7, xmm4
	.return_ok:
	VMOVSS [r8], xmm7
	XOR eax, eax
	.return:
	VMOVAPS xmm7, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm9, [byte rsp + 32]
	VMOVAPS xmm10, [byte rsp + 48]
	VMOVAPS xmm11, [byte rsp + 64]
	VMOVAPS xmm12, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm14, [byte rsp + 112]
	VMOVAPS xmm15, [dword rsp + 128]
	VMOVAPS xmm6, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$n code align=16
global _yepCore_DotProduct_V32fV32f_S32f_Bulldozer
_yepCore_DotProduct_V32fV32f_S32f_Bulldozer:
	.ENTRY:
	SUB rsp, 104
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm1, xmm1, xmm1
	TEST r9, r9
	JZ .return_ok
	VXORPS xmm0, xmm0, xmm0
	VXORPS xmm6, xmm6, xmm6
	VXORPS xmm7, xmm7, xmm7
	VXORPS xmm8, xmm8, xmm8
	VXORPS xmm9, xmm9, xmm9
	TEST rdx, 31
	JZ .source_y_32b_aligned
	.source_y_32b_misaligned:
	VMOVSS xmm10, [rcx]
	VMOVSS xmm11, [rdx]
	VFMADDPS xmm1, xmm10, xmm11, xmm1
	ADD rcx, 4
	ADD rdx, 4
	SUB r9, 1
	JZ .reduce_batch
	TEST rdx, 31
	JNZ .source_y_32b_misaligned
	.source_y_32b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS xmm10, [rcx]
	VMOVUPS xmm4, [byte rcx + 16]
	VMOVUPS ymm5, [byte rcx + 32]
	VMOVUPS xmm11, [byte rcx + 64]
	VFMADDPS xmm1, xmm10, [rdx], xmm1
	VMOVUPS xmm2, [byte rcx + 80]
	VFMADDPS xmm0, xmm4, [byte rdx + 16], xmm0
	VMOVUPS ymm3, [byte rcx + 96]
	VFMADDPS ymm6, ymm5, [byte rdx + 32], ymm6
	ADD rcx, 128
	VFMADDPS xmm7, xmm11, [byte rdx + 64], xmm7
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS xmm10, [rcx]
	VFMADDPS xmm8, xmm2, [byte rdx + 80], xmm8
	VMOVUPS xmm4, [byte rcx + 16]
	VFMADDPS ymm9, ymm3, [byte rdx + 96], ymm9
	VMOVUPS ymm5, [byte rcx + 32]
	ADD rdx, 128
	VMOVUPS xmm11, [byte rcx + 64]
	VFMADDPS xmm1, xmm10, [rdx], xmm1
	VMOVUPS xmm2, [byte rcx + 80]
	VFMADDPS xmm0, xmm4, [byte rdx + 16], xmm0
	VMOVUPS ymm3, [byte rcx + 96]
	VFMADDPS ymm6, ymm5, [byte rdx + 32], ymm6
	ADD rcx, 128
	VFMADDPS xmm7, xmm11, [byte rdx + 64], xmm7
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	VFMADDPS xmm8, xmm2, [byte rdx + 80], xmm8
	VFMADDPS ymm9, ymm3, [byte rdx + 96], ymm9
	ADD rdx, 128
	.batch_process_finish:
	ADD r9, 32
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm4, [rcx]
	VMOVSS xmm5, [rdx]
	VFMADDPS xmm1, xmm4, xmm5, xmm1
	ADD rcx, 4
	ADD rdx, 4
	SUB r9, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS xmm1, xmm1, xmm0
	VADDPS ymm6, ymm6, ymm7
	VADDPS ymm8, ymm8, ymm9
	VADDPS ymm1, ymm1, ymm6
	VADDPS ymm1, ymm1, ymm8
	VEXTRACTF128 xmm4, ymm1, 1
	VADDPS xmm1, xmm1, xmm4
	VUNPCKHPD xmm4, xmm1, xmm1
	VADDPS xmm1, xmm1, xmm4
	VMOVSHDUP xmm4, xmm1
	VADDSS xmm1, xmm1, xmm4
	.return_ok:
	VMOVSS [r8], xmm1
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	ADD rsp, 104
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_DotProduct_V32fV32f_S32f_Haswell
_yepCore_DotProduct_V32fV32f_S32f_Haswell:
	.ENTRY:
	SUB rsp, 152
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm14
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm1, xmm1, xmm1
	TEST r9, r9
	JZ .return_ok
	VXORPS xmm0, xmm0, xmm0
	VXORPS xmm6, xmm6, xmm6
	VXORPS xmm7, xmm7, xmm7
	VXORPS xmm8, xmm8, xmm8
	VXORPS xmm9, xmm9, xmm9
	VXORPS xmm10, xmm10, xmm10
	VXORPS xmm11, xmm11, xmm11
	TEST rdx, 31
	JZ .source_y_32b_aligned
	.source_y_32b_misaligned:
	VMOVSS xmm12, [rcx]
	VMOVSS xmm13, [rdx]
	VFMADD231PS ymm1, ymm12, ymm13
	ADD rcx, 4
	ADD rdx, 4
	SUB r9, 1
	JZ .reduce_batch
	TEST rdx, 31
	JNZ .source_y_32b_misaligned
	.source_y_32b_aligned:
	SUB r9, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS ymm12, [rcx]
	VMOVUPS ymm4, [byte rcx + 32]
	VMOVUPS ymm5, [byte rcx + 64]
	VMOVUPS ymm2, [byte rcx + 96]
	VFMADD231PS ymm1, ymm12, [rdx]
	VMOVUPS ymm13, [dword rcx + 128]
	VFMADD231PS ymm0, ymm4, [byte rdx + 32]
	VMOVUPS ymm14, [dword rcx + 160]
	VFMADD231PS ymm6, ymm5, [byte rdx + 64]
	VMOVUPS ymm3, [dword rcx + 192]
	VFMADD231PS ymm7, ymm2, [byte rdx + 96]
	VMOVUPS ymm2, [dword rcx + 224]
	VFMADD231PS ymm8, ymm13, [dword rdx + 128]
	ADD rcx, 256
	VFMADD231PS ymm9, ymm14, [dword rdx + 160]
	SUB r9, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS ymm12, [rcx]
	VFMADD231PS ymm10, ymm3, [dword rdx + 192]
	VMOVUPS ymm4, [byte rcx + 32]
	VFMADD231PS ymm11, ymm2, [dword rdx + 224]
	VMOVUPS ymm5, [byte rcx + 64]
	ADD rdx, 256
	VMOVUPS ymm2, [byte rcx + 96]
	VFMADD231PS ymm1, ymm12, [rdx]
	VMOVUPS ymm13, [dword rcx + 128]
	VFMADD231PS ymm0, ymm4, [byte rdx + 32]
	VMOVUPS ymm14, [dword rcx + 160]
	VFMADD231PS ymm6, ymm5, [byte rdx + 64]
	VMOVUPS ymm3, [dword rcx + 192]
	VFMADD231PS ymm7, ymm2, [byte rdx + 96]
	VMOVUPS ymm2, [dword rcx + 224]
	VFMADD231PS ymm8, ymm13, [dword rdx + 128]
	ADD rcx, 256
	VFMADD231PS ymm9, ymm14, [dword rdx + 160]
	SUB r9, 64
	JAE .process_batch
	.process_batch_epilogue:
	VFMADD231PS ymm10, ymm3, [dword rdx + 192]
	VFMADD231PS ymm11, ymm2, [dword rdx + 224]
	ADD rdx, 256
	.batch_process_finish:
	ADD r9, 64
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm4, [rcx]
	VMOVSS xmm5, [rdx]
	VFMADD231PS ymm1, ymm4, ymm5
	ADD rcx, 4
	ADD rdx, 4
	SUB r9, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS ymm1, ymm1, ymm0
	VADDPS ymm6, ymm6, ymm7
	VADDPS ymm8, ymm8, ymm9
	VADDPS ymm10, ymm10, ymm11
	VADDPS ymm1, ymm1, ymm6
	VADDPS ymm8, ymm8, ymm10
	VADDPS ymm1, ymm1, ymm8
	VEXTRACTF128 xmm4, ymm1, 1
	VADDPS xmm1, xmm1, xmm4
	VUNPCKHPD xmm4, xmm1, xmm1
	VADDPS xmm1, xmm1, xmm4
	VMOVSHDUP xmm4, xmm1
	VADDSS xmm1, xmm1, xmm4
	.return_ok:
	VMOVSS [r8], xmm1
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm14, [dword rsp + 128]
	ADD rsp, 152
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_DotProduct_V64fV64f_S64f_Nehalem
_yepCore_DotProduct_V64fV64f_S64f_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm7
	MOVAPS [byte rsp + 16], xmm8
	MOVAPS [byte rsp + 32], xmm9
	MOVAPS [byte rsp + 48], xmm10
	MOVAPS [byte rsp + 64], xmm11
	MOVAPS [byte rsp + 80], xmm12
	MOVAPS [byte rsp + 96], xmm13
	MOVAPS [byte rsp + 112], xmm14
	MOVAPS [dword rsp + 128], xmm15
	MOVAPS [dword rsp + 144], xmm6
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	XORPD xmm7, xmm7
	TEST r9, r9
	JZ .return_ok
	XORPD xmm8, xmm8
	XORPD xmm9, xmm9
	XORPD xmm10, xmm10
	XORPD xmm11, xmm11
	XORPD xmm12, xmm12
	XORPD xmm13, xmm13
	XORPD xmm14, xmm14
	TEST rdx, 15
	JZ .source_y_16b_aligned
	.source_y_16b_misaligned:
	MOVSD xmm15, [rcx]
	MULSD xmm15, [rdx]
	ADDPD xmm7, xmm15
	ADD rcx, 8
	ADD rdx, 8
	SUB r9, 1
	JZ .reduce_batch
	TEST rdx, 15
	JNZ .source_y_16b_misaligned
	.source_y_16b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPD xmm15, [rcx]
	MOVUPD xmm4, [byte rcx + 16]
	MOVUPD xmm5, [byte rcx + 32]
	MULPD xmm15, [rdx]
	MOVUPD xmm2, [byte rcx + 48]
	MULPD xmm4, [byte rdx + 16]
	MOVUPD xmm1, [byte rcx + 64]
	MULPD xmm5, [byte rdx + 32]
	MOVUPD xmm3, [byte rcx + 80]
	MULPD xmm2, [byte rdx + 48]
	ADDPD xmm7, xmm15
	MOVUPD xmm0, [byte rcx + 96]
	MULPD xmm1, [byte rdx + 64]
	ADDPD xmm8, xmm4
	MOVUPD xmm6, [byte rcx + 112]
	MULPD xmm3, [byte rdx + 80]
	ADDPD xmm9, xmm5
	ADD rcx, 128
	MULPD xmm0, [byte rdx + 96]
	ADDPD xmm10, xmm2
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPD xmm15, [rcx]
	MULPD xmm6, [byte rdx + 112]
	ADDPD xmm11, xmm1
	MOVUPD xmm4, [byte rcx + 16]
	ADD rdx, 128
	ADDPD xmm12, xmm3
	MOVUPD xmm5, [byte rcx + 32]
	MULPD xmm15, [rdx]
	ADDPD xmm13, xmm0
	MOVUPD xmm2, [byte rcx + 48]
	MULPD xmm4, [byte rdx + 16]
	ADDPD xmm14, xmm6
	MOVUPD xmm1, [byte rcx + 64]
	MULPD xmm5, [byte rdx + 32]
	MOVUPD xmm3, [byte rcx + 80]
	MULPD xmm2, [byte rdx + 48]
	ADDPD xmm7, xmm15
	MOVUPD xmm0, [byte rcx + 96]
	MULPD xmm1, [byte rdx + 64]
	ADDPD xmm8, xmm4
	MOVUPD xmm6, [byte rcx + 112]
	MULPD xmm3, [byte rdx + 80]
	ADDPD xmm9, xmm5
	ADD rcx, 128
	MULPD xmm0, [byte rdx + 96]
	ADDPD xmm10, xmm2
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	MULPD xmm6, [byte rdx + 112]
	ADDPD xmm11, xmm1
	ADD rdx, 128
	ADDPD xmm12, xmm3
	ADDPD xmm13, xmm0
	ADDPD xmm14, xmm6
	.batch_process_finish:
	ADD r9, 16
	JZ .reduce_batch
	.process_single:
	MOVSD xmm4, [rcx]
	MULSD xmm4, [rdx]
	ADDPD xmm7, xmm4
	ADD rcx, 8
	ADD rdx, 8
	SUB r9, 1
	JNZ .process_single
	.reduce_batch:
	ADDPD xmm7, xmm8
	ADDPD xmm9, xmm10
	ADDPD xmm11, xmm12
	ADDPD xmm13, xmm14
	ADDPD xmm7, xmm9
	ADDPD xmm11, xmm13
	ADDPD xmm7, xmm11
	MOVHLPS xmm4, xmm7
	ADDSD xmm7, xmm4
	.return_ok:
	MOVSD [r8], xmm7
	XOR eax, eax
	.return:
	MOVAPS xmm7, [rsp]
	MOVAPS xmm8, [byte rsp + 16]
	MOVAPS xmm9, [byte rsp + 32]
	MOVAPS xmm10, [byte rsp + 48]
	MOVAPS xmm11, [byte rsp + 64]
	MOVAPS xmm12, [byte rsp + 80]
	MOVAPS xmm13, [byte rsp + 96]
	MOVAPS xmm14, [byte rsp + 112]
	MOVAPS xmm15, [dword rsp + 128]
	MOVAPS xmm6, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$i code align=16
global _yepCore_DotProduct_V64fV64f_S64f_Bonnell
_yepCore_DotProduct_V64fV64f_S64f_Bonnell:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm7
	MOVAPS [byte rsp + 16], xmm8
	MOVAPS [byte rsp + 32], xmm9
	MOVAPS [byte rsp + 48], xmm10
	MOVAPS [byte rsp + 64], xmm11
	MOVAPS [byte rsp + 80], xmm12
	MOVAPS [byte rsp + 96], xmm13
	MOVAPS [byte rsp + 112], xmm14
	MOVAPS [dword rsp + 128], xmm15
	MOVAPS [dword rsp + 144], xmm6
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	XORPD xmm7, xmm7
	TEST r9, r9
	JZ .return_ok
	XORPD xmm8, xmm8
	XORPD xmm9, xmm9
	XORPD xmm10, xmm10
	XORPD xmm11, xmm11
	XORPD xmm12, xmm12
	XORPD xmm13, xmm13
	XORPD xmm14, xmm14
	SUB r9, 8
	JB .batch_process_finish
	.process_batch_prologue:
	MOVSD xmm15, [rcx]
	MOVSD xmm4, [byte rcx + 8]
	MULSD xmm15, [rdx]
	MOVSD xmm5, [byte rcx + 16]
	MULSD xmm4, [byte rdx + 8]
	MOVSD xmm2, [byte rcx + 24]
	MULSD xmm5, [byte rdx + 16]
	MOVSD xmm1, [byte rcx + 32]
	MULSD xmm2, [byte rdx + 24]
	MOVSD xmm3, [byte rcx + 40]
	MULSD xmm1, [byte rdx + 32]
	ADDSD xmm7, xmm15
	MOVSD xmm0, [byte rcx + 48]
	MULSD xmm3, [byte rdx + 40]
	ADDSD xmm8, xmm4
	MOVSD xmm6, [byte rcx + 56]
	MULSD xmm0, [byte rdx + 48]
	ADDSD xmm9, xmm5
	ADD rcx, 64
	MULSD xmm6, [byte rdx + 56]
	ADDSD xmm10, xmm2
	SUB r9, 8
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVSD xmm15, [rcx]
	ADD rdx, 64
	ADDSD xmm11, xmm1
	MOVSD xmm4, [byte rcx + 8]
	MULSD xmm15, [rdx]
	ADDSD xmm12, xmm3
	MOVSD xmm5, [byte rcx + 16]
	MULSD xmm4, [byte rdx + 8]
	ADDSD xmm13, xmm0
	MOVSD xmm2, [byte rcx + 24]
	MULSD xmm5, [byte rdx + 16]
	ADDSD xmm14, xmm6
	MOVSD xmm1, [byte rcx + 32]
	MULSD xmm2, [byte rdx + 24]
	MOVSD xmm3, [byte rcx + 40]
	MULSD xmm1, [byte rdx + 32]
	ADDSD xmm7, xmm15
	MOVSD xmm0, [byte rcx + 48]
	MULSD xmm3, [byte rdx + 40]
	ADDSD xmm8, xmm4
	MOVSD xmm6, [byte rcx + 56]
	MULSD xmm0, [byte rdx + 48]
	ADDSD xmm9, xmm5
	ADD rcx, 64
	MULSD xmm6, [byte rdx + 56]
	ADDSD xmm10, xmm2
	SUB r9, 8
	JAE .process_batch
	.process_batch_epilogue:
	ADD rdx, 64
	ADDSD xmm11, xmm1
	ADDSD xmm12, xmm3
	ADDSD xmm13, xmm0
	ADDSD xmm14, xmm6
	.batch_process_finish:
	ADD r9, 8
	JZ .reduce_batch
	.process_single:
	MOVSD xmm4, [rcx]
	MULSD xmm4, [rdx]
	ADDPD xmm7, xmm4
	ADD rcx, 8
	ADD rdx, 8
	SUB r9, 1
	JNZ .process_single
	.reduce_batch:
	ADDPD xmm7, xmm8
	ADDPD xmm9, xmm10
	ADDPD xmm11, xmm12
	ADDPD xmm13, xmm14
	ADDPD xmm7, xmm9
	ADDPD xmm11, xmm13
	ADDPD xmm7, xmm11
	MOVHLPS xmm4, xmm7
	ADDSD xmm7, xmm4
	.return_ok:
	MOVSD [r8], xmm7
	XOR eax, eax
	.return:
	MOVAPS xmm7, [rsp]
	MOVAPS xmm8, [byte rsp + 16]
	MOVAPS xmm9, [byte rsp + 32]
	MOVAPS xmm10, [byte rsp + 48]
	MOVAPS xmm11, [byte rsp + 64]
	MOVAPS xmm12, [byte rsp + 80]
	MOVAPS xmm13, [byte rsp + 96]
	MOVAPS xmm14, [byte rsp + 112]
	MOVAPS xmm15, [dword rsp + 128]
	MOVAPS xmm6, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_DotProduct_V64fV64f_S64f_SandyBridge
_yepCore_DotProduct_V64fV64f_S64f_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm7
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm9
	VMOVAPS [byte rsp + 48], xmm10
	VMOVAPS [byte rsp + 64], xmm11
	VMOVAPS [byte rsp + 80], xmm12
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm14
	VMOVAPS [dword rsp + 128], xmm15
	VMOVAPS [dword rsp + 144], xmm6
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm7, xmm7, xmm7
	TEST r9, r9
	JZ .return_ok
	VXORPD xmm8, xmm8, xmm8
	VXORPD xmm9, xmm9, xmm9
	VXORPD xmm10, xmm10, xmm10
	VXORPD xmm11, xmm11, xmm11
	VXORPD xmm12, xmm12, xmm12
	VXORPD xmm13, xmm13, xmm13
	VXORPD xmm14, xmm14, xmm14
	TEST rdx, 31
	JZ .source_y_32b_aligned
	.source_y_32b_misaligned:
	VMOVSD xmm15, [rcx]
	VMULSD xmm15, xmm15, [rdx]
	VADDPD ymm7, ymm7, ymm15
	ADD rcx, 8
	ADD rdx, 8
	SUB r9, 1
	JZ .reduce_batch
	TEST rdx, 31
	JNZ .source_y_32b_misaligned
	.source_y_32b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD ymm15, [rcx]
	VMOVUPD ymm4, [byte rcx + 32]
	VMOVUPD ymm5, [byte rcx + 64]
	VMULPD ymm15, ymm15, [rdx]
	VMOVUPD ymm1, [byte rcx + 96]
	VMULPD ymm4, ymm4, [byte rdx + 32]
	VMOVUPD ymm6, [dword rcx + 128]
	VMULPD ymm5, ymm5, [byte rdx + 64]
	VMOVUPD ymm3, [dword rcx + 160]
	VMULPD ymm1, ymm1, [byte rdx + 96]
	VADDPD ymm7, ymm7, ymm15
	VMOVUPD ymm2, [dword rcx + 192]
	VMULPD ymm6, ymm6, [dword rdx + 128]
	VADDPD ymm8, ymm8, ymm4
	VMOVUPD ymm0, [dword rcx + 224]
	VMULPD ymm3, ymm3, [dword rdx + 160]
	VADDPD ymm9, ymm9, ymm5
	ADD rcx, 256
	VMULPD ymm2, ymm2, [dword rdx + 192]
	VADDPD ymm10, ymm10, ymm1
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD ymm15, [rcx]
	VMULPD ymm0, ymm0, [dword rdx + 224]
	VADDPD ymm11, ymm11, ymm6
	VMOVUPD ymm4, [byte rcx + 32]
	ADD rdx, 256
	VADDPD ymm12, ymm12, ymm3
	VMOVUPD ymm5, [byte rcx + 64]
	VMULPD ymm15, ymm15, [rdx]
	VADDPD ymm13, ymm13, ymm2
	VMOVUPD ymm1, [byte rcx + 96]
	VMULPD ymm4, ymm4, [byte rdx + 32]
	VADDPD ymm14, ymm14, ymm0
	VMOVUPD ymm6, [dword rcx + 128]
	VMULPD ymm5, ymm5, [byte rdx + 64]
	VMOVUPD ymm3, [dword rcx + 160]
	VMULPD ymm1, ymm1, [byte rdx + 96]
	VADDPD ymm7, ymm7, ymm15
	VMOVUPD ymm2, [dword rcx + 192]
	VMULPD ymm6, ymm6, [dword rdx + 128]
	VADDPD ymm8, ymm8, ymm4
	VMOVUPD ymm0, [dword rcx + 224]
	VMULPD ymm3, ymm3, [dword rdx + 160]
	VADDPD ymm9, ymm9, ymm5
	ADD rcx, 256
	VMULPD ymm2, ymm2, [dword rdx + 192]
	VADDPD ymm10, ymm10, ymm1
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	VMULPD ymm0, ymm0, [dword rdx + 224]
	VADDPD ymm11, ymm11, ymm6
	ADD rdx, 256
	VADDPD ymm12, ymm12, ymm3
	VADDPD ymm13, ymm13, ymm2
	VADDPD ymm14, ymm14, ymm0
	.batch_process_finish:
	ADD r9, 32
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm4, [rcx]
	VMULSD xmm4, xmm4, [rdx]
	VADDPD ymm7, ymm7, ymm4
	ADD rcx, 8
	ADD rdx, 8
	SUB r9, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD ymm7, ymm7, ymm8
	VADDPD ymm9, ymm9, ymm10
	VADDPD ymm11, ymm11, ymm12
	VADDPD ymm13, ymm13, ymm14
	VADDPD ymm7, ymm7, ymm9
	VADDPD ymm11, ymm11, ymm13
	VADDPD ymm7, ymm7, ymm11
	VEXTRACTF128 xmm4, ymm7, 1
	VADDPD xmm7, xmm7, xmm4
	VUNPCKHPD xmm4, xmm7, xmm7
	VADDSD xmm7, xmm7, xmm4
	.return_ok:
	VMOVSD [r8], xmm7
	XOR eax, eax
	.return:
	VMOVAPS xmm7, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm9, [byte rsp + 32]
	VMOVAPS xmm10, [byte rsp + 48]
	VMOVAPS xmm11, [byte rsp + 64]
	VMOVAPS xmm12, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm14, [byte rsp + 112]
	VMOVAPS xmm15, [dword rsp + 128]
	VMOVAPS xmm6, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$n code align=16
global _yepCore_DotProduct_V64fV64f_S64f_Bulldozer
_yepCore_DotProduct_V64fV64f_S64f_Bulldozer:
	.ENTRY:
	SUB rsp, 104
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm1, xmm1, xmm1
	TEST r9, r9
	JZ .return_ok
	VXORPD xmm0, xmm0, xmm0
	VXORPD xmm6, xmm6, xmm6
	VXORPD xmm7, xmm7, xmm7
	VXORPD xmm8, xmm8, xmm8
	VXORPD xmm9, xmm9, xmm9
	TEST rdx, 31
	JZ .source_y_32b_aligned
	.source_y_32b_misaligned:
	VMOVSD xmm10, [rcx]
	VMOVSD xmm11, [rdx]
	VFMADDPD xmm1, xmm10, xmm11, xmm1
	ADD rcx, 8
	ADD rdx, 8
	SUB r9, 1
	JZ .reduce_batch
	TEST rdx, 31
	JNZ .source_y_32b_misaligned
	.source_y_32b_aligned:
	SUB r9, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD xmm10, [rcx]
	VMOVUPD xmm4, [byte rcx + 16]
	VMOVUPD ymm5, [byte rcx + 32]
	VMOVUPD xmm11, [byte rcx + 64]
	VFMADDPD xmm1, xmm10, [rdx], xmm1
	VMOVUPD xmm2, [byte rcx + 80]
	VFMADDPD xmm0, xmm4, [byte rdx + 16], xmm0
	VMOVUPD ymm3, [byte rcx + 96]
	VFMADDPD ymm6, ymm5, [byte rdx + 32], ymm6
	ADD rcx, 128
	VFMADDPD xmm7, xmm11, [byte rdx + 64], xmm7
	SUB r9, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD xmm10, [rcx]
	VFMADDPD xmm8, xmm2, [byte rdx + 80], xmm8
	VMOVUPD xmm4, [byte rcx + 16]
	VFMADDPD ymm9, ymm3, [byte rdx + 96], ymm9
	VMOVUPD ymm5, [byte rcx + 32]
	ADD rdx, 128
	VMOVUPD xmm11, [byte rcx + 64]
	VFMADDPD xmm1, xmm10, [rdx], xmm1
	VMOVUPD xmm2, [byte rcx + 80]
	VFMADDPD xmm0, xmm4, [byte rdx + 16], xmm0
	VMOVUPD ymm3, [byte rcx + 96]
	VFMADDPD ymm6, ymm5, [byte rdx + 32], ymm6
	ADD rcx, 128
	VFMADDPD xmm7, xmm11, [byte rdx + 64], xmm7
	SUB r9, 16
	JAE .process_batch
	.process_batch_epilogue:
	VFMADDPD xmm8, xmm2, [byte rdx + 80], xmm8
	VFMADDPD ymm9, ymm3, [byte rdx + 96], ymm9
	ADD rdx, 128
	.batch_process_finish:
	ADD r9, 16
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm4, [rcx]
	VMOVSD xmm5, [rdx]
	VFMADDPD xmm1, xmm4, xmm5, xmm1
	ADD rcx, 8
	ADD rdx, 8
	SUB r9, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD xmm1, xmm1, xmm0
	VADDPD ymm6, ymm6, ymm7
	VADDPD ymm8, ymm8, ymm9
	VADDPD ymm1, ymm1, ymm6
	VADDPD ymm1, ymm1, ymm8
	VEXTRACTF128 xmm4, ymm1, 1
	VADDPD xmm1, xmm1, xmm4
	VUNPCKHPD xmm4, xmm1, xmm1
	VADDSD xmm1, xmm1, xmm4
	.return_ok:
	VMOVSD [r8], xmm1
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	ADD rsp, 104
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_DotProduct_V64fV64f_S64f_Haswell
_yepCore_DotProduct_V64fV64f_S64f_Haswell:
	.ENTRY:
	SUB rsp, 152
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm14
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm1, xmm1, xmm1
	TEST r9, r9
	JZ .return_ok
	VXORPD xmm0, xmm0, xmm0
	VXORPD xmm6, xmm6, xmm6
	VXORPD xmm7, xmm7, xmm7
	VXORPD xmm8, xmm8, xmm8
	VXORPD xmm9, xmm9, xmm9
	VXORPD xmm10, xmm10, xmm10
	VXORPD xmm11, xmm11, xmm11
	TEST rdx, 31
	JZ .source_y_32b_aligned
	.source_y_32b_misaligned:
	VMOVSD xmm12, [rcx]
	VMOVSD xmm13, [rdx]
	VFMADD231PD ymm1, ymm12, ymm13
	ADD rcx, 8
	ADD rdx, 8
	SUB r9, 1
	JZ .reduce_batch
	TEST rdx, 31
	JNZ .source_y_32b_misaligned
	.source_y_32b_aligned:
	SUB r9, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD ymm12, [rcx]
	VMOVUPD ymm4, [byte rcx + 32]
	VMOVUPD ymm5, [byte rcx + 64]
	VMOVUPD ymm2, [byte rcx + 96]
	VFMADD231PD ymm1, ymm12, [rdx]
	VMOVUPD ymm13, [dword rcx + 128]
	VFMADD231PD ymm0, ymm4, [byte rdx + 32]
	VMOVUPD ymm14, [dword rcx + 160]
	VFMADD231PD ymm6, ymm5, [byte rdx + 64]
	VMOVUPD ymm3, [dword rcx + 192]
	VFMADD231PD ymm7, ymm2, [byte rdx + 96]
	VMOVUPD ymm2, [dword rcx + 224]
	VFMADD231PD ymm8, ymm13, [dword rdx + 128]
	ADD rcx, 256
	VFMADD231PD ymm9, ymm14, [dword rdx + 160]
	SUB r9, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD ymm12, [rcx]
	VFMADD231PD ymm10, ymm3, [dword rdx + 192]
	VMOVUPD ymm4, [byte rcx + 32]
	VFMADD231PD ymm11, ymm2, [dword rdx + 224]
	VMOVUPD ymm5, [byte rcx + 64]
	ADD rdx, 256
	VMOVUPD ymm2, [byte rcx + 96]
	VFMADD231PD ymm1, ymm12, [rdx]
	VMOVUPD ymm13, [dword rcx + 128]
	VFMADD231PD ymm0, ymm4, [byte rdx + 32]
	VMOVUPD ymm14, [dword rcx + 160]
	VFMADD231PD ymm6, ymm5, [byte rdx + 64]
	VMOVUPD ymm3, [dword rcx + 192]
	VFMADD231PD ymm7, ymm2, [byte rdx + 96]
	VMOVUPD ymm2, [dword rcx + 224]
	VFMADD231PD ymm8, ymm13, [dword rdx + 128]
	ADD rcx, 256
	VFMADD231PD ymm9, ymm14, [dword rdx + 160]
	SUB r9, 32
	JAE .process_batch
	.process_batch_epilogue:
	VFMADD231PD ymm10, ymm3, [dword rdx + 192]
	VFMADD231PD ymm11, ymm2, [dword rdx + 224]
	ADD rdx, 256
	.batch_process_finish:
	ADD r9, 32
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm4, [rcx]
	VMOVSD xmm5, [rdx]
	VFMADD231PD ymm1, ymm4, ymm5
	ADD rcx, 8
	ADD rdx, 8
	SUB r9, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD ymm1, ymm1, ymm0
	VADDPD ymm6, ymm6, ymm7
	VADDPD ymm8, ymm8, ymm9
	VADDPD ymm10, ymm10, ymm11
	VADDPD ymm1, ymm1, ymm6
	VADDPD ymm8, ymm8, ymm10
	VADDPD ymm1, ymm1, ymm8
	VEXTRACTF128 xmm4, ymm1, 1
	VADDPD xmm1, xmm1, xmm4
	VUNPCKHPD xmm4, xmm1, xmm1
	VADDSD xmm1, xmm1, xmm4
	.return_ok:
	VMOVSD [r8], xmm1
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm14, [dword rsp + 128]
	ADD rsp, 152
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return
