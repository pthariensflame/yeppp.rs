;                       Yeppp! library implementation
;                   This file is auto-generated by Peach-Py,
;        Portable Efficient Assembly Code-generator in Higher-level Python,
;                  part of the Yeppp! library infrastructure
; This file is part of Yeppp! library and licensed under the New BSD license.
; See LICENSE.txt for the full text of the license.

section .text$e code align=16
global _yepCore_SumSquares_V32f_S32f_Nehalem
_yepCore_SumSquares_V32f_S32f_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm7
	MOVAPS [byte rsp + 16], xmm8
	MOVAPS [byte rsp + 32], xmm9
	MOVAPS [byte rsp + 48], xmm10
	MOVAPS [byte rsp + 64], xmm11
	MOVAPS [byte rsp + 80], xmm12
	MOVAPS [byte rsp + 96], xmm13
	MOVAPS [byte rsp + 112], xmm14
	MOVAPS [dword rsp + 128], xmm15
	MOVAPS [dword rsp + 144], xmm6
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	XORPS xmm7, xmm7
	TEST r8, r8
	JZ .return_ok
	XORPS xmm8, xmm8
	XORPS xmm9, xmm9
	XORPS xmm10, xmm10
	XORPS xmm11, xmm11
	XORPS xmm12, xmm12
	XORPS xmm13, xmm13
	XORPS xmm14, xmm14
	TEST rcx, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSS xmm15, [rcx]
	MULSS xmm15, xmm15
	ADDPS xmm7, xmm15
	ADD rcx, 4
	SUB r8, 1
	JZ .reduce_batch
	TEST rcx, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB r8, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPS xmm15, [rcx]
	MOVUPS xmm4, [byte rcx + 16]
	MOVUPS xmm5, [byte rcx + 32]
	MULPS xmm15, xmm15
	MOVUPS xmm3, [byte rcx + 48]
	MULPS xmm4, xmm4
	MOVUPS xmm1, [byte rcx + 64]
	MULPS xmm5, xmm5
	MOVUPS xmm2, [byte rcx + 80]
	MULPS xmm3, xmm3
	ADDPS xmm7, xmm15
	MOVUPS xmm0, [byte rcx + 96]
	MULPS xmm1, xmm1
	ADDPS xmm8, xmm4
	MOVUPS xmm6, [byte rcx + 112]
	MULPS xmm2, xmm2
	ADDPS xmm9, xmm5
	ADD rcx, 128
	MULPS xmm0, xmm0
	ADDPS xmm10, xmm3
	SUB r8, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPS xmm15, [rcx]
	MULPS xmm6, xmm6
	ADDPS xmm11, xmm1
	MOVUPS xmm4, [byte rcx + 16]
	ADDPS xmm12, xmm2
	MOVUPS xmm5, [byte rcx + 32]
	MULPS xmm15, xmm15
	ADDPS xmm13, xmm0
	MOVUPS xmm3, [byte rcx + 48]
	MULPS xmm4, xmm4
	ADDPS xmm14, xmm6
	MOVUPS xmm1, [byte rcx + 64]
	MULPS xmm5, xmm5
	MOVUPS xmm2, [byte rcx + 80]
	MULPS xmm3, xmm3
	ADDPS xmm7, xmm15
	MOVUPS xmm0, [byte rcx + 96]
	MULPS xmm1, xmm1
	ADDPS xmm8, xmm4
	MOVUPS xmm6, [byte rcx + 112]
	MULPS xmm2, xmm2
	ADDPS xmm9, xmm5
	ADD rcx, 128
	MULPS xmm0, xmm0
	ADDPS xmm10, xmm3
	SUB r8, 32
	JAE .process_batch
	.process_batch_epilogue:
	MULPS xmm6, xmm6
	ADDPS xmm11, xmm1
	ADDPS xmm12, xmm2
	ADDPS xmm13, xmm0
	ADDPS xmm14, xmm6
	.batch_process_finish:
	ADD r8, 32
	JZ .reduce_batch
	.process_single:
	MOVSS xmm4, [rcx]
	MULSS xmm4, xmm4
	ADDPS xmm7, xmm4
	ADD rcx, 4
	SUB r8, 1
	JNZ .process_single
	.reduce_batch:
	ADDPS xmm7, xmm8
	ADDPS xmm9, xmm10
	ADDPS xmm11, xmm12
	ADDPS xmm13, xmm14
	ADDPS xmm7, xmm9
	ADDPS xmm11, xmm13
	ADDPS xmm7, xmm11
	MOVHLPS xmm4, xmm7
	ADDPS xmm7, xmm4
	MOVSHDUP xmm4, xmm7
	ADDSS xmm7, xmm4
	.return_ok:
	MOVSS [rdx], xmm7
	XOR eax, eax
	.return:
	MOVAPS xmm7, [rsp]
	MOVAPS xmm8, [byte rsp + 16]
	MOVAPS xmm9, [byte rsp + 32]
	MOVAPS xmm10, [byte rsp + 48]
	MOVAPS xmm11, [byte rsp + 64]
	MOVAPS xmm12, [byte rsp + 80]
	MOVAPS xmm13, [byte rsp + 96]
	MOVAPS xmm14, [byte rsp + 112]
	MOVAPS xmm15, [dword rsp + 128]
	MOVAPS xmm6, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_SumSquares_V32f_S32f_SandyBridge
_yepCore_SumSquares_V32f_S32f_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm7
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm9
	VMOVAPS [byte rsp + 48], xmm10
	VMOVAPS [byte rsp + 64], xmm11
	VMOVAPS [byte rsp + 80], xmm12
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm14
	VMOVAPS [dword rsp + 128], xmm15
	VMOVAPS [dword rsp + 144], xmm6
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm7, xmm7, xmm7
	TEST r8, r8
	JZ .return_ok
	VXORPS xmm8, xmm8, xmm8
	VXORPS xmm9, xmm9, xmm9
	VXORPS xmm10, xmm10, xmm10
	VXORPS xmm11, xmm11, xmm11
	VXORPS xmm12, xmm12, xmm12
	VXORPS xmm13, xmm13, xmm13
	VXORPS xmm14, xmm14, xmm14
	TEST rcx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm15, [rcx]
	VMULSS xmm15, xmm15, xmm15
	VADDPS ymm7, ymm7, ymm15
	ADD rcx, 4
	SUB r8, 1
	JZ .reduce_batch
	TEST rcx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS ymm15, [rcx]
	VMOVUPS ymm4, [byte rcx + 32]
	VMOVUPS ymm5, [byte rcx + 64]
	VMULPS ymm15, ymm15, ymm15
	VMOVUPS ymm1, [byte rcx + 96]
	VMULPS ymm4, ymm4, ymm4
	VMOVUPS ymm6, [dword rcx + 128]
	VMULPS ymm5, ymm5, ymm5
	VMOVUPS ymm0, [dword rcx + 160]
	VMULPS ymm1, ymm1, ymm1
	VADDPS ymm7, ymm7, ymm15
	VMOVUPS ymm3, [dword rcx + 192]
	VMULPS ymm6, ymm6, ymm6
	VADDPS ymm8, ymm8, ymm4
	VMOVUPS ymm2, [dword rcx + 224]
	VMULPS ymm0, ymm0, ymm0
	VADDPS ymm9, ymm9, ymm5
	ADD rcx, 256
	VMULPS ymm3, ymm3, ymm3
	VADDPS ymm10, ymm10, ymm1
	SUB r8, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS ymm15, [rcx]
	VMULPS ymm2, ymm2, ymm2
	VADDPS ymm11, ymm11, ymm6
	VMOVUPS ymm4, [byte rcx + 32]
	VADDPS ymm12, ymm12, ymm0
	VMOVUPS ymm5, [byte rcx + 64]
	VMULPS ymm15, ymm15, ymm15
	VADDPS ymm13, ymm13, ymm3
	VMOVUPS ymm1, [byte rcx + 96]
	VMULPS ymm4, ymm4, ymm4
	VADDPS ymm14, ymm14, ymm2
	VMOVUPS ymm6, [dword rcx + 128]
	VMULPS ymm5, ymm5, ymm5
	VMOVUPS ymm0, [dword rcx + 160]
	VMULPS ymm1, ymm1, ymm1
	VADDPS ymm7, ymm7, ymm15
	VMOVUPS ymm3, [dword rcx + 192]
	VMULPS ymm6, ymm6, ymm6
	VADDPS ymm8, ymm8, ymm4
	VMOVUPS ymm2, [dword rcx + 224]
	VMULPS ymm0, ymm0, ymm0
	VADDPS ymm9, ymm9, ymm5
	ADD rcx, 256
	VMULPS ymm3, ymm3, ymm3
	VADDPS ymm10, ymm10, ymm1
	SUB r8, 64
	JAE .process_batch
	.process_batch_epilogue:
	VMULPS ymm2, ymm2, ymm2
	VADDPS ymm11, ymm11, ymm6
	VADDPS ymm12, ymm12, ymm0
	VADDPS ymm13, ymm13, ymm3
	VADDPS ymm14, ymm14, ymm2
	.batch_process_finish:
	ADD r8, 64
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm4, [rcx]
	VMULSS xmm4, xmm4, xmm4
	VADDPS ymm7, ymm7, ymm4
	ADD rcx, 4
	SUB r8, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS ymm7, ymm7, ymm8
	VADDPS ymm9, ymm9, ymm10
	VADDPS ymm11, ymm11, ymm12
	VADDPS ymm13, ymm13, ymm14
	VADDPS ymm7, ymm7, ymm9
	VADDPS ymm11, ymm11, ymm13
	VADDPS ymm7, ymm7, ymm11
	VEXTRACTF128 xmm4, ymm7, 1
	VADDPS xmm7, xmm7, xmm4
	VUNPCKHPD xmm4, xmm7, xmm7
	VADDPS xmm7, xmm7, xmm4
	VMOVSHDUP xmm4, xmm7
	VADDSS xmm7, xmm7, xmm4
	.return_ok:
	VMOVSS [rdx], xmm7
	XOR eax, eax
	.return:
	VMOVAPS xmm7, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm9, [byte rsp + 32]
	VMOVAPS xmm10, [byte rsp + 48]
	VMOVAPS xmm11, [byte rsp + 64]
	VMOVAPS xmm12, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm14, [byte rsp + 112]
	VMOVAPS xmm15, [dword rsp + 128]
	VMOVAPS xmm6, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$n code align=16
global _yepCore_SumSquares_V32f_S32f_Bulldozer
_yepCore_SumSquares_V32f_S32f_Bulldozer:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm14
	VMOVAPS [dword rsp + 144], xmm15
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm2, xmm2, xmm2
	TEST r8, r8
	JZ .return_ok
	VXORPS xmm1, xmm1, xmm1
	VXORPS xmm0, xmm0, xmm0
	VXORPS xmm6, xmm6, xmm6
	VXORPS xmm7, xmm7, xmm7
	VXORPS xmm8, xmm8, xmm8
	VXORPS xmm9, xmm9, xmm9
	VXORPS xmm10, xmm10, xmm10
	VXORPS xmm11, xmm11, xmm11
	TEST rcx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm12, [rcx]
	VFMADDPS xmm2, xmm12, xmm12, xmm2
	ADD rcx, 4
	SUB r8, 1
	JZ .reduce_batch
	TEST rcx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 48
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS xmm12, [rcx]
	VMOVUPS xmm4, [byte rcx + 16]
	VMOVUPS ymm5, [byte rcx + 32]
	VMOVUPS xmm3, [byte rcx + 64]
	VFMADDPS xmm2, xmm12, xmm12, xmm2
	VMOVUPS xmm13, [byte rcx + 80]
	VFMADDPS xmm1, xmm4, xmm4, xmm1
	VMOVUPS ymm14, [byte rcx + 96]
	VFMADDPS ymm0, ymm5, ymm5, ymm0
	VMOVUPS xmm15, [dword rcx + 128]
	VFMADDPS xmm6, xmm3, xmm3, xmm6
	VMOVUPS xmm5, [dword rcx + 144]
	VFMADDPS xmm7, xmm13, xmm13, xmm7
	VMOVUPS ymm3, [dword rcx + 160]
	VFMADDPS ymm8, ymm14, ymm14, ymm8
	ADD rcx, 192
	VFMADDPS xmm9, xmm15, xmm15, xmm9
	SUB r8, 48
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS xmm12, [rcx]
	VFMADDPS xmm10, xmm5, xmm5, xmm10
	VMOVUPS xmm4, [byte rcx + 16]
	VFMADDPS ymm11, ymm3, ymm3, ymm11
	VMOVUPS ymm5, [byte rcx + 32]
	VMOVUPS xmm3, [byte rcx + 64]
	VFMADDPS xmm2, xmm12, xmm12, xmm2
	VMOVUPS xmm13, [byte rcx + 80]
	VFMADDPS xmm1, xmm4, xmm4, xmm1
	VMOVUPS ymm14, [byte rcx + 96]
	VFMADDPS ymm0, ymm5, ymm5, ymm0
	VMOVUPS xmm15, [dword rcx + 128]
	VFMADDPS xmm6, xmm3, xmm3, xmm6
	VMOVUPS xmm5, [dword rcx + 144]
	VFMADDPS xmm7, xmm13, xmm13, xmm7
	VMOVUPS ymm3, [dword rcx + 160]
	VFMADDPS ymm8, ymm14, ymm14, ymm8
	ADD rcx, 192
	VFMADDPS xmm9, xmm15, xmm15, xmm9
	SUB r8, 48
	JAE .process_batch
	.process_batch_epilogue:
	VFMADDPS xmm10, xmm5, xmm5, xmm10
	VFMADDPS ymm11, ymm3, ymm3, ymm11
	.batch_process_finish:
	ADD r8, 48
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm4, [rcx]
	VFMADDPS xmm2, xmm4, xmm4, xmm2
	ADD rcx, 4
	SUB r8, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS xmm2, xmm2, xmm1
	VADDPS ymm0, ymm0, ymm6
	VADDPS ymm7, ymm7, ymm8
	VADDPS xmm9, xmm9, xmm10
	VADDPS ymm2, ymm2, ymm0
	VADDPS ymm7, ymm7, ymm9
	VADDPS ymm2, ymm2, ymm7
	VADDPS ymm2, ymm2, ymm11
	VEXTRACTF128 xmm4, ymm2, 1
	VADDPS xmm2, xmm2, xmm4
	VUNPCKHPD xmm4, xmm2, xmm2
	VADDPS xmm2, xmm2, xmm4
	VMOVSHDUP xmm4, xmm2
	VADDSS xmm2, xmm2, xmm4
	.return_ok:
	VMOVSS [rdx], xmm2
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm14, [dword rsp + 128]
	VMOVAPS xmm15, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_SumSquares_V32f_S32f_Haswell
_yepCore_SumSquares_V32f_S32f_Haswell:
	.ENTRY:
	SUB rsp, 136
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	VMOVAPS [byte rsp + 112], xmm13
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	VXORPS xmm2, xmm2, xmm2
	TEST r8, r8
	JZ .return_ok
	VXORPS xmm1, xmm1, xmm1
	VXORPS xmm0, xmm0, xmm0
	VXORPS xmm6, xmm6, xmm6
	VXORPS xmm7, xmm7, xmm7
	VXORPS xmm8, xmm8, xmm8
	VXORPS xmm9, xmm9, xmm9
	VXORPS xmm10, xmm10, xmm10
	TEST rcx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm11, [rcx]
	VFMADD231PS ymm2, ymm11, ymm11
	ADD rcx, 4
	SUB r8, 1
	JZ .reduce_batch
	TEST rcx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS ymm11, [rcx]
	VMOVUPS ymm4, [byte rcx + 32]
	VMOVUPS ymm5, [byte rcx + 64]
	VMOVUPS ymm3, [byte rcx + 96]
	VFMADD231PS ymm2, ymm11, ymm11
	VMOVUPS ymm12, [dword rcx + 128]
	VFMADD231PS ymm1, ymm4, ymm4
	VMOVUPS ymm13, [dword rcx + 160]
	VFMADD231PS ymm0, ymm5, ymm5
	VMOVUPS ymm5, [dword rcx + 192]
	VFMADD231PS ymm6, ymm3, ymm3
	VMOVUPS ymm3, [dword rcx + 224]
	VFMADD231PS ymm7, ymm12, ymm12
	ADD rcx, 256
	VFMADD231PS ymm8, ymm13, ymm13
	SUB r8, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS ymm11, [rcx]
	VFMADD231PS ymm9, ymm5, ymm5
	VMOVUPS ymm4, [byte rcx + 32]
	VFMADD231PS ymm10, ymm3, ymm3
	VMOVUPS ymm5, [byte rcx + 64]
	VMOVUPS ymm3, [byte rcx + 96]
	VFMADD231PS ymm2, ymm11, ymm11
	VMOVUPS ymm12, [dword rcx + 128]
	VFMADD231PS ymm1, ymm4, ymm4
	VMOVUPS ymm13, [dword rcx + 160]
	VFMADD231PS ymm0, ymm5, ymm5
	VMOVUPS ymm5, [dword rcx + 192]
	VFMADD231PS ymm6, ymm3, ymm3
	VMOVUPS ymm3, [dword rcx + 224]
	VFMADD231PS ymm7, ymm12, ymm12
	ADD rcx, 256
	VFMADD231PS ymm8, ymm13, ymm13
	SUB r8, 64
	JAE .process_batch
	.process_batch_epilogue:
	VFMADD231PS ymm9, ymm5, ymm5
	VFMADD231PS ymm10, ymm3, ymm3
	.batch_process_finish:
	ADD r8, 64
	JZ .reduce_batch
	.process_single:
	VMOVSS xmm4, [rcx]
	VFMADD231PS ymm2, ymm4, ymm4
	ADD rcx, 4
	SUB r8, 1
	JNZ .process_single
	.reduce_batch:
	VADDPS ymm2, ymm2, ymm1
	VADDPS ymm0, ymm0, ymm6
	VADDPS ymm7, ymm7, ymm8
	VADDPS ymm9, ymm9, ymm10
	VADDPS ymm2, ymm2, ymm0
	VADDPS ymm7, ymm7, ymm9
	VADDPS ymm2, ymm2, ymm7
	VEXTRACTF128 xmm4, ymm2, 1
	VADDPS xmm2, xmm2, xmm4
	VUNPCKHPD xmm4, xmm2, xmm2
	VADDPS xmm2, xmm2, xmm4
	VMOVSHDUP xmm4, xmm2
	VADDSS xmm2, xmm2, xmm4
	.return_ok:
	VMOVSS [rdx], xmm2
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	ADD rsp, 136
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepCore_SumSquares_V64f_S64f_Nehalem
_yepCore_SumSquares_V64f_S64f_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm7
	MOVAPS [byte rsp + 16], xmm8
	MOVAPS [byte rsp + 32], xmm9
	MOVAPS [byte rsp + 48], xmm10
	MOVAPS [byte rsp + 64], xmm11
	MOVAPS [byte rsp + 80], xmm12
	MOVAPS [byte rsp + 96], xmm13
	MOVAPS [byte rsp + 112], xmm14
	MOVAPS [dword rsp + 128], xmm15
	MOVAPS [dword rsp + 144], xmm6
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	XORPD xmm7, xmm7
	TEST r8, r8
	JZ .return_ok
	XORPD xmm8, xmm8
	XORPD xmm9, xmm9
	XORPD xmm10, xmm10
	XORPD xmm11, xmm11
	XORPD xmm12, xmm12
	XORPD xmm13, xmm13
	XORPD xmm14, xmm14
	TEST rcx, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSD xmm15, [rcx]
	MULSD xmm15, xmm15
	ADDPD xmm7, xmm15
	ADD rcx, 8
	SUB r8, 1
	JZ .reduce_batch
	TEST rcx, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB r8, 16
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPD xmm15, [rcx]
	MOVUPD xmm4, [byte rcx + 16]
	MOVUPD xmm5, [byte rcx + 32]
	MULPD xmm15, xmm15
	MOVUPD xmm3, [byte rcx + 48]
	MULPD xmm4, xmm4
	MOVUPD xmm1, [byte rcx + 64]
	MULPD xmm5, xmm5
	MOVUPD xmm2, [byte rcx + 80]
	MULPD xmm3, xmm3
	ADDPD xmm7, xmm15
	MOVUPD xmm0, [byte rcx + 96]
	MULPD xmm1, xmm1
	ADDPD xmm8, xmm4
	MOVUPD xmm6, [byte rcx + 112]
	MULPD xmm2, xmm2
	ADDPD xmm9, xmm5
	ADD rcx, 128
	MULPD xmm0, xmm0
	ADDPD xmm10, xmm3
	SUB r8, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPD xmm15, [rcx]
	MULPD xmm6, xmm6
	ADDPD xmm11, xmm1
	MOVUPD xmm4, [byte rcx + 16]
	ADDPD xmm12, xmm2
	MOVUPD xmm5, [byte rcx + 32]
	MULPD xmm15, xmm15
	ADDPD xmm13, xmm0
	MOVUPD xmm3, [byte rcx + 48]
	MULPD xmm4, xmm4
	ADDPD xmm14, xmm6
	MOVUPD xmm1, [byte rcx + 64]
	MULPD xmm5, xmm5
	MOVUPD xmm2, [byte rcx + 80]
	MULPD xmm3, xmm3
	ADDPD xmm7, xmm15
	MOVUPD xmm0, [byte rcx + 96]
	MULPD xmm1, xmm1
	ADDPD xmm8, xmm4
	MOVUPD xmm6, [byte rcx + 112]
	MULPD xmm2, xmm2
	ADDPD xmm9, xmm5
	ADD rcx, 128
	MULPD xmm0, xmm0
	ADDPD xmm10, xmm3
	SUB r8, 16
	JAE .process_batch
	.process_batch_epilogue:
	MULPD xmm6, xmm6
	ADDPD xmm11, xmm1
	ADDPD xmm12, xmm2
	ADDPD xmm13, xmm0
	ADDPD xmm14, xmm6
	.batch_process_finish:
	ADD r8, 16
	JZ .reduce_batch
	.process_single:
	MOVSD xmm4, [rcx]
	MULSD xmm4, xmm4
	ADDPD xmm7, xmm4
	ADD rcx, 8
	SUB r8, 1
	JNZ .process_single
	.reduce_batch:
	ADDPD xmm7, xmm8
	ADDPD xmm9, xmm10
	ADDPD xmm11, xmm12
	ADDPD xmm13, xmm14
	ADDPD xmm7, xmm9
	ADDPD xmm11, xmm13
	ADDPD xmm7, xmm11
	MOVHLPS xmm4, xmm7
	ADDSD xmm7, xmm4
	.return_ok:
	MOVSD [rdx], xmm7
	XOR eax, eax
	.return:
	MOVAPS xmm7, [rsp]
	MOVAPS xmm8, [byte rsp + 16]
	MOVAPS xmm9, [byte rsp + 32]
	MOVAPS xmm10, [byte rsp + 48]
	MOVAPS xmm11, [byte rsp + 64]
	MOVAPS xmm12, [byte rsp + 80]
	MOVAPS xmm13, [byte rsp + 96]
	MOVAPS xmm14, [byte rsp + 112]
	MOVAPS xmm15, [dword rsp + 128]
	MOVAPS xmm6, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepCore_SumSquares_V64f_S64f_SandyBridge
_yepCore_SumSquares_V64f_S64f_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm7
	VMOVAPS [byte rsp + 16], xmm8
	VMOVAPS [byte rsp + 32], xmm9
	VMOVAPS [byte rsp + 48], xmm10
	VMOVAPS [byte rsp + 64], xmm11
	VMOVAPS [byte rsp + 80], xmm12
	VMOVAPS [byte rsp + 96], xmm13
	VMOVAPS [byte rsp + 112], xmm14
	VMOVAPS [dword rsp + 128], xmm15
	VMOVAPS [dword rsp + 144], xmm6
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm7, xmm7, xmm7
	TEST r8, r8
	JZ .return_ok
	VXORPD xmm8, xmm8, xmm8
	VXORPD xmm9, xmm9, xmm9
	VXORPD xmm10, xmm10, xmm10
	VXORPD xmm11, xmm11, xmm11
	VXORPD xmm12, xmm12, xmm12
	VXORPD xmm13, xmm13, xmm13
	VXORPD xmm14, xmm14, xmm14
	TEST rcx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm15, [rcx]
	VMULSD xmm15, xmm15, xmm15
	VADDPD ymm7, ymm7, ymm15
	ADD rcx, 8
	SUB r8, 1
	JZ .reduce_batch
	TEST rcx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD ymm15, [rcx]
	VMOVUPD ymm4, [byte rcx + 32]
	VMOVUPD ymm5, [byte rcx + 64]
	VMULPD ymm15, ymm15, ymm15
	VMOVUPD ymm1, [byte rcx + 96]
	VMULPD ymm4, ymm4, ymm4
	VMOVUPD ymm6, [dword rcx + 128]
	VMULPD ymm5, ymm5, ymm5
	VMOVUPD ymm0, [dword rcx + 160]
	VMULPD ymm1, ymm1, ymm1
	VADDPD ymm7, ymm7, ymm15
	VMOVUPD ymm3, [dword rcx + 192]
	VMULPD ymm6, ymm6, ymm6
	VADDPD ymm8, ymm8, ymm4
	VMOVUPD ymm2, [dword rcx + 224]
	VMULPD ymm0, ymm0, ymm0
	VADDPD ymm9, ymm9, ymm5
	ADD rcx, 256
	VMULPD ymm3, ymm3, ymm3
	VADDPD ymm10, ymm10, ymm1
	SUB r8, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD ymm15, [rcx]
	VMULPD ymm2, ymm2, ymm2
	VADDPD ymm11, ymm11, ymm6
	VMOVUPD ymm4, [byte rcx + 32]
	VADDPD ymm12, ymm12, ymm0
	VMOVUPD ymm5, [byte rcx + 64]
	VMULPD ymm15, ymm15, ymm15
	VADDPD ymm13, ymm13, ymm3
	VMOVUPD ymm1, [byte rcx + 96]
	VMULPD ymm4, ymm4, ymm4
	VADDPD ymm14, ymm14, ymm2
	VMOVUPD ymm6, [dword rcx + 128]
	VMULPD ymm5, ymm5, ymm5
	VMOVUPD ymm0, [dword rcx + 160]
	VMULPD ymm1, ymm1, ymm1
	VADDPD ymm7, ymm7, ymm15
	VMOVUPD ymm3, [dword rcx + 192]
	VMULPD ymm6, ymm6, ymm6
	VADDPD ymm8, ymm8, ymm4
	VMOVUPD ymm2, [dword rcx + 224]
	VMULPD ymm0, ymm0, ymm0
	VADDPD ymm9, ymm9, ymm5
	ADD rcx, 256
	VMULPD ymm3, ymm3, ymm3
	VADDPD ymm10, ymm10, ymm1
	SUB r8, 32
	JAE .process_batch
	.process_batch_epilogue:
	VMULPD ymm2, ymm2, ymm2
	VADDPD ymm11, ymm11, ymm6
	VADDPD ymm12, ymm12, ymm0
	VADDPD ymm13, ymm13, ymm3
	VADDPD ymm14, ymm14, ymm2
	.batch_process_finish:
	ADD r8, 32
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm4, [rcx]
	VMULSD xmm4, xmm4, xmm4
	VADDPD ymm7, ymm7, ymm4
	ADD rcx, 8
	SUB r8, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD ymm7, ymm7, ymm8
	VADDPD ymm9, ymm9, ymm10
	VADDPD ymm11, ymm11, ymm12
	VADDPD ymm13, ymm13, ymm14
	VADDPD ymm7, ymm7, ymm9
	VADDPD ymm11, ymm11, ymm13
	VADDPD ymm7, ymm7, ymm11
	VEXTRACTF128 xmm4, ymm7, 1
	VADDPD xmm7, xmm7, xmm4
	VUNPCKHPD xmm4, xmm7, xmm7
	VADDSD xmm7, xmm7, xmm4
	.return_ok:
	VMOVSD [rdx], xmm7
	XOR eax, eax
	.return:
	VMOVAPS xmm7, [rsp]
	VMOVAPS xmm8, [byte rsp + 16]
	VMOVAPS xmm9, [byte rsp + 32]
	VMOVAPS xmm10, [byte rsp + 48]
	VMOVAPS xmm11, [byte rsp + 64]
	VMOVAPS xmm12, [byte rsp + 80]
	VMOVAPS xmm13, [byte rsp + 96]
	VMOVAPS xmm14, [byte rsp + 112]
	VMOVAPS xmm15, [dword rsp + 128]
	VMOVAPS xmm6, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$n code align=16
global _yepCore_SumSquares_V64f_S64f_Bulldozer
_yepCore_SumSquares_V64f_S64f_Bulldozer:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm14
	VMOVAPS [dword rsp + 144], xmm15
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm2, xmm2, xmm2
	TEST r8, r8
	JZ .return_ok
	VXORPD xmm1, xmm1, xmm1
	VXORPD xmm0, xmm0, xmm0
	VXORPD xmm6, xmm6, xmm6
	VXORPD xmm7, xmm7, xmm7
	VXORPD xmm8, xmm8, xmm8
	VXORPD xmm9, xmm9, xmm9
	VXORPD xmm10, xmm10, xmm10
	VXORPD xmm11, xmm11, xmm11
	TEST rcx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm12, [rcx]
	VFMADDPD xmm2, xmm12, xmm12, xmm2
	ADD rcx, 8
	SUB r8, 1
	JZ .reduce_batch
	TEST rcx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 24
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD xmm12, [rcx]
	VMOVUPD xmm4, [byte rcx + 16]
	VMOVUPD ymm5, [byte rcx + 32]
	VMOVUPD xmm3, [byte rcx + 64]
	VFMADDPD xmm2, xmm12, xmm12, xmm2
	VMOVUPD xmm13, [byte rcx + 80]
	VFMADDPD xmm1, xmm4, xmm4, xmm1
	VMOVUPD ymm14, [byte rcx + 96]
	VFMADDPD ymm0, ymm5, ymm5, ymm0
	VMOVUPD xmm15, [dword rcx + 128]
	VFMADDPD xmm6, xmm3, xmm3, xmm6
	VMOVUPD xmm5, [dword rcx + 144]
	VFMADDPD xmm7, xmm13, xmm13, xmm7
	VMOVUPD ymm3, [dword rcx + 160]
	VFMADDPD ymm8, ymm14, ymm14, ymm8
	ADD rcx, 192
	VFMADDPD xmm9, xmm15, xmm15, xmm9
	SUB r8, 24
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD xmm12, [rcx]
	VFMADDPD xmm10, xmm5, xmm5, xmm10
	VMOVUPD xmm4, [byte rcx + 16]
	VFMADDPD ymm11, ymm3, ymm3, ymm11
	VMOVUPD ymm5, [byte rcx + 32]
	VMOVUPD xmm3, [byte rcx + 64]
	VFMADDPD xmm2, xmm12, xmm12, xmm2
	VMOVUPD xmm13, [byte rcx + 80]
	VFMADDPD xmm1, xmm4, xmm4, xmm1
	VMOVUPD ymm14, [byte rcx + 96]
	VFMADDPD ymm0, ymm5, ymm5, ymm0
	VMOVUPD xmm15, [dword rcx + 128]
	VFMADDPD xmm6, xmm3, xmm3, xmm6
	VMOVUPD xmm5, [dword rcx + 144]
	VFMADDPD xmm7, xmm13, xmm13, xmm7
	VMOVUPD ymm3, [dword rcx + 160]
	VFMADDPD ymm8, ymm14, ymm14, ymm8
	ADD rcx, 192
	VFMADDPD xmm9, xmm15, xmm15, xmm9
	SUB r8, 24
	JAE .process_batch
	.process_batch_epilogue:
	VFMADDPD xmm10, xmm5, xmm5, xmm10
	VFMADDPD ymm11, ymm3, ymm3, ymm11
	.batch_process_finish:
	ADD r8, 24
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm4, [rcx]
	VFMADDPD xmm2, xmm4, xmm4, xmm2
	ADD rcx, 8
	SUB r8, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD xmm2, xmm2, xmm1
	VADDPD ymm0, ymm0, ymm6
	VADDPD ymm7, ymm7, ymm8
	VADDPD xmm9, xmm9, xmm10
	VADDPD ymm2, ymm2, ymm0
	VADDPD ymm7, ymm7, ymm9
	VADDPD ymm2, ymm2, ymm7
	VADDPD ymm2, ymm2, ymm11
	VEXTRACTF128 xmm4, ymm2, 1
	VADDPD xmm2, xmm2, xmm4
	VUNPCKHPD xmm4, xmm2, xmm2
	VADDSD xmm2, xmm2, xmm4
	.return_ok:
	VMOVSD [rdx], xmm2
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm14, [dword rsp + 128]
	VMOVAPS xmm15, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepCore_SumSquares_V64f_S64f_Haswell
_yepCore_SumSquares_V64f_S64f_Haswell:
	.ENTRY:
	SUB rsp, 136
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	VMOVAPS [byte rsp + 112], xmm13
	TEST rcx, rcx
	JZ .return_null_pointer
	TEST rcx, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	VXORPD xmm2, xmm2, xmm2
	TEST r8, r8
	JZ .return_ok
	VXORPD xmm1, xmm1, xmm1
	VXORPD xmm0, xmm0, xmm0
	VXORPD xmm6, xmm6, xmm6
	VXORPD xmm7, xmm7, xmm7
	VXORPD xmm8, xmm8, xmm8
	VXORPD xmm9, xmm9, xmm9
	VXORPD xmm10, xmm10, xmm10
	TEST rcx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm11, [rcx]
	VFMADD231PD ymm2, ymm11, ymm11
	ADD rcx, 8
	SUB r8, 1
	JZ .reduce_batch
	TEST rcx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD ymm11, [rcx]
	VMOVUPD ymm4, [byte rcx + 32]
	VMOVUPD ymm5, [byte rcx + 64]
	VMOVUPD ymm3, [byte rcx + 96]
	VFMADD231PD ymm2, ymm11, ymm11
	VMOVUPD ymm12, [dword rcx + 128]
	VFMADD231PD ymm1, ymm4, ymm4
	VMOVUPD ymm13, [dword rcx + 160]
	VFMADD231PD ymm0, ymm5, ymm5
	VMOVUPD ymm5, [dword rcx + 192]
	VFMADD231PD ymm6, ymm3, ymm3
	VMOVUPD ymm3, [dword rcx + 224]
	VFMADD231PD ymm7, ymm12, ymm12
	ADD rcx, 256
	VFMADD231PD ymm8, ymm13, ymm13
	SUB r8, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD ymm11, [rcx]
	VFMADD231PD ymm9, ymm5, ymm5
	VMOVUPD ymm4, [byte rcx + 32]
	VFMADD231PD ymm10, ymm3, ymm3
	VMOVUPD ymm5, [byte rcx + 64]
	VMOVUPD ymm3, [byte rcx + 96]
	VFMADD231PD ymm2, ymm11, ymm11
	VMOVUPD ymm12, [dword rcx + 128]
	VFMADD231PD ymm1, ymm4, ymm4
	VMOVUPD ymm13, [dword rcx + 160]
	VFMADD231PD ymm0, ymm5, ymm5
	VMOVUPD ymm5, [dword rcx + 192]
	VFMADD231PD ymm6, ymm3, ymm3
	VMOVUPD ymm3, [dword rcx + 224]
	VFMADD231PD ymm7, ymm12, ymm12
	ADD rcx, 256
	VFMADD231PD ymm8, ymm13, ymm13
	SUB r8, 32
	JAE .process_batch
	.process_batch_epilogue:
	VFMADD231PD ymm9, ymm5, ymm5
	VFMADD231PD ymm10, ymm3, ymm3
	.batch_process_finish:
	ADD r8, 32
	JZ .reduce_batch
	.process_single:
	VMOVSD xmm4, [rcx]
	VFMADD231PD ymm2, ymm4, ymm4
	ADD rcx, 8
	SUB r8, 1
	JNZ .process_single
	.reduce_batch:
	VADDPD ymm2, ymm2, ymm1
	VADDPD ymm0, ymm0, ymm6
	VADDPD ymm7, ymm7, ymm8
	VADDPD ymm9, ymm9, ymm10
	VADDPD ymm2, ymm2, ymm0
	VADDPD ymm7, ymm7, ymm9
	VADDPD ymm2, ymm2, ymm7
	VEXTRACTF128 xmm4, ymm2, 1
	VADDPD xmm2, xmm2, xmm4
	VUNPCKHPD xmm4, xmm2, xmm2
	VADDSD xmm2, xmm2, xmm4
	.return_ok:
	VMOVSD [rdx], xmm2
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	ADD rsp, 136
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return
