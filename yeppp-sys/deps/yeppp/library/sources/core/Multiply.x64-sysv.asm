;                       Yeppp! library implementation
;                   This file is auto-generated by Peach-Py,
;        Portable Efficient Assembly Code-generator in Higher-level Python,
;                  part of the Yeppp! library infrastructure
; This file is part of Yeppp! library and licensed under the New BSD license.
; See LICENSE.txt for the full text of the license.

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Multiply_V16sV16s_V16s_Nehalem
_yepCore_Multiply_V16sV16s_V16s_Nehalem:
%else
section .text
global __yepCore_Multiply_V16sV16s_V16s_Nehalem
__yepCore_Multiply_V16sV16s_V16s_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm0, [rdi]
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm1, [rsi]
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm13, [byte rsi + 16]
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm10, [byte rsi + 32]
	MOVDQU xmm14, [byte rdi + 64]
	MOVDQU xmm3, [byte rsi + 48]
	MOVDQU xmm11, [byte rdi + 80]
	MOVDQU xmm6, [byte rsi + 64]
	PMULLW xmm0, xmm1
	MOVDQU xmm4, [byte rdi + 96]
	MOVDQU xmm7, [byte rsi + 80]
	PMULLW xmm8, xmm13
	MOVDQU xmm2, [byte rdi + 112]
	MOVDQU xmm5, [byte rsi + 96]
	PMULLW xmm9, xmm10
	MOVDQA [rdx], xmm0
	ADD rdi, 128
	MOVDQU xmm15, [byte rsi + 112]
	PMULLW xmm12, xmm3
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm0, [rdi]
	ADD rsi, 128
	PMULLW xmm14, xmm6
	MOVDQA [byte rdx + 32], xmm9
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm1, [rsi]
	PMULLW xmm11, xmm7
	MOVDQA [byte rdx + 48], xmm12
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm13, [byte rsi + 16]
	PMULLW xmm4, xmm5
	MOVDQA [byte rdx + 64], xmm14
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm10, [byte rsi + 32]
	PMULLW xmm2, xmm15
	MOVDQA [byte rdx + 80], xmm11
	MOVDQU xmm14, [byte rdi + 64]
	MOVDQU xmm3, [byte rsi + 48]
	MOVDQA [byte rdx + 96], xmm4
	MOVDQU xmm11, [byte rdi + 80]
	MOVDQU xmm6, [byte rsi + 64]
	PMULLW xmm0, xmm1
	MOVDQA [byte rdx + 112], xmm2
	MOVDQU xmm4, [byte rdi + 96]
	MOVDQU xmm7, [byte rsi + 80]
	PMULLW xmm8, xmm13
	ADD rdx, 128
	MOVDQU xmm2, [byte rdi + 112]
	MOVDQU xmm5, [byte rsi + 96]
	PMULLW xmm9, xmm10
	MOVDQA [rdx], xmm0
	ADD rdi, 128
	MOVDQU xmm15, [byte rsi + 112]
	PMULLW xmm12, xmm3
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	PMULLW xmm14, xmm6
	MOVDQA [byte rdx + 32], xmm9
	PMULLW xmm11, xmm7
	MOVDQA [byte rdx + 48], xmm12
	PMULLW xmm4, xmm5
	MOVDQA [byte rdx + 64], xmm14
	PMULLW xmm2, xmm15
	MOVDQA [byte rdx + 80], xmm11
	MOVDQA [byte rdx + 96], xmm4
	MOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Multiply_V16sV16s_V16s_SandyBridge
_yepCore_Multiply_V16sV16s_V16s_SandyBridge:
%else
section .text
global __yepCore_Multiply_V16sV16s_V16s_SandyBridge
__yepCore_Multiply_V16sV16s_V16s_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm0, [rdi]
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm1, [rsi]
	VMOVDQU xmm9, [byte rdi + 32]
	VMOVDQU xmm13, [byte rsi + 16]
	VMOVDQU xmm12, [byte rdi + 48]
	VMOVDQU xmm10, [byte rsi + 32]
	VMOVDQU xmm14, [byte rdi + 64]
	VMOVDQU xmm3, [byte rsi + 48]
	VMOVDQU xmm11, [byte rdi + 80]
	VMOVDQU xmm6, [byte rsi + 64]
	VPMULLW xmm0, xmm0, xmm1
	VMOVDQU xmm4, [byte rdi + 96]
	VMOVDQU xmm7, [byte rsi + 80]
	VPMULLW xmm8, xmm8, xmm13
	VMOVDQU xmm2, [byte rdi + 112]
	VMOVDQU xmm5, [byte rsi + 96]
	VPMULLW xmm9, xmm9, xmm10
	VMOVDQA [rdx], xmm0
	ADD rdi, 128
	VMOVDQU xmm15, [byte rsi + 112]
	VPMULLW xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm0, [rdi]
	ADD rsi, 128
	VPMULLW xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 32], xmm9
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm1, [rsi]
	VPMULLW xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 48], xmm12
	VMOVDQU xmm9, [byte rdi + 32]
	VMOVDQU xmm13, [byte rsi + 16]
	VPMULLW xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 64], xmm14
	VMOVDQU xmm12, [byte rdi + 48]
	VMOVDQU xmm10, [byte rsi + 32]
	VPMULLW xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 80], xmm11
	VMOVDQU xmm14, [byte rdi + 64]
	VMOVDQU xmm3, [byte rsi + 48]
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQU xmm11, [byte rdi + 80]
	VMOVDQU xmm6, [byte rsi + 64]
	VPMULLW xmm0, xmm0, xmm1
	VMOVDQA [byte rdx + 112], xmm2
	VMOVDQU xmm4, [byte rdi + 96]
	VMOVDQU xmm7, [byte rsi + 80]
	VPMULLW xmm8, xmm8, xmm13
	ADD rdx, 128
	VMOVDQU xmm2, [byte rdi + 112]
	VMOVDQU xmm5, [byte rsi + 96]
	VPMULLW xmm9, xmm9, xmm10
	VMOVDQA [rdx], xmm0
	ADD rdi, 128
	VMOVDQU xmm15, [byte rsi + 112]
	VPMULLW xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	VPMULLW xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 32], xmm9
	VPMULLW xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 48], xmm12
	VPMULLW xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 64], xmm14
	VPMULLW xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 80], xmm11
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Multiply_V16sV16s_V16s_Haswell
_yepCore_Multiply_V16sV16s_V16s_Haswell:
%else
section .text
global __yepCore_Multiply_V16sV16s_V16s_Haswell
__yepCore_Multiply_V16sV16s_V16s_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 1
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 128
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm0, [rdi]
	VMOVDQU ymm8, [byte rdi + 32]
	VMOVDQU ymm7, [rsi]
	VMOVDQU ymm9, [byte rdi + 64]
	VMOVDQU ymm5, [byte rsi + 32]
	VMOVDQU ymm1, [byte rdi + 96]
	VMOVDQU ymm3, [byte rsi + 64]
	VMOVDQU ymm14, [dword rdi + 128]
	VMOVDQU ymm2, [byte rsi + 96]
	VMOVDQU ymm12, [dword rdi + 160]
	VMOVDQU ymm15, [dword rsi + 128]
	VPMULLW ymm0, ymm0, ymm7
	VMOVDQU ymm11, [dword rdi + 192]
	VMOVDQU ymm10, [dword rsi + 160]
	VPMULLW ymm8, ymm8, ymm5
	VMOVDQU ymm13, [dword rdi + 224]
	VMOVDQU ymm4, [dword rsi + 192]
	VPMULLW ymm9, ymm9, ymm3
	VMOVDQA [rdx], ymm0
	ADD rdi, 256
	VMOVDQU ymm6, [dword rsi + 224]
	VPMULLW ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 128
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm0, [rdi]
	ADD rsi, 256
	VPMULLW ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 64], ymm9
	VMOVDQU ymm8, [byte rdi + 32]
	VMOVDQU ymm7, [rsi]
	VPMULLW ymm12, ymm12, ymm10
	VMOVDQA [byte rdx + 96], ymm1
	VMOVDQU ymm9, [byte rdi + 64]
	VMOVDQU ymm5, [byte rsi + 32]
	VPMULLW ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 128], ymm14
	VMOVDQU ymm1, [byte rdi + 96]
	VMOVDQU ymm3, [byte rsi + 64]
	VPMULLW ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 160], ymm12
	VMOVDQU ymm14, [dword rdi + 128]
	VMOVDQU ymm2, [byte rsi + 96]
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQU ymm12, [dword rdi + 160]
	VMOVDQU ymm15, [dword rsi + 128]
	VPMULLW ymm0, ymm0, ymm7
	VMOVDQA [dword rdx + 224], ymm13
	VMOVDQU ymm11, [dword rdi + 192]
	VMOVDQU ymm10, [dword rsi + 160]
	VPMULLW ymm8, ymm8, ymm5
	ADD rdx, 256
	VMOVDQU ymm13, [dword rdi + 224]
	VMOVDQU ymm4, [dword rsi + 192]
	VPMULLW ymm9, ymm9, ymm3
	VMOVDQA [rdx], ymm0
	ADD rdi, 256
	VMOVDQU ymm6, [dword rsi + 224]
	VPMULLW ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 128
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 256
	VPMULLW ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 64], ymm9
	VPMULLW ymm12, ymm12, ymm10
	VMOVDQA [byte rdx + 96], ymm1
	VPMULLW ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 128], ymm14
	VPMULLW ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 160], ymm12
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm13
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 128
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], ax
	ADD rdx, 2
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Multiply_V16sV16s_V32s_Nehalem
_yepCore_Multiply_V16sV16s_V32s_Nehalem:
%else
section .text
global __yepCore_Multiply_V16sV16s_V32s_Nehalem
__yepCore_Multiply_V16sV16s_V32s_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm4, [rdi]
	MOVDQU xmm3, [rsi]
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm13, [byte rsi + 16]
	MOVDQA xmm14, xmm4
	PMULLW xmm4, xmm3
	PMULHW xmm14, xmm3
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm10, [byte rsi + 32]
	MOVDQA xmm11, xmm8
	PMULLW xmm8, xmm13
	PMULHW xmm11, xmm13
	MOVDQA xmm5, xmm4
	PUNPCKLWD xmm4, xmm14
	PUNPCKHWD xmm5, xmm14
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm7, [byte rsi + 48]
	MOVDQA xmm2, xmm9
	PMULLW xmm9, xmm10
	PMULHW xmm2, xmm10
	MOVDQA xmm1, xmm8
	PUNPCKLWD xmm8, xmm11
	PUNPCKHWD xmm1, xmm11
	MOVDQA [rdx], xmm4
	MOVDQA [byte rdx + 16], xmm5
	ADD rdi, 64
	ADD rsi, 64
	MOVDQA xmm6, xmm12
	PMULLW xmm12, xmm7
	PMULHW xmm6, xmm7
	MOVDQA xmm7, xmm9
	PUNPCKLWD xmm9, xmm2
	PUNPCKHWD xmm7, xmm2
	MOVDQA [byte rdx + 32], xmm8
	MOVDQA [byte rdx + 48], xmm1
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm4, [rdi]
	MOVDQU xmm3, [rsi]
	MOVDQA xmm15, xmm12
	PUNPCKLWD xmm12, xmm6
	PUNPCKHWD xmm15, xmm6
	MOVDQA [byte rdx + 64], xmm9
	MOVDQA [byte rdx + 80], xmm7
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm13, [byte rsi + 16]
	MOVDQA xmm14, xmm4
	PMULLW xmm4, xmm3
	PMULHW xmm14, xmm3
	MOVDQA [byte rdx + 96], xmm12
	MOVDQA [byte rdx + 112], xmm15
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm10, [byte rsi + 32]
	MOVDQA xmm11, xmm8
	PMULLW xmm8, xmm13
	PMULHW xmm11, xmm13
	MOVDQA xmm5, xmm4
	PUNPCKLWD xmm4, xmm14
	PUNPCKHWD xmm5, xmm14
	ADD rdx, 128
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm7, [byte rsi + 48]
	MOVDQA xmm2, xmm9
	PMULLW xmm9, xmm10
	PMULHW xmm2, xmm10
	MOVDQA xmm1, xmm8
	PUNPCKLWD xmm8, xmm11
	PUNPCKHWD xmm1, xmm11
	MOVDQA [rdx], xmm4
	MOVDQA [byte rdx + 16], xmm5
	ADD rdi, 64
	ADD rsi, 64
	MOVDQA xmm6, xmm12
	PMULLW xmm12, xmm7
	PMULHW xmm6, xmm7
	MOVDQA xmm7, xmm9
	PUNPCKLWD xmm9, xmm2
	PUNPCKHWD xmm7, xmm2
	MOVDQA [byte rdx + 32], xmm8
	MOVDQA [byte rdx + 48], xmm1
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	MOVDQA xmm15, xmm12
	PUNPCKLWD xmm12, xmm6
	PUNPCKHWD xmm15, xmm6
	MOVDQA [byte rdx + 64], xmm9
	MOVDQA [byte rdx + 80], xmm7
	MOVDQA [byte rdx + 96], xmm12
	MOVDQA [byte rdx + 112], xmm15
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Bulldozer progbits alloc exec nowrite align=16
global _yepCore_Multiply_V16sV16s_V32s_Bulldozer
_yepCore_Multiply_V16sV16s_V32s_Bulldozer:
%else
section .text
global __yepCore_Multiply_V16sV16s_V32s_Bulldozer
__yepCore_Multiply_V16sV16s_V32s_Bulldozer:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 24
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm14, [rdi]
	VMOVDQU xmm15, [rsi]
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm9, [byte rsi + 16]
	VPMULLW xmm13, xmm14, xmm15
	VPMULHW xmm12, xmm14, xmm15
	VMOVDQU xmm11, [byte rdi + 32]
	VMOVDQU xmm7, [byte rsi + 32]
	VPMULLW xmm6, xmm8, xmm9
	VPMULHW xmm5, xmm8, xmm9
	VPUNPCKLWD xmm10, xmm13, xmm12
	VPUNPCKHWD xmm4, xmm13, xmm12
	VMOVDQA [rdx], xmm10
	VMOVDQA [byte rdx + 16], xmm4
	ADD rdi, 48
	ADD rsi, 48
	VPMULLW xmm10, xmm11, xmm7
	VPMULHW xmm11, xmm11, xmm7
	VPUNPCKLWD xmm7, xmm6, xmm5
	VPUNPCKHWD xmm6, xmm6, xmm5
	VMOVDQA [byte rdx + 32], xmm7
	VMOVDQA [byte rdx + 48], xmm6
	SUB rcx, 24
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm14, [rdi]
	VMOVDQU xmm15, [rsi]
	VPUNPCKLWD xmm7, xmm10, xmm11
	VPUNPCKHWD xmm10, xmm10, xmm11
	VMOVDQA [byte rdx + 64], xmm7
	VMOVDQA [byte rdx + 80], xmm10
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm9, [byte rsi + 16]
	VPMULLW xmm13, xmm14, xmm15
	VPMULHW xmm12, xmm14, xmm15
	ADD rdx, 96
	VMOVDQU xmm11, [byte rdi + 32]
	VMOVDQU xmm7, [byte rsi + 32]
	VPMULLW xmm6, xmm8, xmm9
	VPMULHW xmm5, xmm8, xmm9
	VPUNPCKLWD xmm10, xmm13, xmm12
	VPUNPCKHWD xmm4, xmm13, xmm12
	VMOVDQA [rdx], xmm10
	VMOVDQA [byte rdx + 16], xmm4
	ADD rdi, 48
	ADD rsi, 48
	VPMULLW xmm10, xmm11, xmm7
	VPMULHW xmm11, xmm11, xmm7
	VPUNPCKLWD xmm7, xmm6, xmm5
	VPUNPCKHWD xmm6, xmm6, xmm5
	VMOVDQA [byte rdx + 32], xmm7
	VMOVDQA [byte rdx + 48], xmm6
	SUB rcx, 24
	JAE .process_batch
	.process_batch_epilogue:
	VPUNPCKLWD xmm7, xmm10, xmm11
	VPUNPCKHWD xmm10, xmm10, xmm11
	VMOVDQA [byte rdx + 64], xmm7
	VMOVDQA [byte rdx + 80], xmm10
	ADD rdx, 96
	.batch_process_finish:
	ADD rcx, 24
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Multiply_V16sV16s_V32s_SandyBridge
_yepCore_Multiply_V16sV16s_V32s_SandyBridge:
%else
section .text
global __yepCore_Multiply_V16sV16s_V32s_SandyBridge
__yepCore_Multiply_V16sV16s_V32s_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 20
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXWD xmm2, [rdi]
	VPMOVSXWD xmm1, [rsi]
	VPMOVSXWD xmm8, [byte rdi + 8]
	VPMOVSXWD xmm11, [byte rsi + 8]
	VPMOVSXWD xmm9, [byte rdi + 16]
	VPMOVSXWD xmm4, [byte rsi + 16]
	VPMULLD xmm10, xmm2, xmm1
	VPMOVSXWD xmm12, [byte rdi + 24]
	VPMOVSXWD xmm3, [byte rsi + 24]
	VPMULLD xmm5, xmm8, xmm11
	VPMOVSXWD xmm14, [byte rdi + 32]
	VPMOVSXWD xmm13, [byte rsi + 32]
	VPMULLD xmm7, xmm9, xmm4
	VMOVDQA [rdx], xmm10
	ADD rdi, 40
	ADD rsi, 40
	VPMULLD xmm15, xmm12, xmm3
	VMOVDQA [byte rdx + 16], xmm5
	SUB rcx, 20
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXWD xmm2, [rdi]
	VPMOVSXWD xmm1, [rsi]
	VPMULLD xmm6, xmm14, xmm13
	VMOVDQA [byte rdx + 32], xmm7
	VPMOVSXWD xmm8, [byte rdi + 8]
	VPMOVSXWD xmm11, [byte rsi + 8]
	VMOVDQA [byte rdx + 48], xmm15
	VPMOVSXWD xmm9, [byte rdi + 16]
	VPMOVSXWD xmm4, [byte rsi + 16]
	VPMULLD xmm10, xmm2, xmm1
	VMOVDQA [byte rdx + 64], xmm6
	VPMOVSXWD xmm12, [byte rdi + 24]
	VPMOVSXWD xmm3, [byte rsi + 24]
	VPMULLD xmm5, xmm8, xmm11
	ADD rdx, 80
	VPMOVSXWD xmm14, [byte rdi + 32]
	VPMOVSXWD xmm13, [byte rsi + 32]
	VPMULLD xmm7, xmm9, xmm4
	VMOVDQA [rdx], xmm10
	ADD rdi, 40
	ADD rsi, 40
	VPMULLD xmm15, xmm12, xmm3
	VMOVDQA [byte rdx + 16], xmm5
	SUB rcx, 20
	JAE .process_batch
	.process_batch_epilogue:
	VPMULLD xmm6, xmm14, xmm13
	VMOVDQA [byte rdx + 32], xmm7
	VMOVDQA [byte rdx + 48], xmm15
	VMOVDQA [byte rdx + 64], xmm6
	ADD rdx, 80
	.batch_process_finish:
	ADD rcx, 20
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Multiply_V16sV16s_V32s_Haswell
_yepCore_Multiply_V16sV16s_V32s_Haswell:
%else
section .text
global __yepCore_Multiply_V16sV16s_V32s_Haswell
__yepCore_Multiply_V16sV16s_V32s_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 40
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVSXWD ymm2, [rdi]
	VPMOVSXWD ymm1, [rsi]
	VPMOVSXWD ymm8, [byte rdi + 16]
	VPMOVSXWD ymm12, [byte rsi + 16]
	VPMOVSXWD ymm9, [byte rdi + 32]
	VPMOVSXWD ymm13, [byte rsi + 32]
	VPMULLD ymm5, ymm2, ymm1
	VPMOVSXWD ymm3, [byte rdi + 48]
	VPMOVSXWD ymm7, [byte rsi + 48]
	VPMULLD ymm4, ymm8, ymm12
	VPMOVSXWD ymm14, [byte rdi + 64]
	VPMOVSXWD ymm6, [byte rsi + 64]
	VPMULLD ymm15, ymm9, ymm13
	VMOVDQA [rdx], ymm5
	ADD rdi, 80
	ADD rsi, 80
	VPMULLD ymm10, ymm3, ymm7
	VMOVDQA [byte rdx + 32], ymm4
	SUB rcx, 40
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVSXWD ymm2, [rdi]
	VPMOVSXWD ymm1, [rsi]
	VPMULLD ymm11, ymm14, ymm6
	VMOVDQA [byte rdx + 64], ymm15
	VPMOVSXWD ymm8, [byte rdi + 16]
	VPMOVSXWD ymm12, [byte rsi + 16]
	VMOVDQA [byte rdx + 96], ymm10
	VPMOVSXWD ymm9, [byte rdi + 32]
	VPMOVSXWD ymm13, [byte rsi + 32]
	VPMULLD ymm5, ymm2, ymm1
	VMOVDQA [dword rdx + 128], ymm11
	VPMOVSXWD ymm3, [byte rdi + 48]
	VPMOVSXWD ymm7, [byte rsi + 48]
	VPMULLD ymm4, ymm8, ymm12
	ADD rdx, 160
	VPMOVSXWD ymm14, [byte rdi + 64]
	VPMOVSXWD ymm6, [byte rsi + 64]
	VPMULLD ymm15, ymm9, ymm13
	VMOVDQA [rdx], ymm5
	ADD rdi, 80
	ADD rsi, 80
	VPMULLD ymm10, ymm3, ymm7
	VMOVDQA [byte rdx + 32], ymm4
	SUB rcx, 40
	JAE .process_batch
	.process_batch_epilogue:
	VPMULLD ymm11, ymm14, ymm6
	VMOVDQA [byte rdx + 64], ymm15
	VMOVDQA [byte rdx + 96], ymm10
	VMOVDQA [dword rdx + 128], ymm11
	ADD rdx, 160
	.batch_process_finish:
	ADD rcx, 40
	JZ .return_ok
	.process_single:
	MOVSX eax, word [rdi]
	ADD rdi, 2
	MOVSX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Multiply_V16uV16u_V32u_Nehalem
_yepCore_Multiply_V16uV16u_V32u_Nehalem:
%else
section .text
global __yepCore_Multiply_V16uV16u_V32u_Nehalem
__yepCore_Multiply_V16uV16u_V32u_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm4, [rdi]
	MOVDQU xmm3, [rsi]
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm13, [byte rsi + 16]
	MOVDQA xmm14, xmm4
	PMULLW xmm4, xmm3
	PMULHUW xmm14, xmm3
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm10, [byte rsi + 32]
	MOVDQA xmm11, xmm8
	PMULLW xmm8, xmm13
	PMULHUW xmm11, xmm13
	MOVDQA xmm5, xmm4
	PUNPCKLWD xmm4, xmm14
	PUNPCKHWD xmm5, xmm14
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm7, [byte rsi + 48]
	MOVDQA xmm2, xmm9
	PMULLW xmm9, xmm10
	PMULHUW xmm2, xmm10
	MOVDQA xmm1, xmm8
	PUNPCKLWD xmm8, xmm11
	PUNPCKHWD xmm1, xmm11
	MOVDQA [rdx], xmm4
	MOVDQA [byte rdx + 16], xmm5
	ADD rdi, 64
	ADD rsi, 64
	MOVDQA xmm6, xmm12
	PMULLW xmm12, xmm7
	PMULHUW xmm6, xmm7
	MOVDQA xmm7, xmm9
	PUNPCKLWD xmm9, xmm2
	PUNPCKHWD xmm7, xmm2
	MOVDQA [byte rdx + 32], xmm8
	MOVDQA [byte rdx + 48], xmm1
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm4, [rdi]
	MOVDQU xmm3, [rsi]
	MOVDQA xmm15, xmm12
	PUNPCKLWD xmm12, xmm6
	PUNPCKHWD xmm15, xmm6
	MOVDQA [byte rdx + 64], xmm9
	MOVDQA [byte rdx + 80], xmm7
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm13, [byte rsi + 16]
	MOVDQA xmm14, xmm4
	PMULLW xmm4, xmm3
	PMULHUW xmm14, xmm3
	MOVDQA [byte rdx + 96], xmm12
	MOVDQA [byte rdx + 112], xmm15
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm10, [byte rsi + 32]
	MOVDQA xmm11, xmm8
	PMULLW xmm8, xmm13
	PMULHUW xmm11, xmm13
	MOVDQA xmm5, xmm4
	PUNPCKLWD xmm4, xmm14
	PUNPCKHWD xmm5, xmm14
	ADD rdx, 128
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm7, [byte rsi + 48]
	MOVDQA xmm2, xmm9
	PMULLW xmm9, xmm10
	PMULHUW xmm2, xmm10
	MOVDQA xmm1, xmm8
	PUNPCKLWD xmm8, xmm11
	PUNPCKHWD xmm1, xmm11
	MOVDQA [rdx], xmm4
	MOVDQA [byte rdx + 16], xmm5
	ADD rdi, 64
	ADD rsi, 64
	MOVDQA xmm6, xmm12
	PMULLW xmm12, xmm7
	PMULHUW xmm6, xmm7
	MOVDQA xmm7, xmm9
	PUNPCKLWD xmm9, xmm2
	PUNPCKHWD xmm7, xmm2
	MOVDQA [byte rdx + 32], xmm8
	MOVDQA [byte rdx + 48], xmm1
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	MOVDQA xmm15, xmm12
	PUNPCKLWD xmm12, xmm6
	PUNPCKHWD xmm15, xmm6
	MOVDQA [byte rdx + 64], xmm9
	MOVDQA [byte rdx + 80], xmm7
	MOVDQA [byte rdx + 96], xmm12
	MOVDQA [byte rdx + 112], xmm15
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Bulldozer progbits alloc exec nowrite align=16
global _yepCore_Multiply_V16uV16u_V32u_Bulldozer
_yepCore_Multiply_V16uV16u_V32u_Bulldozer:
%else
section .text
global __yepCore_Multiply_V16uV16u_V32u_Bulldozer
__yepCore_Multiply_V16uV16u_V32u_Bulldozer:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 24
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm14, [rdi]
	VMOVDQU xmm15, [rsi]
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm9, [byte rsi + 16]
	VPMULLW xmm13, xmm14, xmm15
	VPMULHUW xmm12, xmm14, xmm15
	VMOVDQU xmm11, [byte rdi + 32]
	VMOVDQU xmm7, [byte rsi + 32]
	VPMULLW xmm6, xmm8, xmm9
	VPMULHUW xmm5, xmm8, xmm9
	VPUNPCKLWD xmm10, xmm13, xmm12
	VPUNPCKHWD xmm4, xmm13, xmm12
	VMOVDQA [rdx], xmm10
	VMOVDQA [byte rdx + 16], xmm4
	ADD rdi, 48
	ADD rsi, 48
	VPMULLW xmm10, xmm11, xmm7
	VPMULHUW xmm11, xmm11, xmm7
	VPUNPCKLWD xmm7, xmm6, xmm5
	VPUNPCKHWD xmm6, xmm6, xmm5
	VMOVDQA [byte rdx + 32], xmm7
	VMOVDQA [byte rdx + 48], xmm6
	SUB rcx, 24
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm14, [rdi]
	VMOVDQU xmm15, [rsi]
	VPUNPCKLWD xmm7, xmm10, xmm11
	VPUNPCKHWD xmm10, xmm10, xmm11
	VMOVDQA [byte rdx + 64], xmm7
	VMOVDQA [byte rdx + 80], xmm10
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm9, [byte rsi + 16]
	VPMULLW xmm13, xmm14, xmm15
	VPMULHUW xmm12, xmm14, xmm15
	ADD rdx, 96
	VMOVDQU xmm11, [byte rdi + 32]
	VMOVDQU xmm7, [byte rsi + 32]
	VPMULLW xmm6, xmm8, xmm9
	VPMULHUW xmm5, xmm8, xmm9
	VPUNPCKLWD xmm10, xmm13, xmm12
	VPUNPCKHWD xmm4, xmm13, xmm12
	VMOVDQA [rdx], xmm10
	VMOVDQA [byte rdx + 16], xmm4
	ADD rdi, 48
	ADD rsi, 48
	VPMULLW xmm10, xmm11, xmm7
	VPMULHUW xmm11, xmm11, xmm7
	VPUNPCKLWD xmm7, xmm6, xmm5
	VPUNPCKHWD xmm6, xmm6, xmm5
	VMOVDQA [byte rdx + 32], xmm7
	VMOVDQA [byte rdx + 48], xmm6
	SUB rcx, 24
	JAE .process_batch
	.process_batch_epilogue:
	VPUNPCKLWD xmm7, xmm10, xmm11
	VPUNPCKHWD xmm10, xmm10, xmm11
	VMOVDQA [byte rdx + 64], xmm7
	VMOVDQA [byte rdx + 80], xmm10
	ADD rdx, 96
	.batch_process_finish:
	ADD rcx, 24
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Multiply_V16uV16u_V32u_SandyBridge
_yepCore_Multiply_V16uV16u_V32u_SandyBridge:
%else
section .text
global __yepCore_Multiply_V16uV16u_V32u_SandyBridge
__yepCore_Multiply_V16uV16u_V32u_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 20
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXWD xmm2, [rdi]
	VPMOVZXWD xmm1, [rsi]
	VPMOVZXWD xmm8, [byte rdi + 8]
	VPMOVZXWD xmm11, [byte rsi + 8]
	VPMOVZXWD xmm9, [byte rdi + 16]
	VPMOVZXWD xmm4, [byte rsi + 16]
	VPMULLD xmm10, xmm2, xmm1
	VPMOVZXWD xmm12, [byte rdi + 24]
	VPMOVZXWD xmm3, [byte rsi + 24]
	VPMULLD xmm5, xmm8, xmm11
	VPMOVZXWD xmm14, [byte rdi + 32]
	VPMOVZXWD xmm13, [byte rsi + 32]
	VPMULLD xmm7, xmm9, xmm4
	VMOVDQA [rdx], xmm10
	ADD rdi, 40
	ADD rsi, 40
	VPMULLD xmm15, xmm12, xmm3
	VMOVDQA [byte rdx + 16], xmm5
	SUB rcx, 20
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXWD xmm2, [rdi]
	VPMOVZXWD xmm1, [rsi]
	VPMULLD xmm6, xmm14, xmm13
	VMOVDQA [byte rdx + 32], xmm7
	VPMOVZXWD xmm8, [byte rdi + 8]
	VPMOVZXWD xmm11, [byte rsi + 8]
	VMOVDQA [byte rdx + 48], xmm15
	VPMOVZXWD xmm9, [byte rdi + 16]
	VPMOVZXWD xmm4, [byte rsi + 16]
	VPMULLD xmm10, xmm2, xmm1
	VMOVDQA [byte rdx + 64], xmm6
	VPMOVZXWD xmm12, [byte rdi + 24]
	VPMOVZXWD xmm3, [byte rsi + 24]
	VPMULLD xmm5, xmm8, xmm11
	ADD rdx, 80
	VPMOVZXWD xmm14, [byte rdi + 32]
	VPMOVZXWD xmm13, [byte rsi + 32]
	VPMULLD xmm7, xmm9, xmm4
	VMOVDQA [rdx], xmm10
	ADD rdi, 40
	ADD rsi, 40
	VPMULLD xmm15, xmm12, xmm3
	VMOVDQA [byte rdx + 16], xmm5
	SUB rcx, 20
	JAE .process_batch
	.process_batch_epilogue:
	VPMULLD xmm6, xmm14, xmm13
	VMOVDQA [byte rdx + 32], xmm7
	VMOVDQA [byte rdx + 48], xmm15
	VMOVDQA [byte rdx + 64], xmm6
	ADD rdx, 80
	.batch_process_finish:
	ADD rcx, 20
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Multiply_V16uV16u_V32u_Haswell
_yepCore_Multiply_V16uV16u_V32u_Haswell:
%else
section .text
global __yepCore_Multiply_V16uV16u_V32u_Haswell
__yepCore_Multiply_V16uV16u_V32u_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 1
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 1
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 40
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXWD ymm2, [rdi]
	VPMOVZXWD ymm1, [rsi]
	VPMOVZXWD ymm8, [byte rdi + 16]
	VPMOVZXWD ymm12, [byte rsi + 16]
	VPMOVZXWD ymm9, [byte rdi + 32]
	VPMOVZXWD ymm13, [byte rsi + 32]
	VPMULLD ymm5, ymm2, ymm1
	VPMOVZXWD ymm3, [byte rdi + 48]
	VPMOVZXWD ymm7, [byte rsi + 48]
	VPMULLD ymm4, ymm8, ymm12
	VPMOVZXWD ymm14, [byte rdi + 64]
	VPMOVZXWD ymm6, [byte rsi + 64]
	VPMULLD ymm15, ymm9, ymm13
	VMOVDQA [rdx], ymm5
	ADD rdi, 80
	ADD rsi, 80
	VPMULLD ymm10, ymm3, ymm7
	VMOVDQA [byte rdx + 32], ymm4
	SUB rcx, 40
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXWD ymm2, [rdi]
	VPMOVZXWD ymm1, [rsi]
	VPMULLD ymm11, ymm14, ymm6
	VMOVDQA [byte rdx + 64], ymm15
	VPMOVZXWD ymm8, [byte rdi + 16]
	VPMOVZXWD ymm12, [byte rsi + 16]
	VMOVDQA [byte rdx + 96], ymm10
	VPMOVZXWD ymm9, [byte rdi + 32]
	VPMOVZXWD ymm13, [byte rsi + 32]
	VPMULLD ymm5, ymm2, ymm1
	VMOVDQA [dword rdx + 128], ymm11
	VPMOVZXWD ymm3, [byte rdi + 48]
	VPMOVZXWD ymm7, [byte rsi + 48]
	VPMULLD ymm4, ymm8, ymm12
	ADD rdx, 160
	VPMOVZXWD ymm14, [byte rdi + 64]
	VPMOVZXWD ymm6, [byte rsi + 64]
	VPMULLD ymm15, ymm9, ymm13
	VMOVDQA [rdx], ymm5
	ADD rdi, 80
	ADD rsi, 80
	VPMULLD ymm10, ymm3, ymm7
	VMOVDQA [byte rdx + 32], ymm4
	SUB rcx, 40
	JAE .process_batch
	.process_batch_epilogue:
	VPMULLD ymm11, ymm14, ymm6
	VMOVDQA [byte rdx + 64], ymm15
	VMOVDQA [byte rdx + 96], ymm10
	VMOVDQA [dword rdx + 128], ymm11
	ADD rdx, 160
	.batch_process_finish:
	ADD rcx, 40
	JZ .return_ok
	.process_single:
	MOVZX eax, word [rdi]
	ADD rdi, 2
	MOVZX r10d, word [rsi]
	ADD rsi, 2
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Multiply_V32sV32s_V32s_Nehalem
_yepCore_Multiply_V32sV32s_V32s_Nehalem:
%else
section .text
global __yepCore_Multiply_V32sV32s_V32s_Nehalem
__yepCore_Multiply_V32sV32s_V32s_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	MOVDQU xmm0, [rdi]
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm1, [rsi]
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm13, [byte rsi + 16]
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm10, [byte rsi + 32]
	MOVDQU xmm14, [byte rdi + 64]
	MOVDQU xmm3, [byte rsi + 48]
	MOVDQU xmm11, [byte rdi + 80]
	MOVDQU xmm6, [byte rsi + 64]
	PMULLD xmm0, xmm1
	MOVDQU xmm4, [byte rdi + 96]
	MOVDQU xmm7, [byte rsi + 80]
	PMULLD xmm8, xmm13
	MOVDQU xmm2, [byte rdi + 112]
	MOVDQU xmm5, [byte rsi + 96]
	PMULLD xmm9, xmm10
	MOVDQA [rdx], xmm0
	ADD rdi, 128
	MOVDQU xmm15, [byte rsi + 112]
	PMULLD xmm12, xmm3
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVDQU xmm0, [rdi]
	ADD rsi, 128
	PMULLD xmm14, xmm6
	MOVDQA [byte rdx + 32], xmm9
	MOVDQU xmm8, [byte rdi + 16]
	MOVDQU xmm1, [rsi]
	PMULLD xmm11, xmm7
	MOVDQA [byte rdx + 48], xmm12
	MOVDQU xmm9, [byte rdi + 32]
	MOVDQU xmm13, [byte rsi + 16]
	PMULLD xmm4, xmm5
	MOVDQA [byte rdx + 64], xmm14
	MOVDQU xmm12, [byte rdi + 48]
	MOVDQU xmm10, [byte rsi + 32]
	PMULLD xmm2, xmm15
	MOVDQA [byte rdx + 80], xmm11
	MOVDQU xmm14, [byte rdi + 64]
	MOVDQU xmm3, [byte rsi + 48]
	MOVDQA [byte rdx + 96], xmm4
	MOVDQU xmm11, [byte rdi + 80]
	MOVDQU xmm6, [byte rsi + 64]
	PMULLD xmm0, xmm1
	MOVDQA [byte rdx + 112], xmm2
	MOVDQU xmm4, [byte rdi + 96]
	MOVDQU xmm7, [byte rsi + 80]
	PMULLD xmm8, xmm13
	ADD rdx, 128
	MOVDQU xmm2, [byte rdi + 112]
	MOVDQU xmm5, [byte rsi + 96]
	PMULLD xmm9, xmm10
	MOVDQA [rdx], xmm0
	ADD rdi, 128
	MOVDQU xmm15, [byte rsi + 112]
	PMULLD xmm12, xmm3
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	PMULLD xmm14, xmm6
	MOVDQA [byte rdx + 32], xmm9
	PMULLD xmm11, xmm7
	MOVDQA [byte rdx + 48], xmm12
	PMULLD xmm4, xmm5
	MOVDQA [byte rdx + 64], xmm14
	PMULLD xmm2, xmm15
	MOVDQA [byte rdx + 80], xmm11
	MOVDQA [byte rdx + 96], xmm4
	MOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Multiply_V32sV32s_V32s_SandyBridge
_yepCore_Multiply_V32sV32s_V32s_SandyBridge:
%else
section .text
global __yepCore_Multiply_V32sV32s_V32s_SandyBridge
__yepCore_Multiply_V32sV32s_V32s_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU xmm0, [rdi]
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm1, [rsi]
	VMOVDQU xmm9, [byte rdi + 32]
	VMOVDQU xmm13, [byte rsi + 16]
	VMOVDQU xmm12, [byte rdi + 48]
	VMOVDQU xmm10, [byte rsi + 32]
	VMOVDQU xmm14, [byte rdi + 64]
	VMOVDQU xmm3, [byte rsi + 48]
	VMOVDQU xmm11, [byte rdi + 80]
	VMOVDQU xmm6, [byte rsi + 64]
	VPMULLD xmm0, xmm0, xmm1
	VMOVDQU xmm4, [byte rdi + 96]
	VMOVDQU xmm7, [byte rsi + 80]
	VPMULLD xmm8, xmm8, xmm13
	VMOVDQU xmm2, [byte rdi + 112]
	VMOVDQU xmm5, [byte rsi + 96]
	VPMULLD xmm9, xmm9, xmm10
	VMOVDQA [rdx], xmm0
	ADD rdi, 128
	VMOVDQU xmm15, [byte rsi + 112]
	VPMULLD xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU xmm0, [rdi]
	ADD rsi, 128
	VPMULLD xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 32], xmm9
	VMOVDQU xmm8, [byte rdi + 16]
	VMOVDQU xmm1, [rsi]
	VPMULLD xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 48], xmm12
	VMOVDQU xmm9, [byte rdi + 32]
	VMOVDQU xmm13, [byte rsi + 16]
	VPMULLD xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 64], xmm14
	VMOVDQU xmm12, [byte rdi + 48]
	VMOVDQU xmm10, [byte rsi + 32]
	VPMULLD xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 80], xmm11
	VMOVDQU xmm14, [byte rdi + 64]
	VMOVDQU xmm3, [byte rsi + 48]
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQU xmm11, [byte rdi + 80]
	VMOVDQU xmm6, [byte rsi + 64]
	VPMULLD xmm0, xmm0, xmm1
	VMOVDQA [byte rdx + 112], xmm2
	VMOVDQU xmm4, [byte rdi + 96]
	VMOVDQU xmm7, [byte rsi + 80]
	VPMULLD xmm8, xmm8, xmm13
	ADD rdx, 128
	VMOVDQU xmm2, [byte rdi + 112]
	VMOVDQU xmm5, [byte rsi + 96]
	VPMULLD xmm9, xmm9, xmm10
	VMOVDQA [rdx], xmm0
	ADD rdi, 128
	VMOVDQU xmm15, [byte rsi + 112]
	VPMULLD xmm12, xmm12, xmm3
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 128
	VPMULLD xmm14, xmm14, xmm6
	VMOVDQA [byte rdx + 32], xmm9
	VPMULLD xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 48], xmm12
	VPMULLD xmm4, xmm4, xmm5
	VMOVDQA [byte rdx + 64], xmm14
	VPMULLD xmm2, xmm2, xmm15
	VMOVDQA [byte rdx + 80], xmm11
	VMOVDQA [byte rdx + 96], xmm4
	VMOVDQA [byte rdx + 112], xmm2
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Multiply_V32sV32s_V32s_Haswell
_yepCore_Multiply_V32sV32s_V32s_Haswell:
%else
section .text
global __yepCore_Multiply_V32sV32s_V32s_Haswell
__yepCore_Multiply_V32sV32s_V32s_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 64
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVDQU ymm0, [rdi]
	VMOVDQU ymm8, [byte rdi + 32]
	VMOVDQU ymm7, [rsi]
	VMOVDQU ymm9, [byte rdi + 64]
	VMOVDQU ymm5, [byte rsi + 32]
	VMOVDQU ymm1, [byte rdi + 96]
	VMOVDQU ymm3, [byte rsi + 64]
	VMOVDQU ymm14, [dword rdi + 128]
	VMOVDQU ymm2, [byte rsi + 96]
	VMOVDQU ymm12, [dword rdi + 160]
	VMOVDQU ymm15, [dword rsi + 128]
	VPMULLD ymm0, ymm0, ymm7
	VMOVDQU ymm11, [dword rdi + 192]
	VMOVDQU ymm10, [dword rsi + 160]
	VPMULLD ymm8, ymm8, ymm5
	VMOVDQU ymm13, [dword rdi + 224]
	VMOVDQU ymm4, [dword rsi + 192]
	VPMULLD ymm9, ymm9, ymm3
	VMOVDQA [rdx], ymm0
	ADD rdi, 256
	VMOVDQU ymm6, [dword rsi + 224]
	VPMULLD ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 64
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVDQU ymm0, [rdi]
	ADD rsi, 256
	VPMULLD ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 64], ymm9
	VMOVDQU ymm8, [byte rdi + 32]
	VMOVDQU ymm7, [rsi]
	VPMULLD ymm12, ymm12, ymm10
	VMOVDQA [byte rdx + 96], ymm1
	VMOVDQU ymm9, [byte rdi + 64]
	VMOVDQU ymm5, [byte rsi + 32]
	VPMULLD ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 128], ymm14
	VMOVDQU ymm1, [byte rdi + 96]
	VMOVDQU ymm3, [byte rsi + 64]
	VPMULLD ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 160], ymm12
	VMOVDQU ymm14, [dword rdi + 128]
	VMOVDQU ymm2, [byte rsi + 96]
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQU ymm12, [dword rdi + 160]
	VMOVDQU ymm15, [dword rsi + 128]
	VPMULLD ymm0, ymm0, ymm7
	VMOVDQA [dword rdx + 224], ymm13
	VMOVDQU ymm11, [dword rdi + 192]
	VMOVDQU ymm10, [dword rsi + 160]
	VPMULLD ymm8, ymm8, ymm5
	ADD rdx, 256
	VMOVDQU ymm13, [dword rdi + 224]
	VMOVDQU ymm4, [dword rsi + 192]
	VPMULLD ymm9, ymm9, ymm3
	VMOVDQA [rdx], ymm0
	ADD rdi, 256
	VMOVDQU ymm6, [dword rsi + 224]
	VPMULLD ymm1, ymm1, ymm2
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 64
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 256
	VPMULLD ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 64], ymm9
	VPMULLD ymm12, ymm12, ymm10
	VMOVDQA [byte rdx + 96], ymm1
	VPMULLD ymm11, ymm11, ymm4
	VMOVDQA [dword rdx + 128], ymm14
	VPMULLD ymm13, ymm13, ymm6
	VMOVDQA [dword rdx + 160], ymm12
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm13
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 64
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	IMUL eax, r10d
	MOV [rdx], eax
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Multiply_V32sV32s_V64s_Nehalem
_yepCore_Multiply_V32sV32s_V64s_Nehalem:
%else
section .text
global __yepCore_Multiply_V32sV32s_V64s_Nehalem
__yepCore_Multiply_V32sV32s_V64s_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	MOVSX r10, dword [rsi]
	ADD rsi, 4
	IMUL rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXDQ xmm1, [rdi]
	PMOVZXDQ xmm0, [rsi]
	PMOVZXDQ xmm8, [byte rdi + 8]
	PMOVZXDQ xmm13, [byte rsi + 8]
	PMOVZXDQ xmm9, [byte rdi + 16]
	PMOVZXDQ xmm10, [byte rsi + 16]
	PMOVZXDQ xmm12, [byte rdi + 24]
	PMOVZXDQ xmm4, [byte rsi + 24]
	PMOVZXDQ xmm14, [byte rdi + 32]
	PMOVZXDQ xmm2, [byte rsi + 32]
	PMOVZXDQ xmm11, [byte rdi + 40]
	PMOVZXDQ xmm7, [byte rsi + 40]
	PMULDQ xmm1, xmm0
	PMOVZXDQ xmm5, [byte rdi + 48]
	PMOVZXDQ xmm6, [byte rsi + 48]
	PMULDQ xmm8, xmm13
	PMOVZXDQ xmm3, [byte rdi + 56]
	PMOVZXDQ xmm15, [byte rsi + 56]
	PMULDQ xmm9, xmm10
	MOVDQA [rdx], xmm1
	ADD rdi, 64
	ADD rsi, 64
	PMULDQ xmm12, xmm4
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXDQ xmm1, [rdi]
	PMOVZXDQ xmm0, [rsi]
	PMULDQ xmm14, xmm2
	MOVDQA [byte rdx + 32], xmm9
	PMOVZXDQ xmm8, [byte rdi + 8]
	PMOVZXDQ xmm13, [byte rsi + 8]
	PMULDQ xmm11, xmm7
	MOVDQA [byte rdx + 48], xmm12
	PMOVZXDQ xmm9, [byte rdi + 16]
	PMOVZXDQ xmm10, [byte rsi + 16]
	PMULDQ xmm5, xmm6
	MOVDQA [byte rdx + 64], xmm14
	PMOVZXDQ xmm12, [byte rdi + 24]
	PMOVZXDQ xmm4, [byte rsi + 24]
	PMULDQ xmm3, xmm15
	MOVDQA [byte rdx + 80], xmm11
	PMOVZXDQ xmm14, [byte rdi + 32]
	PMOVZXDQ xmm2, [byte rsi + 32]
	MOVDQA [byte rdx + 96], xmm5
	PMOVZXDQ xmm11, [byte rdi + 40]
	PMOVZXDQ xmm7, [byte rsi + 40]
	PMULDQ xmm1, xmm0
	MOVDQA [byte rdx + 112], xmm3
	PMOVZXDQ xmm5, [byte rdi + 48]
	PMOVZXDQ xmm6, [byte rsi + 48]
	PMULDQ xmm8, xmm13
	ADD rdx, 128
	PMOVZXDQ xmm3, [byte rdi + 56]
	PMOVZXDQ xmm15, [byte rsi + 56]
	PMULDQ xmm9, xmm10
	MOVDQA [rdx], xmm1
	ADD rdi, 64
	ADD rsi, 64
	PMULDQ xmm12, xmm4
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	PMULDQ xmm14, xmm2
	MOVDQA [byte rdx + 32], xmm9
	PMULDQ xmm11, xmm7
	MOVDQA [byte rdx + 48], xmm12
	PMULDQ xmm5, xmm6
	MOVDQA [byte rdx + 64], xmm14
	PMULDQ xmm3, xmm15
	MOVDQA [byte rdx + 80], xmm11
	MOVDQA [byte rdx + 96], xmm5
	MOVDQA [byte rdx + 112], xmm3
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	MOVSX r10, dword [rsi]
	ADD rsi, 4
	IMUL rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Multiply_V32sV32s_V64s_SandyBridge
_yepCore_Multiply_V32sV32s_V64s_SandyBridge:
%else
section .text
global __yepCore_Multiply_V32sV32s_V64s_SandyBridge
__yepCore_Multiply_V32sV32s_V64s_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	MOVSX r10, dword [rsi]
	ADD rsi, 4
	IMUL rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ xmm1, [rdi]
	VPMOVZXDQ xmm0, [rsi]
	VPMOVZXDQ xmm8, [byte rdi + 8]
	VPMOVZXDQ xmm13, [byte rsi + 8]
	VPMOVZXDQ xmm9, [byte rdi + 16]
	VPMOVZXDQ xmm10, [byte rsi + 16]
	VPMOVZXDQ xmm12, [byte rdi + 24]
	VPMOVZXDQ xmm4, [byte rsi + 24]
	VPMOVZXDQ xmm14, [byte rdi + 32]
	VPMOVZXDQ xmm2, [byte rsi + 32]
	VPMULDQ xmm1, xmm1, xmm0
	VPMOVZXDQ xmm11, [byte rdi + 40]
	VPMOVZXDQ xmm7, [byte rsi + 40]
	VPMULDQ xmm8, xmm8, xmm13
	VPMOVZXDQ xmm5, [byte rdi + 48]
	VPMOVZXDQ xmm6, [byte rsi + 48]
	VPMULDQ xmm9, xmm9, xmm10
	VPMOVZXDQ xmm3, [byte rdi + 56]
	VPMOVZXDQ xmm15, [byte rsi + 56]
	VPMULDQ xmm12, xmm12, xmm4
	VMOVDQA [rdx], xmm1
	ADD rdi, 64
	ADD rsi, 64
	VPMULDQ xmm14, xmm14, xmm2
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ xmm1, [rdi]
	VPMOVZXDQ xmm0, [rsi]
	VPMULDQ xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 32], xmm9
	VPMOVZXDQ xmm8, [byte rdi + 8]
	VPMOVZXDQ xmm13, [byte rsi + 8]
	VPMULDQ xmm5, xmm5, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPMOVZXDQ xmm9, [byte rdi + 16]
	VPMOVZXDQ xmm10, [byte rsi + 16]
	VPMULDQ xmm3, xmm3, xmm15
	VMOVDQA [byte rdx + 64], xmm14
	VPMOVZXDQ xmm12, [byte rdi + 24]
	VPMOVZXDQ xmm4, [byte rsi + 24]
	VMOVDQA [byte rdx + 80], xmm11
	VPMOVZXDQ xmm14, [byte rdi + 32]
	VPMOVZXDQ xmm2, [byte rsi + 32]
	VPMULDQ xmm1, xmm1, xmm0
	VMOVDQA [byte rdx + 96], xmm5
	VPMOVZXDQ xmm11, [byte rdi + 40]
	VPMOVZXDQ xmm7, [byte rsi + 40]
	VPMULDQ xmm8, xmm8, xmm13
	VMOVDQA [byte rdx + 112], xmm3
	VPMOVZXDQ xmm5, [byte rdi + 48]
	VPMOVZXDQ xmm6, [byte rsi + 48]
	VPMULDQ xmm9, xmm9, xmm10
	ADD rdx, 128
	VPMOVZXDQ xmm3, [byte rdi + 56]
	VPMOVZXDQ xmm15, [byte rsi + 56]
	VPMULDQ xmm12, xmm12, xmm4
	VMOVDQA [rdx], xmm1
	ADD rdi, 64
	ADD rsi, 64
	VPMULDQ xmm14, xmm14, xmm2
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	VPMULDQ xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 32], xmm9
	VPMULDQ xmm5, xmm5, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPMULDQ xmm3, xmm3, xmm15
	VMOVDQA [byte rdx + 64], xmm14
	VMOVDQA [byte rdx + 80], xmm11
	VMOVDQA [byte rdx + 96], xmm5
	VMOVDQA [byte rdx + 112], xmm3
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	MOVSX r10, dword [rsi]
	ADD rsi, 4
	IMUL rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Multiply_V32sV32s_V64s_Haswell
_yepCore_Multiply_V32sV32s_V64s_Haswell:
%else
section .text
global __yepCore_Multiply_V32sV32s_V64s_Haswell
__yepCore_Multiply_V32sV32s_V64s_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	MOVSX r10, dword [rsi]
	ADD rsi, 4
	IMUL rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ ymm1, [rdi]
	VPMOVZXDQ ymm0, [rsi]
	VPMOVZXDQ ymm8, [byte rdi + 16]
	VPMOVZXDQ ymm6, [byte rsi + 16]
	VPMOVZXDQ ymm9, [byte rdi + 32]
	VPMOVZXDQ ymm4, [byte rsi + 32]
	VPMOVZXDQ ymm2, [byte rdi + 48]
	VPMOVZXDQ ymm3, [byte rsi + 48]
	VPMOVZXDQ ymm14, [byte rdi + 64]
	VPMOVZXDQ ymm15, [byte rsi + 64]
	VPMULDQ ymm1, ymm1, ymm0
	VPMOVZXDQ ymm12, [byte rdi + 80]
	VPMOVZXDQ ymm10, [byte rsi + 80]
	VPMULDQ ymm8, ymm8, ymm6
	VPMOVZXDQ ymm11, [byte rdi + 96]
	VPMOVZXDQ ymm5, [byte rsi + 96]
	VPMULDQ ymm9, ymm9, ymm4
	VPMOVZXDQ ymm13, [byte rdi + 112]
	VPMOVZXDQ ymm7, [byte rsi + 112]
	VPMULDQ ymm2, ymm2, ymm3
	VMOVDQA [rdx], ymm1
	ADD rdi, 128
	ADD rsi, 128
	VPMULDQ ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ ymm1, [rdi]
	VPMOVZXDQ ymm0, [rsi]
	VPMULDQ ymm12, ymm12, ymm10
	VMOVDQA [byte rdx + 64], ymm9
	VPMOVZXDQ ymm8, [byte rdi + 16]
	VPMOVZXDQ ymm6, [byte rsi + 16]
	VPMULDQ ymm11, ymm11, ymm5
	VMOVDQA [byte rdx + 96], ymm2
	VPMOVZXDQ ymm9, [byte rdi + 32]
	VPMOVZXDQ ymm4, [byte rsi + 32]
	VPMULDQ ymm13, ymm13, ymm7
	VMOVDQA [dword rdx + 128], ymm14
	VPMOVZXDQ ymm2, [byte rdi + 48]
	VPMOVZXDQ ymm3, [byte rsi + 48]
	VMOVDQA [dword rdx + 160], ymm12
	VPMOVZXDQ ymm14, [byte rdi + 64]
	VPMOVZXDQ ymm15, [byte rsi + 64]
	VPMULDQ ymm1, ymm1, ymm0
	VMOVDQA [dword rdx + 192], ymm11
	VPMOVZXDQ ymm12, [byte rdi + 80]
	VPMOVZXDQ ymm10, [byte rsi + 80]
	VPMULDQ ymm8, ymm8, ymm6
	VMOVDQA [dword rdx + 224], ymm13
	VPMOVZXDQ ymm11, [byte rdi + 96]
	VPMOVZXDQ ymm5, [byte rsi + 96]
	VPMULDQ ymm9, ymm9, ymm4
	ADD rdx, 256
	VPMOVZXDQ ymm13, [byte rdi + 112]
	VPMOVZXDQ ymm7, [byte rsi + 112]
	VPMULDQ ymm2, ymm2, ymm3
	VMOVDQA [rdx], ymm1
	ADD rdi, 128
	ADD rsi, 128
	VPMULDQ ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	VPMULDQ ymm12, ymm12, ymm10
	VMOVDQA [byte rdx + 64], ymm9
	VPMULDQ ymm11, ymm11, ymm5
	VMOVDQA [byte rdx + 96], ymm2
	VPMULDQ ymm13, ymm13, ymm7
	VMOVDQA [dword rdx + 128], ymm14
	VMOVDQA [dword rdx + 160], ymm12
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm13
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOVSX rax, dword [rdi]
	ADD rdi, 4
	MOVSX r10, dword [rsi]
	ADD rsi, 4
	IMUL rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.K10 progbits alloc exec nowrite align=16
global _yepCore_Multiply_V32uV32u_V64u_K10
_yepCore_Multiply_V32uV32u_V64u_K10:
%else
section .text
global __yepCore_Multiply_V32uV32u_V64u_K10
__yepCore_Multiply_V32uV32u_V64u_K10:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	IMUL rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	MOVQ xmm1, [rdi]
	MOVQ xmm0, [rsi]
	MOVQ xmm8, [byte rdi + 8]
	MOVQ xmm13, [byte rsi + 8]
	MOVQ xmm9, [byte rdi + 16]
	MOVQ xmm10, [byte rsi + 16]
	PUNPCKLDQ xmm1, xmm1
	PUNPCKLDQ xmm0, xmm0
	MOVQ xmm12, [byte rdi + 24]
	MOVQ xmm4, [byte rsi + 24]
	PUNPCKLDQ xmm8, xmm8
	PUNPCKLDQ xmm13, xmm13
	MOVQ xmm14, [byte rdi + 32]
	MOVQ xmm2, [byte rsi + 32]
	PUNPCKLDQ xmm9, xmm9
	PUNPCKLDQ xmm10, xmm10
	PMULUDQ xmm1, xmm0
	MOVQ xmm11, [byte rdi + 40]
	MOVQ xmm7, [byte rsi + 40]
	PUNPCKLDQ xmm12, xmm12
	PUNPCKLDQ xmm4, xmm4
	PMULUDQ xmm8, xmm13
	MOVQ xmm5, [byte rdi + 48]
	MOVQ xmm6, [byte rsi + 48]
	PUNPCKLDQ xmm14, xmm14
	PUNPCKLDQ xmm2, xmm2
	PMULUDQ xmm9, xmm10
	MOVQ xmm3, [byte rdi + 56]
	MOVQ xmm15, [byte rsi + 56]
	PUNPCKLDQ xmm11, xmm11
	PUNPCKLDQ xmm7, xmm7
	PMULUDQ xmm12, xmm4
	MOVDQA [rdx], xmm1
	ADD rdi, 64
	ADD rsi, 64
	PUNPCKLDQ xmm5, xmm5
	PUNPCKLDQ xmm6, xmm6
	PMULUDQ xmm14, xmm2
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVQ xmm1, [rdi]
	MOVQ xmm0, [rsi]
	PUNPCKLDQ xmm3, xmm3
	PUNPCKLDQ xmm15, xmm15
	PMULUDQ xmm11, xmm7
	MOVDQA [byte rdx + 32], xmm9
	MOVQ xmm8, [byte rdi + 8]
	MOVQ xmm13, [byte rsi + 8]
	PMULUDQ xmm5, xmm6
	MOVDQA [byte rdx + 48], xmm12
	MOVQ xmm9, [byte rdi + 16]
	MOVQ xmm10, [byte rsi + 16]
	PUNPCKLDQ xmm1, xmm1
	PUNPCKLDQ xmm0, xmm0
	PMULUDQ xmm3, xmm15
	MOVDQA [byte rdx + 64], xmm14
	MOVQ xmm12, [byte rdi + 24]
	MOVQ xmm4, [byte rsi + 24]
	PUNPCKLDQ xmm8, xmm8
	PUNPCKLDQ xmm13, xmm13
	MOVDQA [byte rdx + 80], xmm11
	MOVQ xmm14, [byte rdi + 32]
	MOVQ xmm2, [byte rsi + 32]
	PUNPCKLDQ xmm9, xmm9
	PUNPCKLDQ xmm10, xmm10
	PMULUDQ xmm1, xmm0
	MOVDQA [byte rdx + 96], xmm5
	MOVQ xmm11, [byte rdi + 40]
	MOVQ xmm7, [byte rsi + 40]
	PUNPCKLDQ xmm12, xmm12
	PUNPCKLDQ xmm4, xmm4
	PMULUDQ xmm8, xmm13
	MOVDQA [byte rdx + 112], xmm3
	MOVQ xmm5, [byte rdi + 48]
	MOVQ xmm6, [byte rsi + 48]
	PUNPCKLDQ xmm14, xmm14
	PUNPCKLDQ xmm2, xmm2
	PMULUDQ xmm9, xmm10
	ADD rdx, 128
	MOVQ xmm3, [byte rdi + 56]
	MOVQ xmm15, [byte rsi + 56]
	PUNPCKLDQ xmm11, xmm11
	PUNPCKLDQ xmm7, xmm7
	PMULUDQ xmm12, xmm4
	MOVDQA [rdx], xmm1
	ADD rdi, 64
	ADD rsi, 64
	PUNPCKLDQ xmm5, xmm5
	PUNPCKLDQ xmm6, xmm6
	PMULUDQ xmm14, xmm2
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	PUNPCKLDQ xmm3, xmm3
	PUNPCKLDQ xmm15, xmm15
	PMULUDQ xmm11, xmm7
	MOVDQA [byte rdx + 32], xmm9
	PMULUDQ xmm5, xmm6
	MOVDQA [byte rdx + 48], xmm12
	PMULUDQ xmm3, xmm15
	MOVDQA [byte rdx + 64], xmm14
	MOVDQA [byte rdx + 80], xmm11
	MOVDQA [byte rdx + 96], xmm5
	MOVDQA [byte rdx + 112], xmm3
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	IMUL rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Multiply_V32uV32u_V64u_Nehalem
_yepCore_Multiply_V32uV32u_V64u_Nehalem:
%else
section .text
global __yepCore_Multiply_V32uV32u_V64u_Nehalem
__yepCore_Multiply_V32uV32u_V64u_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	IMUL rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	PMOVZXDQ xmm1, [rdi]
	PMOVZXDQ xmm0, [rsi]
	PMOVZXDQ xmm8, [byte rdi + 8]
	PMOVZXDQ xmm13, [byte rsi + 8]
	PMOVZXDQ xmm9, [byte rdi + 16]
	PMOVZXDQ xmm10, [byte rsi + 16]
	PMOVZXDQ xmm12, [byte rdi + 24]
	PMOVZXDQ xmm4, [byte rsi + 24]
	PMOVZXDQ xmm14, [byte rdi + 32]
	PMOVZXDQ xmm2, [byte rsi + 32]
	PMOVZXDQ xmm11, [byte rdi + 40]
	PMOVZXDQ xmm7, [byte rsi + 40]
	PMULUDQ xmm1, xmm0
	PMOVZXDQ xmm5, [byte rdi + 48]
	PMOVZXDQ xmm6, [byte rsi + 48]
	PMULUDQ xmm8, xmm13
	PMOVZXDQ xmm3, [byte rdi + 56]
	PMOVZXDQ xmm15, [byte rsi + 56]
	PMULUDQ xmm9, xmm10
	MOVDQA [rdx], xmm1
	ADD rdi, 64
	ADD rsi, 64
	PMULUDQ xmm12, xmm4
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	PMOVZXDQ xmm1, [rdi]
	PMOVZXDQ xmm0, [rsi]
	PMULUDQ xmm14, xmm2
	MOVDQA [byte rdx + 32], xmm9
	PMOVZXDQ xmm8, [byte rdi + 8]
	PMOVZXDQ xmm13, [byte rsi + 8]
	PMULUDQ xmm11, xmm7
	MOVDQA [byte rdx + 48], xmm12
	PMOVZXDQ xmm9, [byte rdi + 16]
	PMOVZXDQ xmm10, [byte rsi + 16]
	PMULUDQ xmm5, xmm6
	MOVDQA [byte rdx + 64], xmm14
	PMOVZXDQ xmm12, [byte rdi + 24]
	PMOVZXDQ xmm4, [byte rsi + 24]
	PMULUDQ xmm3, xmm15
	MOVDQA [byte rdx + 80], xmm11
	PMOVZXDQ xmm14, [byte rdi + 32]
	PMOVZXDQ xmm2, [byte rsi + 32]
	MOVDQA [byte rdx + 96], xmm5
	PMOVZXDQ xmm11, [byte rdi + 40]
	PMOVZXDQ xmm7, [byte rsi + 40]
	PMULUDQ xmm1, xmm0
	MOVDQA [byte rdx + 112], xmm3
	PMOVZXDQ xmm5, [byte rdi + 48]
	PMOVZXDQ xmm6, [byte rsi + 48]
	PMULUDQ xmm8, xmm13
	ADD rdx, 128
	PMOVZXDQ xmm3, [byte rdi + 56]
	PMOVZXDQ xmm15, [byte rsi + 56]
	PMULUDQ xmm9, xmm10
	MOVDQA [rdx], xmm1
	ADD rdi, 64
	ADD rsi, 64
	PMULUDQ xmm12, xmm4
	MOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	PMULUDQ xmm14, xmm2
	MOVDQA [byte rdx + 32], xmm9
	PMULUDQ xmm11, xmm7
	MOVDQA [byte rdx + 48], xmm12
	PMULUDQ xmm5, xmm6
	MOVDQA [byte rdx + 64], xmm14
	PMULUDQ xmm3, xmm15
	MOVDQA [byte rdx + 80], xmm11
	MOVDQA [byte rdx + 96], xmm5
	MOVDQA [byte rdx + 112], xmm3
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	IMUL rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Multiply_V32uV32u_V64u_SandyBridge
_yepCore_Multiply_V32uV32u_V64u_SandyBridge:
%else
section .text
global __yepCore_Multiply_V32uV32u_V64u_SandyBridge
__yepCore_Multiply_V32uV32u_V64u_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	IMUL rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 16
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ xmm1, [rdi]
	VPMOVZXDQ xmm0, [rsi]
	VPMOVZXDQ xmm8, [byte rdi + 8]
	VPMOVZXDQ xmm13, [byte rsi + 8]
	VPMOVZXDQ xmm9, [byte rdi + 16]
	VPMOVZXDQ xmm10, [byte rsi + 16]
	VPMOVZXDQ xmm12, [byte rdi + 24]
	VPMOVZXDQ xmm4, [byte rsi + 24]
	VPMOVZXDQ xmm14, [byte rdi + 32]
	VPMOVZXDQ xmm2, [byte rsi + 32]
	VPMULUDQ xmm1, xmm1, xmm0
	VPMOVZXDQ xmm11, [byte rdi + 40]
	VPMOVZXDQ xmm7, [byte rsi + 40]
	VPMULUDQ xmm8, xmm8, xmm13
	VPMOVZXDQ xmm5, [byte rdi + 48]
	VPMOVZXDQ xmm6, [byte rsi + 48]
	VPMULUDQ xmm9, xmm9, xmm10
	VPMOVZXDQ xmm3, [byte rdi + 56]
	VPMOVZXDQ xmm15, [byte rsi + 56]
	VPMULUDQ xmm12, xmm12, xmm4
	VMOVDQA [rdx], xmm1
	ADD rdi, 64
	ADD rsi, 64
	VPMULUDQ xmm14, xmm14, xmm2
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ xmm1, [rdi]
	VPMOVZXDQ xmm0, [rsi]
	VPMULUDQ xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 32], xmm9
	VPMOVZXDQ xmm8, [byte rdi + 8]
	VPMOVZXDQ xmm13, [byte rsi + 8]
	VPMULUDQ xmm5, xmm5, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPMOVZXDQ xmm9, [byte rdi + 16]
	VPMOVZXDQ xmm10, [byte rsi + 16]
	VPMULUDQ xmm3, xmm3, xmm15
	VMOVDQA [byte rdx + 64], xmm14
	VPMOVZXDQ xmm12, [byte rdi + 24]
	VPMOVZXDQ xmm4, [byte rsi + 24]
	VMOVDQA [byte rdx + 80], xmm11
	VPMOVZXDQ xmm14, [byte rdi + 32]
	VPMOVZXDQ xmm2, [byte rsi + 32]
	VPMULUDQ xmm1, xmm1, xmm0
	VMOVDQA [byte rdx + 96], xmm5
	VPMOVZXDQ xmm11, [byte rdi + 40]
	VPMOVZXDQ xmm7, [byte rsi + 40]
	VPMULUDQ xmm8, xmm8, xmm13
	VMOVDQA [byte rdx + 112], xmm3
	VPMOVZXDQ xmm5, [byte rdi + 48]
	VPMOVZXDQ xmm6, [byte rsi + 48]
	VPMULUDQ xmm9, xmm9, xmm10
	ADD rdx, 128
	VPMOVZXDQ xmm3, [byte rdi + 56]
	VPMOVZXDQ xmm15, [byte rsi + 56]
	VPMULUDQ xmm12, xmm12, xmm4
	VMOVDQA [rdx], xmm1
	ADD rdi, 64
	ADD rsi, 64
	VPMULUDQ xmm14, xmm14, xmm2
	VMOVDQA [byte rdx + 16], xmm8
	SUB rcx, 16
	JAE .process_batch
	.process_batch_epilogue:
	VPMULUDQ xmm11, xmm11, xmm7
	VMOVDQA [byte rdx + 32], xmm9
	VPMULUDQ xmm5, xmm5, xmm6
	VMOVDQA [byte rdx + 48], xmm12
	VPMULUDQ xmm3, xmm3, xmm15
	VMOVDQA [byte rdx + 64], xmm14
	VMOVDQA [byte rdx + 80], xmm11
	VMOVDQA [byte rdx + 96], xmm5
	VMOVDQA [byte rdx + 112], xmm3
	ADD rdx, 128
	.batch_process_finish:
	ADD rcx, 16
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	IMUL rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepCore_Multiply_V32uV32u_V64u_Haswell
_yepCore_Multiply_V32uV32u_V64u_Haswell:
%else
section .text
global __yepCore_Multiply_V32uV32u_V64u_Haswell
__yepCore_Multiply_V32uV32u_V64u_Haswell:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	IMUL rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 32
	JB .batch_process_finish
	.process_batch_prologue:
	VPMOVZXDQ ymm1, [rdi]
	VPMOVZXDQ ymm0, [rsi]
	VPMOVZXDQ ymm8, [byte rdi + 16]
	VPMOVZXDQ ymm6, [byte rsi + 16]
	VPMOVZXDQ ymm9, [byte rdi + 32]
	VPMOVZXDQ ymm4, [byte rsi + 32]
	VPMOVZXDQ ymm2, [byte rdi + 48]
	VPMOVZXDQ ymm3, [byte rsi + 48]
	VPMOVZXDQ ymm14, [byte rdi + 64]
	VPMOVZXDQ ymm15, [byte rsi + 64]
	VPMULUDQ ymm1, ymm1, ymm0
	VPMOVZXDQ ymm12, [byte rdi + 80]
	VPMOVZXDQ ymm10, [byte rsi + 80]
	VPMULUDQ ymm8, ymm8, ymm6
	VPMOVZXDQ ymm11, [byte rdi + 96]
	VPMOVZXDQ ymm5, [byte rsi + 96]
	VPMULUDQ ymm9, ymm9, ymm4
	VPMOVZXDQ ymm13, [byte rdi + 112]
	VPMOVZXDQ ymm7, [byte rsi + 112]
	VPMULUDQ ymm2, ymm2, ymm3
	VMOVDQA [rdx], ymm1
	ADD rdi, 128
	ADD rsi, 128
	VPMULUDQ ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 32
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VPMOVZXDQ ymm1, [rdi]
	VPMOVZXDQ ymm0, [rsi]
	VPMULUDQ ymm12, ymm12, ymm10
	VMOVDQA [byte rdx + 64], ymm9
	VPMOVZXDQ ymm8, [byte rdi + 16]
	VPMOVZXDQ ymm6, [byte rsi + 16]
	VPMULUDQ ymm11, ymm11, ymm5
	VMOVDQA [byte rdx + 96], ymm2
	VPMOVZXDQ ymm9, [byte rdi + 32]
	VPMOVZXDQ ymm4, [byte rsi + 32]
	VPMULUDQ ymm13, ymm13, ymm7
	VMOVDQA [dword rdx + 128], ymm14
	VPMOVZXDQ ymm2, [byte rdi + 48]
	VPMOVZXDQ ymm3, [byte rsi + 48]
	VMOVDQA [dword rdx + 160], ymm12
	VPMOVZXDQ ymm14, [byte rdi + 64]
	VPMOVZXDQ ymm15, [byte rsi + 64]
	VPMULUDQ ymm1, ymm1, ymm0
	VMOVDQA [dword rdx + 192], ymm11
	VPMOVZXDQ ymm12, [byte rdi + 80]
	VPMOVZXDQ ymm10, [byte rsi + 80]
	VPMULUDQ ymm8, ymm8, ymm6
	VMOVDQA [dword rdx + 224], ymm13
	VPMOVZXDQ ymm11, [byte rdi + 96]
	VPMOVZXDQ ymm5, [byte rsi + 96]
	VPMULUDQ ymm9, ymm9, ymm4
	ADD rdx, 256
	VPMOVZXDQ ymm13, [byte rdi + 112]
	VPMOVZXDQ ymm7, [byte rsi + 112]
	VPMULUDQ ymm2, ymm2, ymm3
	VMOVDQA [rdx], ymm1
	ADD rdi, 128
	ADD rsi, 128
	VPMULUDQ ymm14, ymm14, ymm15
	VMOVDQA [byte rdx + 32], ymm8
	SUB rcx, 32
	JAE .process_batch
	.process_batch_epilogue:
	VPMULUDQ ymm12, ymm12, ymm10
	VMOVDQA [byte rdx + 64], ymm9
	VPMULUDQ ymm11, ymm11, ymm5
	VMOVDQA [byte rdx + 96], ymm2
	VPMULUDQ ymm13, ymm13, ymm7
	VMOVDQA [dword rdx + 128], ymm14
	VMOVDQA [dword rdx + 160], ymm12
	VMOVDQA [dword rdx + 192], ymm11
	VMOVDQA [dword rdx + 224], ymm13
	ADD rdx, 256
	.batch_process_finish:
	ADD rcx, 32
	JZ .return_ok
	.process_single:
	MOV eax, [rdi]
	ADD rdi, 4
	MOV r10d, [rsi]
	ADD rsi, 4
	IMUL rax, r10
	MOV [rdx], rax
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Multiply_V32fV32f_V32f_Nehalem
_yepCore_Multiply_V32fV32f_V32f_Nehalem:
%else
section .text
global __yepCore_Multiply_V32fV32f_V32f_Nehalem
__yepCore_Multiply_V32fV32f_V32f_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSS xmm2, [rdi]
	ADD rdi, 4
	MOVSS xmm1, [rsi]
	ADD rsi, 4
	MULSS xmm2, xmm1
	MOVSS [rdx], xmm2
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 28
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPS xmm2, [rdi]
	MOVUPS xmm8, [byte rdi + 16]
	MOVUPS xmm4, [rsi]
	MOVUPS xmm9, [byte rdi + 32]
	MOVUPS xmm3, [byte rsi + 16]
	MOVUPS xmm12, [byte rdi + 48]
	MOVUPS xmm13, [byte rsi + 32]
	MOVUPS xmm14, [byte rdi + 64]
	MOVUPS xmm10, [byte rsi + 48]
	MOVUPS xmm11, [byte rdi + 80]
	MOVUPS xmm5, [byte rsi + 64]
	MULPS xmm2, xmm4
	MOVUPS xmm6, [byte rdi + 96]
	MOVUPS xmm7, [byte rsi + 80]
	MULPS xmm8, xmm3
	MOVAPS [rdx], xmm2
	ADD rdi, 112
	MOVUPS xmm15, [byte rsi + 96]
	MULPS xmm9, xmm13
	MOVAPS [byte rdx + 16], xmm8
	SUB rcx, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPS xmm2, [rdi]
	ADD rsi, 112
	MULPS xmm12, xmm10
	MOVAPS [byte rdx + 32], xmm9
	MOVUPS xmm8, [byte rdi + 16]
	MOVUPS xmm4, [rsi]
	MULPS xmm14, xmm5
	MOVAPS [byte rdx + 48], xmm12
	MOVUPS xmm9, [byte rdi + 32]
	MOVUPS xmm3, [byte rsi + 16]
	MULPS xmm11, xmm7
	MOVAPS [byte rdx + 64], xmm14
	MOVUPS xmm12, [byte rdi + 48]
	MOVUPS xmm13, [byte rsi + 32]
	MULPS xmm6, xmm15
	MOVAPS [byte rdx + 80], xmm11
	MOVUPS xmm14, [byte rdi + 64]
	MOVUPS xmm10, [byte rsi + 48]
	MOVAPS [byte rdx + 96], xmm6
	MOVUPS xmm11, [byte rdi + 80]
	MOVUPS xmm5, [byte rsi + 64]
	MULPS xmm2, xmm4
	ADD rdx, 112
	MOVUPS xmm6, [byte rdi + 96]
	MOVUPS xmm7, [byte rsi + 80]
	MULPS xmm8, xmm3
	MOVAPS [rdx], xmm2
	ADD rdi, 112
	MOVUPS xmm15, [byte rsi + 96]
	MULPS xmm9, xmm13
	MOVAPS [byte rdx + 16], xmm8
	SUB rcx, 28
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 112
	MULPS xmm12, xmm10
	MOVAPS [byte rdx + 32], xmm9
	MULPS xmm14, xmm5
	MOVAPS [byte rdx + 48], xmm12
	MULPS xmm11, xmm7
	MOVAPS [byte rdx + 64], xmm14
	MULPS xmm6, xmm15
	MOVAPS [byte rdx + 80], xmm11
	MOVAPS [byte rdx + 96], xmm6
	ADD rdx, 112
	.batch_process_finish:
	ADD rcx, 28
	JZ .return_ok
	.process_single:
	MOVSS xmm8, [rdi]
	ADD rdi, 4
	MOVSS xmm9, [rsi]
	ADD rsi, 4
	MULSS xmm8, xmm9
	MOVSS [rdx], xmm8
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Multiply_V32fV32f_V32f_SandyBridge
_yepCore_Multiply_V32fV32f_V32f_SandyBridge:
%else
section .text
global __yepCore_Multiply_V32fV32f_V32f_SandyBridge
__yepCore_Multiply_V32fV32f_V32f_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 3
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	VMOVSS xmm2, [rdi]
	ADD rdi, 4
	VMOVSS xmm1, [rsi]
	ADD rsi, 4
	VMULSS xmm2, xmm2, xmm1
	VMOVSS [rdx], xmm2
	ADD rdx, 4
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 56
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPS ymm2, [rdi]
	VMOVUPS ymm8, [byte rdi + 32]
	VMOVUPS ymm13, [rsi]
	VMOVUPS ymm9, [byte rdi + 64]
	VMOVUPS ymm7, [byte rsi + 32]
	VMOVUPS ymm3, [byte rdi + 96]
	VMOVUPS ymm6, [byte rsi + 64]
	VMOVUPS ymm14, [dword rdi + 128]
	VMOVUPS ymm5, [byte rsi + 96]
	VMOVUPS ymm12, [dword rdi + 160]
	VMOVUPS ymm4, [dword rsi + 128]
	VMULPS ymm2, ymm2, ymm13
	VMOVUPS ymm11, [dword rdi + 192]
	VMOVUPS ymm15, [dword rsi + 160]
	VMULPS ymm8, ymm8, ymm7
	VMOVAPS [rdx], ymm2
	ADD rdi, 224
	VMOVUPS ymm10, [dword rsi + 192]
	VMULPS ymm9, ymm9, ymm6
	VMOVAPS [byte rdx + 32], ymm8
	SUB rcx, 56
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPS ymm2, [rdi]
	ADD rsi, 224
	VMULPS ymm3, ymm3, ymm5
	VMOVAPS [byte rdx + 64], ymm9
	VMOVUPS ymm8, [byte rdi + 32]
	VMOVUPS ymm13, [rsi]
	VMULPS ymm14, ymm14, ymm4
	VMOVAPS [byte rdx + 96], ymm3
	VMOVUPS ymm9, [byte rdi + 64]
	VMOVUPS ymm7, [byte rsi + 32]
	VMULPS ymm12, ymm12, ymm15
	VMOVAPS [dword rdx + 128], ymm14
	VMOVUPS ymm3, [byte rdi + 96]
	VMOVUPS ymm6, [byte rsi + 64]
	VMULPS ymm11, ymm11, ymm10
	VMOVAPS [dword rdx + 160], ymm12
	VMOVUPS ymm14, [dword rdi + 128]
	VMOVUPS ymm5, [byte rsi + 96]
	VMOVAPS [dword rdx + 192], ymm11
	VMOVUPS ymm12, [dword rdi + 160]
	VMOVUPS ymm4, [dword rsi + 128]
	VMULPS ymm2, ymm2, ymm13
	ADD rdx, 224
	VMOVUPS ymm11, [dword rdi + 192]
	VMOVUPS ymm15, [dword rsi + 160]
	VMULPS ymm8, ymm8, ymm7
	VMOVAPS [rdx], ymm2
	ADD rdi, 224
	VMOVUPS ymm10, [dword rsi + 192]
	VMULPS ymm9, ymm9, ymm6
	VMOVAPS [byte rdx + 32], ymm8
	SUB rcx, 56
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 224
	VMULPS ymm3, ymm3, ymm5
	VMOVAPS [byte rdx + 64], ymm9
	VMULPS ymm14, ymm14, ymm4
	VMOVAPS [byte rdx + 96], ymm3
	VMULPS ymm12, ymm12, ymm15
	VMOVAPS [dword rdx + 128], ymm14
	VMULPS ymm11, ymm11, ymm10
	VMOVAPS [dword rdx + 160], ymm12
	VMOVAPS [dword rdx + 192], ymm11
	ADD rdx, 224
	.batch_process_finish:
	ADD rcx, 56
	JZ .return_ok
	.process_single:
	VMOVSS xmm8, [rdi]
	ADD rdi, 4
	VMOVSS xmm9, [rsi]
	ADD rsi, 4
	VMULSS xmm8, xmm8, xmm9
	VMOVSS [rdx], xmm8
	ADD rdx, 4
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepCore_Multiply_V64fV64f_V64f_Nehalem
_yepCore_Multiply_V64fV64f_V64f_Nehalem:
%else
section .text
global __yepCore_Multiply_V64fV64f_V64f_Nehalem
__yepCore_Multiply_V64fV64f_V64f_Nehalem:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 15
	JZ .source_z_16b_aligned
	.source_z_16b_misaligned:
	MOVSD xmm2, [rdi]
	ADD rdi, 8
	MOVSD xmm1, [rsi]
	ADD rsi, 8
	MULSD xmm2, xmm1
	MOVSD [rdx], xmm2
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_z_16b_misaligned
	.source_z_16b_aligned:
	SUB rcx, 14
	JB .batch_process_finish
	.process_batch_prologue:
	MOVUPD xmm2, [rdi]
	MOVUPD xmm8, [byte rdi + 16]
	MOVUPD xmm4, [rsi]
	MOVUPD xmm9, [byte rdi + 32]
	MOVUPD xmm3, [byte rsi + 16]
	MOVUPD xmm12, [byte rdi + 48]
	MOVUPD xmm13, [byte rsi + 32]
	MOVUPD xmm14, [byte rdi + 64]
	MOVUPD xmm10, [byte rsi + 48]
	MOVUPD xmm11, [byte rdi + 80]
	MOVUPD xmm5, [byte rsi + 64]
	MULPD xmm2, xmm4
	MOVUPD xmm6, [byte rdi + 96]
	MOVUPD xmm7, [byte rsi + 80]
	MULPD xmm8, xmm3
	MOVAPD [rdx], xmm2
	ADD rdi, 112
	MOVUPD xmm15, [byte rsi + 96]
	MULPD xmm9, xmm13
	MOVAPD [byte rdx + 16], xmm8
	SUB rcx, 14
	JB .process_batch_epilogue
	align 16
	.process_batch:
	MOVUPD xmm2, [rdi]
	ADD rsi, 112
	MULPD xmm12, xmm10
	MOVAPD [byte rdx + 32], xmm9
	MOVUPD xmm8, [byte rdi + 16]
	MOVUPD xmm4, [rsi]
	MULPD xmm14, xmm5
	MOVAPD [byte rdx + 48], xmm12
	MOVUPD xmm9, [byte rdi + 32]
	MOVUPD xmm3, [byte rsi + 16]
	MULPD xmm11, xmm7
	MOVAPD [byte rdx + 64], xmm14
	MOVUPD xmm12, [byte rdi + 48]
	MOVUPD xmm13, [byte rsi + 32]
	MULPD xmm6, xmm15
	MOVAPD [byte rdx + 80], xmm11
	MOVUPD xmm14, [byte rdi + 64]
	MOVUPD xmm10, [byte rsi + 48]
	MOVAPD [byte rdx + 96], xmm6
	MOVUPD xmm11, [byte rdi + 80]
	MOVUPD xmm5, [byte rsi + 64]
	MULPD xmm2, xmm4
	ADD rdx, 112
	MOVUPD xmm6, [byte rdi + 96]
	MOVUPD xmm7, [byte rsi + 80]
	MULPD xmm8, xmm3
	MOVAPD [rdx], xmm2
	ADD rdi, 112
	MOVUPD xmm15, [byte rsi + 96]
	MULPD xmm9, xmm13
	MOVAPD [byte rdx + 16], xmm8
	SUB rcx, 14
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 112
	MULPD xmm12, xmm10
	MOVAPD [byte rdx + 32], xmm9
	MULPD xmm14, xmm5
	MOVAPD [byte rdx + 48], xmm12
	MULPD xmm11, xmm7
	MOVAPD [byte rdx + 64], xmm14
	MULPD xmm6, xmm15
	MOVAPD [byte rdx + 80], xmm11
	MOVAPD [byte rdx + 96], xmm6
	ADD rdx, 112
	.batch_process_finish:
	ADD rcx, 14
	JZ .return_ok
	.process_single:
	MOVSD xmm8, [rdi]
	ADD rdi, 8
	MOVSD xmm9, [rsi]
	ADD rsi, 8
	MULSD xmm8, xmm9
	MOVSD [rdx], xmm8
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepCore_Multiply_V64fV64f_V64f_SandyBridge
_yepCore_Multiply_V64fV64f_V64f_SandyBridge:
%else
section .text
global __yepCore_Multiply_V64fV64f_V64f_SandyBridge
__yepCore_Multiply_V64fV64f_V64f_SandyBridge:
%endif
	.ENTRY:
	TEST rdi, rdi
	JZ .return_null_pointer
	TEST rdi, 7
	JNZ .return_misaligned_pointer
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST rcx, rcx
	JZ .return_ok
	TEST rdx, 31
	JZ .source_z_32b_aligned
	.source_z_32b_misaligned:
	VMOVSD xmm2, [rdi]
	ADD rdi, 8
	VMOVSD xmm1, [rsi]
	ADD rsi, 8
	VMULSD xmm2, xmm2, xmm1
	VMOVSD [rdx], xmm2
	ADD rdx, 8
	SUB rcx, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_z_32b_misaligned
	.source_z_32b_aligned:
	SUB rcx, 28
	JB .batch_process_finish
	.process_batch_prologue:
	VMOVUPD ymm2, [rdi]
	VMOVUPD ymm8, [byte rdi + 32]
	VMOVUPD ymm13, [rsi]
	VMOVUPD ymm9, [byte rdi + 64]
	VMOVUPD ymm7, [byte rsi + 32]
	VMOVUPD ymm3, [byte rdi + 96]
	VMOVUPD ymm6, [byte rsi + 64]
	VMOVUPD ymm14, [dword rdi + 128]
	VMOVUPD ymm5, [byte rsi + 96]
	VMOVUPD ymm12, [dword rdi + 160]
	VMOVUPD ymm4, [dword rsi + 128]
	VMULPD ymm2, ymm2, ymm13
	VMOVUPD ymm11, [dword rdi + 192]
	VMOVUPD ymm15, [dword rsi + 160]
	VMULPD ymm8, ymm8, ymm7
	VMOVAPD [rdx], ymm2
	ADD rdi, 224
	VMOVUPD ymm10, [dword rsi + 192]
	VMULPD ymm9, ymm9, ymm6
	VMOVAPD [byte rdx + 32], ymm8
	SUB rcx, 28
	JB .process_batch_epilogue
	align 16
	.process_batch:
	VMOVUPD ymm2, [rdi]
	ADD rsi, 224
	VMULPD ymm3, ymm3, ymm5
	VMOVAPD [byte rdx + 64], ymm9
	VMOVUPD ymm8, [byte rdi + 32]
	VMOVUPD ymm13, [rsi]
	VMULPD ymm14, ymm14, ymm4
	VMOVAPD [byte rdx + 96], ymm3
	VMOVUPD ymm9, [byte rdi + 64]
	VMOVUPD ymm7, [byte rsi + 32]
	VMULPD ymm12, ymm12, ymm15
	VMOVAPD [dword rdx + 128], ymm14
	VMOVUPD ymm3, [byte rdi + 96]
	VMOVUPD ymm6, [byte rsi + 64]
	VMULPD ymm11, ymm11, ymm10
	VMOVAPD [dword rdx + 160], ymm12
	VMOVUPD ymm14, [dword rdi + 128]
	VMOVUPD ymm5, [byte rsi + 96]
	VMOVAPD [dword rdx + 192], ymm11
	VMOVUPD ymm12, [dword rdi + 160]
	VMOVUPD ymm4, [dword rsi + 128]
	VMULPD ymm2, ymm2, ymm13
	ADD rdx, 224
	VMOVUPD ymm11, [dword rdi + 192]
	VMOVUPD ymm15, [dword rsi + 160]
	VMULPD ymm8, ymm8, ymm7
	VMOVAPD [rdx], ymm2
	ADD rdi, 224
	VMOVUPD ymm10, [dword rsi + 192]
	VMULPD ymm9, ymm9, ymm6
	VMOVAPD [byte rdx + 32], ymm8
	SUB rcx, 28
	JAE .process_batch
	.process_batch_epilogue:
	ADD rsi, 224
	VMULPD ymm3, ymm3, ymm5
	VMOVAPD [byte rdx + 64], ymm9
	VMULPD ymm14, ymm14, ymm4
	VMOVAPD [byte rdx + 96], ymm3
	VMULPD ymm12, ymm12, ymm15
	VMOVAPD [dword rdx + 128], ymm14
	VMULPD ymm11, ymm11, ymm10
	VMOVAPD [dword rdx + 160], ymm12
	VMOVAPD [dword rdx + 192], ymm11
	ADD rdx, 224
	.batch_process_finish:
	ADD rcx, 28
	JZ .return_ok
	.process_single:
	VMOVSD xmm8, [rdi]
	ADD rdi, 8
	VMOVSD xmm9, [rsi]
	ADD rsi, 8
	VMULSD xmm8, xmm8, xmm9
	VMOVSD [rdx], xmm8
	ADD rdx, 8
	SUB rcx, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return
