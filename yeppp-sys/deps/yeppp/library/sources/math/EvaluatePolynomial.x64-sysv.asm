;                       Yeppp! library implementation
;                   This file is auto-generated by Peach-Py,
;        Portable Efficient Assembly Code-generator in Higher-level Python,
;                  part of the Yeppp! library infrastructure
; This file is part of Yeppp! library and licensed under the New BSD license.
; See LICENSE.txt for the full text of the license.

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Unknown progbits alloc exec nowrite align=16
global _V32fV32f_V32f_Unknown
_V32fV32f_V32f_Unknown:
%else
section .text
global __V32fV32f_V32f_Unknown
__V32fV32f_V32f_Unknown:
%endif
	.ENTRY:
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_ok
	TEST rsi, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSS xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 4 - 4]
	MOVSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	MULSS xmm9, xmm8
	ADDSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	MOVSS [rdx], xmm9
	ADD rsi, 4
	ADD rdx, 4
	SUB r8, 1
	JZ .return_ok
	TEST rsi, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB r8, 24
	JB .process_restore
	align 32
	.process_batch_full:
	LEA rax, [byte rdi + rcx * 4 - 4]
	MOVSS xmm8, [rax]
	SHUFPS xmm8, xmm8, 0
	MOVAPS xmm9, xmm8
	MOVAPS xmm10, xmm8
	MOVAPS xmm11, xmm8
	MOVAPS xmm12, xmm8
	MOVAPS xmm13, xmm8
	SUB rax, 4
	CMP rax, rdi
	JB .batch_polevl_finish
	MOVSS xmm14, [rax]
	SHUFPS xmm14, xmm14, 0
	MOVAPS xmm15, [rsi]
	MULPS xmm8, xmm15
	ADDPS xmm8, xmm14
	MOVAPS xmm7, [byte rsi + 16]
	MULPS xmm9, xmm7
	ADDPS xmm9, xmm14
	MOVAPS xmm6, [byte rsi + 32]
	MULPS xmm10, xmm6
	ADDPS xmm10, xmm14
	MOVAPS xmm5, [byte rsi + 48]
	MULPS xmm11, xmm5
	ADDPS xmm11, xmm14
	MOVAPS xmm4, [byte rsi + 64]
	MULPS xmm12, xmm4
	ADDPS xmm12, xmm14
	MOVAPS xmm3, [byte rsi + 80]
	MULPS xmm13, xmm3
	ADDPS xmm13, xmm14
	SUB rax, 4
	CMP rax, rdi
	JB .batch_polevl_finish
	.batch_polevl_next:
	MOVSS xmm14, [rax]
	SHUFPS xmm14, xmm14, 0
	MULPS xmm8, xmm15
	ADDPS xmm8, xmm14
	MULPS xmm9, xmm7
	ADDPS xmm9, xmm14
	MULPS xmm10, xmm6
	ADDPS xmm10, xmm14
	MULPS xmm11, xmm5
	ADDPS xmm11, xmm14
	MULPS xmm12, xmm4
	ADDPS xmm12, xmm14
	MULPS xmm13, xmm3
	ADDPS xmm13, xmm14
	SUB rax, 4
	CMP rax, rdi
	JAE .batch_polevl_next
	.batch_polevl_finish:
	MOVUPS [rdx], xmm8
	MOVUPS [byte rdx + 16], xmm9
	MOVUPS [byte rdx + 32], xmm10
	MOVUPS [byte rdx + 48], xmm11
	MOVUPS [byte rdx + 64], xmm12
	MOVUPS [byte rdx + 80], xmm13
	ADD rsi, 96
	ADD rdx, 96
	SUB r8, 24
	JAE .process_batch_full
	.process_restore:
	ADD r8, 24
	JZ .return_ok
	.process_single:
	MOVSS xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 4 - 4]
	MOVSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	MULSS xmm9, xmm8
	ADDSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	MOVSS [rdx], xmm9
	ADD rsi, 4
	ADD rdx, 4
	SUB r8, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepMath_EvaluatePolynomial_V32fV32f_V32f_Nehalem
_yepMath_EvaluatePolynomial_V32fV32f_V32f_Nehalem:
%else
section .text
global __yepMath_EvaluatePolynomial_V32fV32f_V32f_Nehalem
__yepMath_EvaluatePolynomial_V32fV32f_V32f_Nehalem:
%endif
	.ENTRY:
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_ok
	TEST rsi, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSS xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 4 - 4]
	MOVSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	MULSS xmm9, xmm8
	ADDSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	MOVSS [rdx], xmm9
	ADD rsi, 4
	ADD rdx, 4
	SUB r8, 1
	JZ .return_ok
	TEST rsi, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB r8, 40
	JB .process_restore
	align 32
	.process_batch_full:
	LEA rax, [byte rdi + rcx * 4 - 4]
	MOVSS xmm8, [rax]
	SHUFPS xmm8, xmm8, 0
	MOVAPS xmm9, xmm8
	MOVAPS xmm10, xmm8
	MOVAPS xmm11, xmm8
	MOVAPS xmm12, xmm8
	MOVAPS xmm13, xmm8
	MOVAPS xmm14, xmm8
	MOVAPS xmm15, xmm8
	MOVAPS xmm7, xmm8
	MOVAPS xmm6, xmm8
	SUB rax, 4
	CMP rax, rdi
	JB .batch_polevl_finish
	MOVSS xmm5, [rax]
	SHUFPS xmm5, xmm5, 0
	MOVAPS xmm4, [rsi]
	MULPS xmm8, xmm4
	ADDPS xmm8, xmm5
	MULPS xmm9, [byte rsi + 16]
	ADDPS xmm9, xmm5
	MOVAPS xmm3, [byte rsi + 32]
	MULPS xmm10, xmm3
	ADDPS xmm10, xmm5
	MULPS xmm11, [byte rsi + 48]
	ADDPS xmm11, xmm5
	MOVAPS xmm2, [byte rsi + 64]
	MULPS xmm12, xmm2
	ADDPS xmm12, xmm5
	MULPS xmm13, [byte rsi + 80]
	ADDPS xmm13, xmm5
	MOVAPS xmm1, [byte rsi + 96]
	MULPS xmm14, xmm1
	ADDPS xmm14, xmm5
	MULPS xmm15, [byte rsi + 112]
	ADDPS xmm15, xmm5
	MOVAPS xmm0, [dword rsi + 128]
	MULPS xmm7, xmm0
	ADDPS xmm7, xmm5
	MULPS xmm6, [dword rsi + 144]
	ADDPS xmm6, xmm5
	SUB rax, 4
	CMP rax, rdi
	JB .batch_polevl_finish
	.batch_polevl_next:
	MOVSS xmm5, [rax]
	SHUFPS xmm5, xmm5, 0
	MULPS xmm8, xmm4
	ADDPS xmm8, xmm5
	MULPS xmm9, [byte rsi + 16]
	ADDPS xmm9, xmm5
	MULPS xmm10, xmm3
	ADDPS xmm10, xmm5
	MULPS xmm11, [byte rsi + 48]
	ADDPS xmm11, xmm5
	MULPS xmm12, xmm2
	ADDPS xmm12, xmm5
	MULPS xmm13, [byte rsi + 80]
	ADDPS xmm13, xmm5
	MULPS xmm14, xmm1
	ADDPS xmm14, xmm5
	MULPS xmm15, [byte rsi + 112]
	ADDPS xmm15, xmm5
	MULPS xmm7, xmm0
	ADDPS xmm7, xmm5
	MULPS xmm6, [dword rsi + 144]
	ADDPS xmm6, xmm5
	SUB rax, 4
	CMP rax, rdi
	JAE .batch_polevl_next
	.batch_polevl_finish:
	MOVUPS [rdx], xmm8
	MOVUPS [byte rdx + 16], xmm9
	MOVUPS [byte rdx + 32], xmm10
	MOVUPS [byte rdx + 48], xmm11
	MOVUPS [byte rdx + 64], xmm12
	MOVUPS [byte rdx + 80], xmm13
	MOVUPS [byte rdx + 96], xmm14
	MOVUPS [byte rdx + 112], xmm15
	MOVUPS [dword rdx + 128], xmm7
	MOVUPS [dword rdx + 144], xmm6
	ADD rsi, 160
	ADD rdx, 160
	SUB r8, 40
	JAE .process_batch_full
	.process_restore:
	ADD r8, 40
	JZ .return_ok
	.process_single:
	MOVSS xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 4 - 4]
	MOVSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	MULSS xmm9, xmm8
	ADDSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	MOVSS [rdx], xmm9
	ADD rsi, 4
	ADD rdx, 4
	SUB r8, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Bonnell progbits alloc exec nowrite align=16
global _yepMath_EvaluatePolynomial_V32fV32f_V32f_Bonnell
_yepMath_EvaluatePolynomial_V32fV32f_V32f_Bonnell:
%else
section .text
global __yepMath_EvaluatePolynomial_V32fV32f_V32f_Bonnell
__yepMath_EvaluatePolynomial_V32fV32f_V32f_Bonnell:
%endif
	.ENTRY:
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_ok
	TEST rsi, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSS xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 4 - 4]
	MOVSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	MULSS xmm9, xmm8
	ADDSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	MOVSS [rdx], xmm9
	ADD rsi, 4
	ADD rdx, 4
	SUB r8, 1
	JZ .return_ok
	TEST rsi, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB r8, 56
	JB .process_restore
	align 32
	.process_batch_full:
	LEA rax, [byte rdi + rcx * 4 - 4]
	MOVSS xmm8, [rax]
	SHUFPS xmm8, xmm8, 0
	MOVAPS xmm9, [rsi]
	MOVAPS xmm10, xmm8
	MOVAPS xmm11, xmm8
	MOVAPS xmm12, xmm8
	MOVAPS xmm13, xmm8
	MOVAPS xmm14, xmm8
	MOVAPS xmm15, xmm8
	MOVAPS xmm7, xmm8
	MOVAPS xmm6, xmm8
	MOVAPS xmm5, xmm8
	MOVAPS xmm4, xmm8
	MOVAPS xmm3, xmm8
	MOVAPS xmm2, xmm8
	MOVAPS xmm1, xmm8
	SUB rax, 4
	CMP rax, rdi
	JB .batch_polevl_finish
	.batch_polevl_next:
	MOVSS xmm0, [rax]
	MULPS xmm8, xmm9
	SHUFPS xmm0, xmm0, 0
	MULPS xmm10, [byte rsi + 16]
	MULPS xmm11, [byte rsi + 32]
	ADDPS xmm8, xmm0
	MULPS xmm12, [byte rsi + 48]
	ADDPS xmm10, xmm0
	MULPS xmm13, [byte rsi + 64]
	ADDPS xmm11, xmm0
	MULPS xmm14, [byte rsi + 80]
	ADDPS xmm12, xmm0
	MULPS xmm15, [byte rsi + 96]
	ADDPS xmm13, xmm0
	MULPS xmm7, [byte rsi + 112]
	ADDPS xmm14, xmm0
	MULPS xmm6, [dword rsi + 128]
	ADDPS xmm15, xmm0
	MULPS xmm5, [dword rsi + 144]
	ADDPS xmm7, xmm0
	MULPS xmm4, [dword rsi + 160]
	ADDPS xmm6, xmm0
	MULPS xmm3, [dword rsi + 176]
	ADDPS xmm5, xmm0
	MULPS xmm2, [dword rsi + 192]
	ADDPS xmm4, xmm0
	MULPS xmm1, [dword rsi + 208]
	ADDPS xmm3, xmm0
	SUB rax, 4
	ADDPS xmm2, xmm0
	CMP rax, rdi
	ADDPS xmm1, xmm0
	JAE .batch_polevl_next
	.batch_polevl_finish:
	MOVUPS [rdx], xmm8
	MOVUPS [byte rdx + 16], xmm10
	MOVUPS [byte rdx + 32], xmm11
	MOVUPS [byte rdx + 48], xmm12
	MOVUPS [byte rdx + 64], xmm13
	MOVUPS [byte rdx + 80], xmm14
	MOVUPS [byte rdx + 96], xmm15
	MOVUPS [byte rdx + 112], xmm7
	MOVUPS [dword rdx + 128], xmm6
	MOVUPS [dword rdx + 144], xmm5
	MOVUPS [dword rdx + 160], xmm4
	MOVUPS [dword rdx + 176], xmm3
	MOVUPS [dword rdx + 192], xmm2
	MOVUPS [dword rdx + 208], xmm1
	ADD rsi, 224
	ADD rdx, 224
	SUB r8, 56
	JAE .process_batch_full
	.process_restore:
	ADD r8, 56
	JZ .return_ok
	.process_single:
	MOVSS xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 4 - 4]
	MOVSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	MULSS xmm9, xmm8
	ADDSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	MOVSS [rdx], xmm9
	ADD rsi, 4
	ADD rdx, 4
	SUB r8, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Bulldozer progbits alloc exec nowrite align=16
global _yepMath_EvaluatePolynomial_V32fV32f_V32f_Bulldozer
_yepMath_EvaluatePolynomial_V32fV32f_V32f_Bulldozer:
%else
section .text
global __yepMath_EvaluatePolynomial_V32fV32f_V32f_Bulldozer
__yepMath_EvaluatePolynomial_V32fV32f_V32f_Bulldozer:
%endif
	.ENTRY:
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_ok
	TEST rsi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 4 - 4]
	VMOVSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	VFMADDSS xmm9, xmm9, xmm8, [rax]
	SUB rax, 4
	CMP rax, rdi
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	VMOVSS [rdx], xmm9
	ADD rsi, 4
	ADD rdx, 4
	SUB r8, 1
	JZ .return_ok
	TEST rsi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 40
	JB .process_restore
	align 32
	.process_batch_full:
	LEA rax, [byte rdi + rcx * 4 - 4]
	VBROADCASTSS ymm8, [rax]
	VMOVAPS xmm9, xmm8
	VMOVAPS xmm10, xmm8
	VMOVAPS ymm11, ymm8
	VMOVAPS ymm12, ymm8
	VMOVAPS ymm13, ymm8
	SUB rax, 4
	CMP rax, rdi
	JB .batch_polevl_finish
	VBROADCASTSS ymm14, [rax]
	VMOVAPS xmm15, [rsi]
	VFMADDPS xmm9, xmm9, xmm15, xmm14
	VMOVAPS xmm7, [byte rsi + 16]
	VFMADDPS xmm10, xmm10, xmm7, xmm14
	VMOVAPS ymm6, [byte rsi + 32]
	VFMADDPS ymm11, ymm11, ymm6, ymm14
	VMOVAPS ymm5, [byte rsi + 64]
	VFMADDPS ymm12, ymm12, ymm5, ymm14
	VMOVAPS ymm4, [byte rsi + 96]
	VFMADDPS ymm13, ymm13, ymm4, ymm14
	VMOVAPS ymm3, [dword rsi + 128]
	VFMADDPS ymm8, ymm8, ymm3, ymm14
	SUB rax, 4
	CMP rax, rdi
	JB .batch_polevl_finish
	align 16
	.batch_polevl_next:
	VBROADCASTSS ymm14, [rax]
	VFMADDPS xmm9, xmm9, xmm15, xmm14
	VFMADDPS xmm10, xmm10, xmm7, xmm14
	VFMADDPS ymm11, ymm11, ymm6, ymm14
	VFMADDPS ymm12, ymm12, ymm5, ymm14
	VFMADDPS ymm13, ymm13, ymm4, ymm14
	VFMADDPS ymm8, ymm8, ymm3, ymm14
	SUB rax, 4
	CMP rax, rdi
	JAE .batch_polevl_next
	.batch_polevl_finish:
	VMOVUPS [rdx], xmm9
	VMOVUPS [byte rdx + 16], xmm10
	VMOVUPS [byte rdx + 32], xmm11
	VEXTRACTF128 [byte rdx + 48], ymm11, 1
	VMOVUPS [byte rdx + 64], xmm12
	VEXTRACTF128 [byte rdx + 80], ymm12, 1
	VMOVUPS [byte rdx + 96], xmm13
	VEXTRACTF128 [byte rdx + 112], ymm13, 1
	VMOVUPS [dword rdx + 128], xmm8
	VEXTRACTF128 [dword rdx + 144], ymm8, 1
	ADD rsi, 160
	ADD rdx, 160
	SUB r8, 40
	JAE .process_batch_full
	.process_restore:
	ADD r8, 40
	JZ .return_ok
	.process_single:
	VMOVSS xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 4 - 4]
	VMOVSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	VFMADDSS xmm9, xmm9, xmm8, [rax]
	SUB rax, 4
	CMP rax, rdi
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	VMOVSS [rdx], xmm9
	ADD rsi, 4
	ADD rdx, 4
	SUB r8, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepMath_EvaluatePolynomial_V32fV32f_V32f_SandyBridge
_yepMath_EvaluatePolynomial_V32fV32f_V32f_SandyBridge:
%else
section .text
global __yepMath_EvaluatePolynomial_V32fV32f_V32f_SandyBridge
__yepMath_EvaluatePolynomial_V32fV32f_V32f_SandyBridge:
%endif
	.ENTRY:
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_ok
	TEST rsi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 4 - 4]
	VMOVSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	VMULSS xmm9, xmm9, xmm8
	VADDSS xmm9, xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	VMOVSS [rdx], xmm9
	ADD rsi, 4
	ADD rdx, 4
	SUB r8, 1
	JZ .return_ok
	TEST rsi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 64
	JB .process_restore
	align 32
	.process_batch_full:
	LEA rax, [byte rdi + rcx * 4 - 4]
	VBROADCASTSS ymm8, [rax]
	VMOVAPS ymm9, ymm8
	VMOVAPS ymm10, ymm8
	VMOVAPS ymm11, ymm8
	VMOVAPS ymm12, ymm8
	VMOVAPS ymm13, ymm8
	VMOVAPS ymm14, ymm8
	VMOVAPS ymm15, ymm8
	SUB rax, 4
	CMP rax, rdi
	JB .batch_polevl_finish
	VBROADCASTSS ymm7, [rax]
	VMOVAPS ymm6, [rsi]
	VMULPS ymm8, ymm8, ymm6
	VADDPS ymm8, ymm8, ymm7
	VMOVAPS ymm5, [byte rsi + 32]
	VMULPS ymm9, ymm9, ymm5
	VADDPS ymm9, ymm9, ymm7
	VMOVAPS ymm4, [byte rsi + 64]
	VMULPS ymm10, ymm10, ymm4
	VADDPS ymm10, ymm10, ymm7
	VMULPS ymm11, ymm11, [byte rsi + 96]
	VADDPS ymm11, ymm11, ymm7
	VMOVAPS ymm3, [dword rsi + 128]
	VMULPS ymm12, ymm12, ymm3
	VADDPS ymm12, ymm12, ymm7
	VMOVAPS ymm2, [dword rsi + 160]
	VMULPS ymm13, ymm13, ymm2
	VADDPS ymm13, ymm13, ymm7
	VMOVAPS ymm1, [dword rsi + 192]
	VMULPS ymm14, ymm14, ymm1
	VADDPS ymm14, ymm14, ymm7
	VMOVAPS ymm0, [dword rsi + 224]
	VMULPS ymm15, ymm15, ymm0
	VADDPS ymm15, ymm15, ymm7
	SUB rax, 4
	CMP rax, rdi
	JB .batch_polevl_finish
	.batch_polevl_next:
	VBROADCASTSS ymm7, [rax]
	VMULPS ymm8, ymm8, ymm6
	VADDPS ymm8, ymm8, ymm7
	VMULPS ymm9, ymm9, ymm5
	VADDPS ymm9, ymm9, ymm7
	VMULPS ymm10, ymm10, ymm4
	VADDPS ymm10, ymm10, ymm7
	VMULPS ymm11, ymm11, [byte rsi + 96]
	VADDPS ymm11, ymm11, ymm7
	VMULPS ymm12, ymm12, ymm3
	VADDPS ymm12, ymm12, ymm7
	VMULPS ymm13, ymm13, ymm2
	VADDPS ymm13, ymm13, ymm7
	VMULPS ymm14, ymm14, ymm1
	VADDPS ymm14, ymm14, ymm7
	VMULPS ymm15, ymm15, ymm0
	VADDPS ymm15, ymm15, ymm7
	SUB rax, 4
	CMP rax, rdi
	JAE .batch_polevl_next
	.batch_polevl_finish:
	VMOVUPS [rdx], ymm8
	VMOVUPS [byte rdx + 32], ymm9
	VMOVUPS [byte rdx + 64], ymm10
	VMOVUPS [byte rdx + 96], ymm11
	VMOVUPS [dword rdx + 128], ymm12
	VMOVUPS [dword rdx + 160], ymm13
	VMOVUPS [dword rdx + 192], ymm14
	VMOVUPS [dword rdx + 224], ymm15
	ADD rsi, 256
	ADD rdx, 256
	SUB r8, 64
	JAE .process_batch_full
	.process_restore:
	ADD r8, 64
	JZ .return_ok
	.process_single:
	VMOVSS xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 4 - 4]
	VMOVSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	VMULSS xmm9, xmm9, xmm8
	VADDSS xmm9, xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	VMOVSS [rdx], xmm9
	ADD rsi, 4
	ADD rdx, 4
	SUB r8, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepMath_EvaluatePolynomial_V32fV32f_V32f_Haswell
_yepMath_EvaluatePolynomial_V32fV32f_V32f_Haswell:
%else
section .text
global __yepMath_EvaluatePolynomial_V32fV32f_V32f_Haswell
__yepMath_EvaluatePolynomial_V32fV32f_V32f_Haswell:
%endif
	.ENTRY:
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 3
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_ok
	TEST rsi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 4 - 4]
	VMOVSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	VFMADD213SS xmm9, xmm8, [rax]
	SUB rax, 4
	CMP rax, rdi
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	VMOVSS [rdx], xmm9
	ADD rsi, 4
	ADD rdx, 4
	SUB r8, 1
	JZ .return_ok
	TEST rsi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 80
	JB .process_restore
	align 32
	.process_batch_full:
	LEA rax, [byte rdi + rcx * 4 - 4]
	VBROADCASTSS ymm8, [rax]
	VMOVAPS ymm9, ymm8
	VMOVAPS ymm10, ymm8
	VMOVAPS ymm11, ymm8
	VMOVAPS ymm12, ymm8
	VMOVAPS ymm13, ymm8
	VMOVAPS ymm14, ymm8
	VMOVAPS ymm15, ymm8
	VMOVAPS ymm7, ymm8
	VMOVAPS ymm6, ymm8
	PREFETCHNTA [dword rsi + 768]
	PREFETCHNTA [dword rsi + 800]
	PREFETCHNTA [dword rsi + 832]
	SUB rax, 4
	CMP rax, rdi
	JB .batch_polevl_finish
	VBROADCASTSS ymm5, [rax]
	VMOVAPS ymm4, [rsi]
	VFMADD132PS ymm8, ymm5, ymm4
	VFMADD132PS ymm9, ymm5, [byte rsi + 32]
	VMOVAPS ymm3, [byte rsi + 64]
	VFMADD132PS ymm10, ymm5, ymm3
	VFMADD132PS ymm11, ymm5, [byte rsi + 96]
	VMOVAPS ymm2, [dword rsi + 128]
	VFMADD132PS ymm12, ymm5, ymm2
	VFMADD132PS ymm13, ymm5, [dword rsi + 160]
	VMOVAPS ymm1, [dword rsi + 192]
	VFMADD132PS ymm14, ymm5, ymm1
	VFMADD132PS ymm15, ymm5, [dword rsi + 224]
	VMOVAPS ymm0, [dword rsi + 256]
	VFMADD132PS ymm7, ymm5, ymm0
	VFMADD132PS ymm6, ymm5, [dword rsi + 288]
	SUB rax, 4
	CMP rax, rdi
	JB .batch_polevl_finish
	align 16
	.batch_polevl_next:
	VBROADCASTSS ymm5, [rax]
	VFMADD132PS ymm8, ymm5, ymm4
	VFMADD132PS ymm9, ymm5, [byte rsi + 32]
	VFMADD132PS ymm10, ymm5, ymm3
	VFMADD132PS ymm11, ymm5, [byte rsi + 96]
	VFMADD132PS ymm12, ymm5, ymm2
	VFMADD132PS ymm13, ymm5, [dword rsi + 160]
	VFMADD132PS ymm14, ymm5, ymm1
	VFMADD132PS ymm15, ymm5, [dword rsi + 224]
	VFMADD132PS ymm7, ymm5, ymm0
	VFMADD132PS ymm6, ymm5, [dword rsi + 288]
	SUB rax, 4
	CMP rax, rdi
	JAE .batch_polevl_next
	.batch_polevl_finish:
	VMOVUPS [rdx], ymm8
	VMOVUPS [byte rdx + 32], ymm9
	VMOVUPS [byte rdx + 64], ymm10
	VMOVUPS [byte rdx + 96], ymm11
	VMOVUPS [dword rdx + 128], ymm12
	VMOVUPS [dword rdx + 160], ymm13
	VMOVUPS [dword rdx + 192], ymm14
	VMOVUPS [dword rdx + 224], ymm15
	VMOVUPS [dword rdx + 256], ymm7
	VMOVUPS [dword rdx + 288], ymm6
	ADD rsi, 320
	ADD rdx, 320
	SUB r8, 80
	JAE .process_batch_full
	.process_restore:
	ADD r8, 80
	JZ .return_ok
	.process_single:
	VMOVSS xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 4 - 4]
	VMOVSS xmm9, [rax]
	SUB rax, 4
	CMP rax, rdi
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	VFMADD213SS xmm9, xmm8, [rax]
	SUB rax, 4
	CMP rax, rdi
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	VMOVSS [rdx], xmm9
	ADD rsi, 4
	ADD rdx, 4
	SUB r8, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Unknown progbits alloc exec nowrite align=16
global _V64fV64f_V64f_Unknown
_V64fV64f_V64f_Unknown:
%else
section .text
global __V64fV64f_V64f_Unknown
__V64fV64f_V64f_Unknown:
%endif
	.ENTRY:
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_ok
	TEST rsi, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSD xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 8 - 8]
	MOVSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	MULSD xmm9, xmm8
	ADDSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	MOVSD [rdx], xmm9
	ADD rsi, 8
	ADD rdx, 8
	SUB r8, 1
	JZ .return_ok
	TEST rsi, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB r8, 12
	JB .process_restore
	align 32
	.process_batch_full:
	LEA rax, [byte rdi + rcx * 8 - 8]
	MOVDDUP xmm8, [rax]
	MOVAPD xmm9, xmm8
	MOVAPD xmm10, xmm8
	MOVAPD xmm11, xmm8
	MOVAPD xmm12, xmm8
	MOVAPD xmm13, xmm8
	SUB rax, 8
	CMP rax, rdi
	JB .batch_polevl_finish
	MOVDDUP xmm14, [rax]
	MOVAPD xmm15, [rsi]
	MULPD xmm8, xmm15
	ADDPD xmm8, xmm14
	MOVAPD xmm7, [byte rsi + 16]
	MULPD xmm9, xmm7
	ADDPD xmm9, xmm14
	MOVAPD xmm6, [byte rsi + 32]
	MULPD xmm10, xmm6
	ADDPD xmm10, xmm14
	MOVAPD xmm5, [byte rsi + 48]
	MULPD xmm11, xmm5
	ADDPD xmm11, xmm14
	MOVAPD xmm4, [byte rsi + 64]
	MULPD xmm12, xmm4
	ADDPD xmm12, xmm14
	MOVAPD xmm3, [byte rsi + 80]
	MULPD xmm13, xmm3
	ADDPD xmm13, xmm14
	SUB rax, 8
	CMP rax, rdi
	JB .batch_polevl_finish
	.batch_polevl_next:
	MOVDDUP xmm14, [rax]
	MULPD xmm8, xmm15
	ADDPD xmm8, xmm14
	MULPD xmm9, xmm7
	ADDPD xmm9, xmm14
	MULPD xmm10, xmm6
	ADDPD xmm10, xmm14
	MULPD xmm11, xmm5
	ADDPD xmm11, xmm14
	MULPD xmm12, xmm4
	ADDPD xmm12, xmm14
	MULPD xmm13, xmm3
	ADDPD xmm13, xmm14
	SUB rax, 8
	CMP rax, rdi
	JAE .batch_polevl_next
	.batch_polevl_finish:
	MOVUPD [rdx], xmm8
	MOVUPD [byte rdx + 16], xmm9
	MOVUPD [byte rdx + 32], xmm10
	MOVUPD [byte rdx + 48], xmm11
	MOVUPD [byte rdx + 64], xmm12
	MOVUPD [byte rdx + 80], xmm13
	ADD rsi, 96
	ADD rdx, 96
	SUB r8, 12
	JAE .process_batch_full
	.process_restore:
	ADD r8, 12
	JZ .return_ok
	.process_single:
	MOVSD xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 8 - 8]
	MOVSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	MULSD xmm9, xmm8
	ADDSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	MOVSD [rdx], xmm9
	ADD rsi, 8
	ADD rdx, 8
	SUB r8, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Nehalem progbits alloc exec nowrite align=16
global _yepMath_EvaluatePolynomial_V64fV64f_V64f_Nehalem
_yepMath_EvaluatePolynomial_V64fV64f_V64f_Nehalem:
%else
section .text
global __yepMath_EvaluatePolynomial_V64fV64f_V64f_Nehalem
__yepMath_EvaluatePolynomial_V64fV64f_V64f_Nehalem:
%endif
	.ENTRY:
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_ok
	TEST rsi, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSD xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 8 - 8]
	MOVSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	MULSD xmm9, xmm8
	ADDSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	MOVSD [rdx], xmm9
	ADD rsi, 8
	ADD rdx, 8
	SUB r8, 1
	JZ .return_ok
	TEST rsi, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB r8, 20
	JB .process_restore
	align 32
	.process_batch_full:
	LEA rax, [byte rdi + rcx * 8 - 8]
	MOVDDUP xmm8, [rax]
	MOVAPD xmm9, xmm8
	MOVAPD xmm10, xmm8
	MOVAPD xmm11, xmm8
	MOVAPD xmm12, xmm8
	MOVAPD xmm13, xmm8
	MOVAPD xmm14, xmm8
	MOVAPD xmm15, xmm8
	MOVAPD xmm7, xmm8
	MOVAPD xmm6, xmm8
	SUB rax, 8
	CMP rax, rdi
	JB .batch_polevl_finish
	MOVDDUP xmm5, [rax]
	MOVAPD xmm4, [rsi]
	MULPD xmm8, xmm4
	ADDPD xmm8, xmm5
	MULPD xmm9, [byte rsi + 16]
	ADDPD xmm9, xmm5
	MOVAPD xmm3, [byte rsi + 32]
	MULPD xmm10, xmm3
	ADDPD xmm10, xmm5
	MULPD xmm11, [byte rsi + 48]
	ADDPD xmm11, xmm5
	MOVAPD xmm2, [byte rsi + 64]
	MULPD xmm12, xmm2
	ADDPD xmm12, xmm5
	MULPD xmm13, [byte rsi + 80]
	ADDPD xmm13, xmm5
	MOVAPD xmm1, [byte rsi + 96]
	MULPD xmm14, xmm1
	ADDPD xmm14, xmm5
	MULPD xmm15, [byte rsi + 112]
	ADDPD xmm15, xmm5
	MOVAPD xmm0, [dword rsi + 128]
	MULPD xmm7, xmm0
	ADDPD xmm7, xmm5
	MULPD xmm6, [dword rsi + 144]
	ADDPD xmm6, xmm5
	SUB rax, 8
	CMP rax, rdi
	JB .batch_polevl_finish
	.batch_polevl_next:
	MOVDDUP xmm5, [rax]
	MULPD xmm8, xmm4
	ADDPD xmm8, xmm5
	MULPD xmm9, [byte rsi + 16]
	ADDPD xmm9, xmm5
	MULPD xmm10, xmm3
	ADDPD xmm10, xmm5
	MULPD xmm11, [byte rsi + 48]
	ADDPD xmm11, xmm5
	MULPD xmm12, xmm2
	ADDPD xmm12, xmm5
	MULPD xmm13, [byte rsi + 80]
	ADDPD xmm13, xmm5
	MULPD xmm14, xmm1
	ADDPD xmm14, xmm5
	MULPD xmm15, [byte rsi + 112]
	ADDPD xmm15, xmm5
	MULPD xmm7, xmm0
	ADDPD xmm7, xmm5
	MULPD xmm6, [dword rsi + 144]
	ADDPD xmm6, xmm5
	SUB rax, 8
	CMP rax, rdi
	JAE .batch_polevl_next
	.batch_polevl_finish:
	MOVUPD [rdx], xmm8
	MOVUPD [byte rdx + 16], xmm9
	MOVUPD [byte rdx + 32], xmm10
	MOVUPD [byte rdx + 48], xmm11
	MOVUPD [byte rdx + 64], xmm12
	MOVUPD [byte rdx + 80], xmm13
	MOVUPD [byte rdx + 96], xmm14
	MOVUPD [byte rdx + 112], xmm15
	MOVUPD [dword rdx + 128], xmm7
	MOVUPD [dword rdx + 144], xmm6
	ADD rsi, 160
	ADD rdx, 160
	SUB r8, 20
	JAE .process_batch_full
	.process_restore:
	ADD r8, 20
	JZ .return_ok
	.process_single:
	MOVSD xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 8 - 8]
	MOVSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	MULSD xmm9, xmm8
	ADDSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	MOVSD [rdx], xmm9
	ADD rsi, 8
	ADD rdx, 8
	SUB r8, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Bonnell progbits alloc exec nowrite align=16
global _yepMath_EvaluatePolynomial_V64fV64f_V64f_Bonnell
_yepMath_EvaluatePolynomial_V64fV64f_V64f_Bonnell:
%else
section .text
global __yepMath_EvaluatePolynomial_V64fV64f_V64f_Bonnell
__yepMath_EvaluatePolynomial_V64fV64f_V64f_Bonnell:
%endif
	.ENTRY:
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_ok
	TEST rsi, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSD xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 8 - 8]
	MOVSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	MULSD xmm9, xmm8
	ADDSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	MOVSD [rdx], xmm9
	ADD rsi, 8
	ADD rdx, 8
	SUB r8, 1
	JZ .return_ok
	TEST rsi, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB r8, 14
	JB .process_restore
	align 32
	.process_batch_full:
	LEA rax, [byte rdi + rcx * 8 - 8]
	MOVSD xmm8, [rax]
	MOVSD xmm9, [rsi]
	MOVAPS xmm10, xmm8
	MOVAPS xmm11, xmm8
	MOVAPS xmm12, xmm8
	MOVAPS xmm13, xmm8
	MOVAPS xmm14, xmm8
	MOVAPS xmm15, xmm8
	MOVAPS xmm7, xmm8
	MOVAPS xmm6, xmm8
	MOVAPS xmm5, xmm8
	MOVAPS xmm4, xmm8
	MOVAPS xmm3, xmm8
	MOVAPS xmm2, xmm8
	MOVAPS xmm1, xmm8
	SUB rax, 8
	CMP rax, rdi
	JB .batch_polevl_finish
	.batch_polevl_next:
	MOVSD xmm0, [rax]
	MULSD xmm8, xmm9
	MULSD xmm10, [byte rsi + 8]
	MULSD xmm11, [byte rsi + 16]
	ADDSD xmm8, xmm0
	MULSD xmm12, [byte rsi + 24]
	ADDSD xmm10, xmm0
	MULSD xmm13, [byte rsi + 32]
	ADDSD xmm11, xmm0
	MULSD xmm14, [byte rsi + 40]
	ADDSD xmm12, xmm0
	MULSD xmm15, [byte rsi + 48]
	ADDSD xmm13, xmm0
	MULSD xmm7, [byte rsi + 56]
	ADDSD xmm14, xmm0
	MULSD xmm6, [byte rsi + 64]
	ADDSD xmm15, xmm0
	MULSD xmm5, [byte rsi + 72]
	ADDSD xmm7, xmm0
	MULSD xmm4, [byte rsi + 80]
	ADDSD xmm6, xmm0
	MULSD xmm3, [byte rsi + 88]
	ADDSD xmm5, xmm0
	MULSD xmm2, [byte rsi + 96]
	ADDSD xmm4, xmm0
	MULSD xmm1, [byte rsi + 104]
	ADDSD xmm3, xmm0
	SUB rax, 8
	ADDSD xmm2, xmm0
	CMP rax, rdi
	ADDSD xmm1, xmm0
	JAE .batch_polevl_next
	.batch_polevl_finish:
	MOVSD [rdx], xmm8
	MOVSD [byte rdx + 8], xmm10
	MOVSD [byte rdx + 16], xmm11
	MOVSD [byte rdx + 24], xmm12
	MOVSD [byte rdx + 32], xmm13
	MOVSD [byte rdx + 40], xmm14
	MOVSD [byte rdx + 48], xmm15
	MOVSD [byte rdx + 56], xmm7
	MOVSD [byte rdx + 64], xmm6
	MOVSD [byte rdx + 72], xmm5
	MOVSD [byte rdx + 80], xmm4
	MOVSD [byte rdx + 88], xmm3
	MOVSD [byte rdx + 96], xmm2
	MOVSD [byte rdx + 104], xmm1
	ADD rsi, 112
	ADD rdx, 112
	SUB r8, 14
	JAE .process_batch_full
	.process_restore:
	ADD r8, 14
	JZ .return_ok
	.process_single:
	MOVSD xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 8 - 8]
	MOVSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	MULSD xmm9, xmm8
	ADDSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	MOVSD [rdx], xmm9
	ADD rsi, 8
	ADD rdx, 8
	SUB r8, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Bulldozer progbits alloc exec nowrite align=16
global _yepMath_EvaluatePolynomial_V64fV64f_V64f_Bulldozer
_yepMath_EvaluatePolynomial_V64fV64f_V64f_Bulldozer:
%else
section .text
global __yepMath_EvaluatePolynomial_V64fV64f_V64f_Bulldozer
__yepMath_EvaluatePolynomial_V64fV64f_V64f_Bulldozer:
%endif
	.ENTRY:
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_ok
	TEST rsi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 8 - 8]
	VMOVSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	VFMADDSD xmm9, xmm9, xmm8, [rax]
	SUB rax, 8
	CMP rax, rdi
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	VMOVSD [rdx], xmm9
	ADD rsi, 8
	ADD rdx, 8
	SUB r8, 1
	JZ .return_ok
	TEST rsi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 20
	JB .process_restore
	align 32
	.process_batch_full:
	LEA rax, [byte rdi + rcx * 8 - 8]
	VBROADCASTSD ymm8, [rax]
	VMOVAPD xmm9, xmm8
	VMOVAPD xmm10, xmm8
	VMOVAPD ymm11, ymm8
	VMOVAPD ymm12, ymm8
	VMOVAPD ymm13, ymm8
	SUB rax, 8
	CMP rax, rdi
	JB .batch_polevl_finish
	VBROADCASTSD ymm14, [rax]
	VMOVAPD xmm15, [rsi]
	VFMADDPD xmm9, xmm9, xmm15, xmm14
	VMOVAPD xmm7, [byte rsi + 16]
	VFMADDPD xmm10, xmm10, xmm7, xmm14
	VMOVAPD ymm6, [byte rsi + 32]
	VFMADDPD ymm11, ymm11, ymm6, ymm14
	VMOVAPD ymm5, [byte rsi + 64]
	VFMADDPD ymm12, ymm12, ymm5, ymm14
	VMOVAPD ymm4, [byte rsi + 96]
	VFMADDPD ymm13, ymm13, ymm4, ymm14
	VMOVAPD ymm3, [dword rsi + 128]
	VFMADDPD ymm8, ymm8, ymm3, ymm14
	SUB rax, 8
	CMP rax, rdi
	JB .batch_polevl_finish
	align 16
	.batch_polevl_next:
	VBROADCASTSD ymm14, [rax]
	VFMADDPD xmm9, xmm9, xmm15, xmm14
	VFMADDPD xmm10, xmm10, xmm7, xmm14
	VFMADDPD ymm11, ymm11, ymm6, ymm14
	VFMADDPD ymm12, ymm12, ymm5, ymm14
	VFMADDPD ymm13, ymm13, ymm4, ymm14
	VFMADDPD ymm8, ymm8, ymm3, ymm14
	SUB rax, 8
	CMP rax, rdi
	JAE .batch_polevl_next
	.batch_polevl_finish:
	VMOVUPD [rdx], xmm9
	VMOVUPD [byte rdx + 16], xmm10
	VMOVUPD [byte rdx + 32], xmm11
	VEXTRACTF128 [byte rdx + 48], ymm11, 1
	VMOVUPD [byte rdx + 64], xmm12
	VEXTRACTF128 [byte rdx + 80], ymm12, 1
	VMOVUPD [byte rdx + 96], xmm13
	VEXTRACTF128 [byte rdx + 112], ymm13, 1
	VMOVUPD [dword rdx + 128], xmm8
	VEXTRACTF128 [dword rdx + 144], ymm8, 1
	ADD rsi, 160
	ADD rdx, 160
	SUB r8, 20
	JAE .process_batch_full
	.process_restore:
	ADD r8, 20
	JZ .return_ok
	.process_single:
	VMOVSD xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 8 - 8]
	VMOVSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	VFMADDSD xmm9, xmm9, xmm8, [rax]
	SUB rax, 8
	CMP rax, rdi
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	VMOVSD [rdx], xmm9
	ADD rsi, 8
	ADD rdx, 8
	SUB r8, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.SandyBridge progbits alloc exec nowrite align=16
global _yepMath_EvaluatePolynomial_V64fV64f_V64f_SandyBridge
_yepMath_EvaluatePolynomial_V64fV64f_V64f_SandyBridge:
%else
section .text
global __yepMath_EvaluatePolynomial_V64fV64f_V64f_SandyBridge
__yepMath_EvaluatePolynomial_V64fV64f_V64f_SandyBridge:
%endif
	.ENTRY:
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_ok
	TEST rsi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 8 - 8]
	VMOVSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	VMULSD xmm9, xmm9, xmm8
	VADDSD xmm9, xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	VMOVSD [rdx], xmm9
	ADD rsi, 8
	ADD rdx, 8
	SUB r8, 1
	JZ .return_ok
	TEST rsi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 32
	JB .process_restore
	align 32
	.process_batch_full:
	LEA rax, [byte rdi + rcx * 8 - 8]
	VBROADCASTSD ymm8, [rax]
	VMOVAPD ymm9, ymm8
	VMOVAPD ymm10, ymm8
	VMOVAPD ymm11, ymm8
	VMOVAPD ymm12, ymm8
	VMOVAPD ymm13, ymm8
	VMOVAPD ymm14, ymm8
	VMOVAPD ymm15, ymm8
	SUB rax, 8
	CMP rax, rdi
	JB .batch_polevl_finish
	VBROADCASTSD ymm7, [rax]
	VMOVAPD ymm6, [rsi]
	VMULPD ymm8, ymm8, ymm6
	VADDPD ymm8, ymm8, ymm7
	VMOVAPD ymm5, [byte rsi + 32]
	VMULPD ymm9, ymm9, ymm5
	VADDPD ymm9, ymm9, ymm7
	VMOVAPD ymm4, [byte rsi + 64]
	VMULPD ymm10, ymm10, ymm4
	VADDPD ymm10, ymm10, ymm7
	VMULPD ymm11, ymm11, [byte rsi + 96]
	VADDPD ymm11, ymm11, ymm7
	VMOVAPD ymm3, [dword rsi + 128]
	VMULPD ymm12, ymm12, ymm3
	VADDPD ymm12, ymm12, ymm7
	VMOVAPD ymm2, [dword rsi + 160]
	VMULPD ymm13, ymm13, ymm2
	VADDPD ymm13, ymm13, ymm7
	VMOVAPD ymm1, [dword rsi + 192]
	VMULPD ymm14, ymm14, ymm1
	VADDPD ymm14, ymm14, ymm7
	VMOVAPD ymm0, [dword rsi + 224]
	VMULPD ymm15, ymm15, ymm0
	VADDPD ymm15, ymm15, ymm7
	SUB rax, 8
	CMP rax, rdi
	JB .batch_polevl_finish
	.batch_polevl_next:
	VBROADCASTSD ymm7, [rax]
	VMULPD ymm8, ymm8, ymm6
	VADDPD ymm8, ymm8, ymm7
	VMULPD ymm9, ymm9, ymm5
	VADDPD ymm9, ymm9, ymm7
	VMULPD ymm10, ymm10, ymm4
	VADDPD ymm10, ymm10, ymm7
	VMULPD ymm11, ymm11, [byte rsi + 96]
	VADDPD ymm11, ymm11, ymm7
	VMULPD ymm12, ymm12, ymm3
	VADDPD ymm12, ymm12, ymm7
	VMULPD ymm13, ymm13, ymm2
	VADDPD ymm13, ymm13, ymm7
	VMULPD ymm14, ymm14, ymm1
	VADDPD ymm14, ymm14, ymm7
	VMULPD ymm15, ymm15, ymm0
	VADDPD ymm15, ymm15, ymm7
	SUB rax, 8
	CMP rax, rdi
	JAE .batch_polevl_next
	.batch_polevl_finish:
	VMOVUPD [rdx], ymm8
	VMOVUPD [byte rdx + 32], ymm9
	VMOVUPD [byte rdx + 64], ymm10
	VMOVUPD [byte rdx + 96], ymm11
	VMOVUPD [dword rdx + 128], ymm12
	VMOVUPD [dword rdx + 160], ymm13
	VMOVUPD [dword rdx + 192], ymm14
	VMOVUPD [dword rdx + 224], ymm15
	ADD rsi, 256
	ADD rdx, 256
	SUB r8, 32
	JAE .process_batch_full
	.process_restore:
	ADD r8, 32
	JZ .return_ok
	.process_single:
	VMOVSD xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 8 - 8]
	VMOVSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	VMULSD xmm9, xmm9, xmm8
	VADDSD xmm9, xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	VMOVSD [rdx], xmm9
	ADD rsi, 8
	ADD rdx, 8
	SUB r8, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

%ifidn __OUTPUT_FORMAT__, elf64
section .text.Haswell progbits alloc exec nowrite align=16
global _yepMath_EvaluatePolynomial_V64fV64f_V64f_Haswell
_yepMath_EvaluatePolynomial_V64fV64f_V64f_Haswell:
%else
section .text
global __yepMath_EvaluatePolynomial_V64fV64f_V64f_Haswell
__yepMath_EvaluatePolynomial_V64fV64f_V64f_Haswell:
%endif
	.ENTRY:
	TEST rsi, rsi
	JZ .return_null_pointer
	TEST rsi, 7
	JNZ .return_misaligned_pointer
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_ok
	TEST rsi, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 8 - 8]
	VMOVSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	VFMADD213SD xmm9, xmm8, [rax]
	SUB rax, 8
	CMP rax, rdi
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	VMOVSD [rdx], xmm9
	ADD rsi, 8
	ADD rdx, 8
	SUB r8, 1
	JZ .return_ok
	TEST rsi, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB r8, 40
	JB .process_restore
	align 32
	.process_batch_full:
	LEA rax, [byte rdi + rcx * 8 - 8]
	VBROADCASTSD ymm8, [rax]
	VMOVAPD ymm9, ymm8
	VMOVAPD ymm10, ymm8
	VMOVAPD ymm11, ymm8
	VMOVAPD ymm12, ymm8
	VMOVAPD ymm13, ymm8
	VMOVAPD ymm14, ymm8
	VMOVAPD ymm15, ymm8
	VMOVAPD ymm7, ymm8
	VMOVAPD ymm6, ymm8
	PREFETCHNTA [dword rsi + 768]
	PREFETCHNTA [dword rsi + 800]
	PREFETCHNTA [dword rsi + 832]
	SUB rax, 8
	CMP rax, rdi
	JB .batch_polevl_finish
	VBROADCASTSD ymm5, [rax]
	VMOVAPD ymm4, [rsi]
	VFMADD132PD ymm8, ymm5, ymm4
	VFMADD132PD ymm9, ymm5, [byte rsi + 32]
	VMOVAPD ymm3, [byte rsi + 64]
	VFMADD132PD ymm10, ymm5, ymm3
	VFMADD132PD ymm11, ymm5, [byte rsi + 96]
	VMOVAPD ymm2, [dword rsi + 128]
	VFMADD132PD ymm12, ymm5, ymm2
	VFMADD132PD ymm13, ymm5, [dword rsi + 160]
	VMOVAPD ymm1, [dword rsi + 192]
	VFMADD132PD ymm14, ymm5, ymm1
	VFMADD132PD ymm15, ymm5, [dword rsi + 224]
	VMOVAPD ymm0, [dword rsi + 256]
	VFMADD132PD ymm7, ymm5, ymm0
	VFMADD132PD ymm6, ymm5, [dword rsi + 288]
	SUB rax, 8
	CMP rax, rdi
	JB .batch_polevl_finish
	align 16
	.batch_polevl_next:
	VBROADCASTSD ymm5, [rax]
	VFMADD132PD ymm8, ymm5, ymm4
	VFMADD132PD ymm9, ymm5, [byte rsi + 32]
	VFMADD132PD ymm10, ymm5, ymm3
	VFMADD132PD ymm11, ymm5, [byte rsi + 96]
	VFMADD132PD ymm12, ymm5, ymm2
	VFMADD132PD ymm13, ymm5, [dword rsi + 160]
	VFMADD132PD ymm14, ymm5, ymm1
	VFMADD132PD ymm15, ymm5, [dword rsi + 224]
	VFMADD132PD ymm7, ymm5, ymm0
	VFMADD132PD ymm6, ymm5, [dword rsi + 288]
	SUB rax, 8
	CMP rax, rdi
	JAE .batch_polevl_next
	.batch_polevl_finish:
	VMOVUPD [rdx], ymm8
	VMOVUPD [byte rdx + 32], ymm9
	VMOVUPD [byte rdx + 64], ymm10
	VMOVUPD [byte rdx + 96], ymm11
	VMOVUPD [dword rdx + 128], ymm12
	VMOVUPD [dword rdx + 160], ymm13
	VMOVUPD [dword rdx + 192], ymm14
	VMOVUPD [dword rdx + 224], ymm15
	VMOVUPD [dword rdx + 256], ymm7
	VMOVUPD [dword rdx + 288], ymm6
	ADD rsi, 320
	ADD rdx, 320
	SUB r8, 40
	JAE .process_batch_full
	.process_restore:
	ADD r8, 40
	JZ .return_ok
	.process_single:
	VMOVSD xmm8, [rsi]
	LEA rax, [byte rdi + rcx * 8 - 8]
	VMOVSD xmm9, [rax]
	SUB rax, 8
	CMP rax, rdi
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	VFMADD213SD xmm9, xmm8, [rax]
	SUB rax, 8
	CMP rax, rdi
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	VMOVSD [rdx], xmm9
	ADD rsi, 8
	ADD rdx, 8
	SUB r8, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return
