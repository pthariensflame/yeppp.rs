;                       Yeppp! library implementation
;                   This file is auto-generated by Peach-Py,
;        Portable Efficient Assembly Code-generator in Higher-level Python,
;                  part of the Yeppp! library infrastructure
; This file is part of Yeppp! library and licensed under the New BSD license.
; See LICENSE.txt for the full text of the license.

section .text$a code align=16
global _V32fV32f_V32f_Unknown
_V32fV32f_V32f_Unknown:
	.ENTRY:
	SUB rsp, 120
	MOVAPS [rsp], xmm6
	MOVAPS [byte rsp + 16], xmm7
	MOVAPS [byte rsp + 32], xmm8
	MOVAPS [byte rsp + 48], xmm9
	MOVAPS [byte rsp + 64], xmm10
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm12
	MOV rax, [dword rsp + 160]
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST rax, rax
	JZ .return_ok
	TEST rdx, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSS xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 4 - 4]
	MOVSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	MULSS xmm5, xmm4
	ADDSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	MOVSS [r8], xmm5
	ADD rdx, 4
	ADD r8, 4
	SUB rax, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB rax, 24
	JB .process_restore
	align 32
	.process_batch_full:
	LEA r10, [byte rcx + r9 * 4 - 4]
	MOVSS xmm4, [r10]
	SHUFPS xmm4, xmm4, 0
	MOVAPS xmm5, xmm4
	MOVAPS xmm3, xmm4
	MOVAPS xmm2, xmm4
	MOVAPS xmm1, xmm4
	MOVAPS xmm0, xmm4
	SUB r10, 4
	CMP r10, rcx
	JB .batch_polevl_finish
	MOVSS xmm6, [r10]
	SHUFPS xmm6, xmm6, 0
	MOVAPS xmm7, [rdx]
	MULPS xmm4, xmm7
	ADDPS xmm4, xmm6
	MOVAPS xmm8, [byte rdx + 16]
	MULPS xmm5, xmm8
	ADDPS xmm5, xmm6
	MOVAPS xmm9, [byte rdx + 32]
	MULPS xmm3, xmm9
	ADDPS xmm3, xmm6
	MOVAPS xmm10, [byte rdx + 48]
	MULPS xmm2, xmm10
	ADDPS xmm2, xmm6
	MOVAPS xmm11, [byte rdx + 64]
	MULPS xmm1, xmm11
	ADDPS xmm1, xmm6
	MOVAPS xmm12, [byte rdx + 80]
	MULPS xmm0, xmm12
	ADDPS xmm0, xmm6
	SUB r10, 4
	CMP r10, rcx
	JB .batch_polevl_finish
	.batch_polevl_next:
	MOVSS xmm6, [r10]
	SHUFPS xmm6, xmm6, 0
	MULPS xmm4, xmm7
	ADDPS xmm4, xmm6
	MULPS xmm5, xmm8
	ADDPS xmm5, xmm6
	MULPS xmm3, xmm9
	ADDPS xmm3, xmm6
	MULPS xmm2, xmm10
	ADDPS xmm2, xmm6
	MULPS xmm1, xmm11
	ADDPS xmm1, xmm6
	MULPS xmm0, xmm12
	ADDPS xmm0, xmm6
	SUB r10, 4
	CMP r10, rcx
	JAE .batch_polevl_next
	.batch_polevl_finish:
	MOVUPS [r8], xmm4
	MOVUPS [byte r8 + 16], xmm5
	MOVUPS [byte r8 + 32], xmm3
	MOVUPS [byte r8 + 48], xmm2
	MOVUPS [byte r8 + 64], xmm1
	MOVUPS [byte r8 + 80], xmm0
	ADD rdx, 96
	ADD r8, 96
	SUB rax, 24
	JAE .process_batch_full
	.process_restore:
	ADD rax, 24
	JZ .return_ok
	.process_single:
	MOVSS xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 4 - 4]
	MOVSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	MULSS xmm5, xmm4
	ADDSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	MOVSS [r8], xmm5
	ADD rdx, 4
	ADD r8, 4
	SUB rax, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm6, [rsp]
	MOVAPS xmm7, [byte rsp + 16]
	MOVAPS xmm8, [byte rsp + 32]
	MOVAPS xmm9, [byte rsp + 48]
	MOVAPS xmm10, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm12, [byte rsp + 96]
	ADD rsp, 120
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepMath_EvaluatePolynomial_V32fV32f_V32f_Nehalem
_yepMath_EvaluatePolynomial_V32fV32f_V32f_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm6
	MOVAPS [byte rsp + 16], xmm7
	MOVAPS [byte rsp + 32], xmm8
	MOVAPS [byte rsp + 48], xmm9
	MOVAPS [byte rsp + 64], xmm10
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm12
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm14
	MOVAPS [dword rsp + 144], xmm15
	MOV rax, [dword rsp + 208]
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST rax, rax
	JZ .return_ok
	TEST rdx, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSS xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 4 - 4]
	MOVSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	MULSS xmm5, xmm4
	ADDSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	MOVSS [r8], xmm5
	ADD rdx, 4
	ADD r8, 4
	SUB rax, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB rax, 40
	JB .process_restore
	align 32
	.process_batch_full:
	LEA r10, [byte rcx + r9 * 4 - 4]
	MOVSS xmm4, [r10]
	SHUFPS xmm4, xmm4, 0
	MOVAPS xmm5, xmm4
	MOVAPS xmm3, xmm4
	MOVAPS xmm2, xmm4
	MOVAPS xmm1, xmm4
	MOVAPS xmm0, xmm4
	MOVAPS xmm6, xmm4
	MOVAPS xmm7, xmm4
	MOVAPS xmm8, xmm4
	MOVAPS xmm9, xmm4
	SUB r10, 4
	CMP r10, rcx
	JB .batch_polevl_finish
	MOVSS xmm10, [r10]
	SHUFPS xmm10, xmm10, 0
	MOVAPS xmm11, [rdx]
	MULPS xmm4, xmm11
	ADDPS xmm4, xmm10
	MULPS xmm5, [byte rdx + 16]
	ADDPS xmm5, xmm10
	MOVAPS xmm12, [byte rdx + 32]
	MULPS xmm3, xmm12
	ADDPS xmm3, xmm10
	MULPS xmm2, [byte rdx + 48]
	ADDPS xmm2, xmm10
	MOVAPS xmm13, [byte rdx + 64]
	MULPS xmm1, xmm13
	ADDPS xmm1, xmm10
	MULPS xmm0, [byte rdx + 80]
	ADDPS xmm0, xmm10
	MOVAPS xmm14, [byte rdx + 96]
	MULPS xmm6, xmm14
	ADDPS xmm6, xmm10
	MULPS xmm7, [byte rdx + 112]
	ADDPS xmm7, xmm10
	MOVAPS xmm15, [dword rdx + 128]
	MULPS xmm8, xmm15
	ADDPS xmm8, xmm10
	MULPS xmm9, [dword rdx + 144]
	ADDPS xmm9, xmm10
	SUB r10, 4
	CMP r10, rcx
	JB .batch_polevl_finish
	.batch_polevl_next:
	MOVSS xmm10, [r10]
	SHUFPS xmm10, xmm10, 0
	MULPS xmm4, xmm11
	ADDPS xmm4, xmm10
	MULPS xmm5, [byte rdx + 16]
	ADDPS xmm5, xmm10
	MULPS xmm3, xmm12
	ADDPS xmm3, xmm10
	MULPS xmm2, [byte rdx + 48]
	ADDPS xmm2, xmm10
	MULPS xmm1, xmm13
	ADDPS xmm1, xmm10
	MULPS xmm0, [byte rdx + 80]
	ADDPS xmm0, xmm10
	MULPS xmm6, xmm14
	ADDPS xmm6, xmm10
	MULPS xmm7, [byte rdx + 112]
	ADDPS xmm7, xmm10
	MULPS xmm8, xmm15
	ADDPS xmm8, xmm10
	MULPS xmm9, [dword rdx + 144]
	ADDPS xmm9, xmm10
	SUB r10, 4
	CMP r10, rcx
	JAE .batch_polevl_next
	.batch_polevl_finish:
	MOVUPS [r8], xmm4
	MOVUPS [byte r8 + 16], xmm5
	MOVUPS [byte r8 + 32], xmm3
	MOVUPS [byte r8 + 48], xmm2
	MOVUPS [byte r8 + 64], xmm1
	MOVUPS [byte r8 + 80], xmm0
	MOVUPS [byte r8 + 96], xmm6
	MOVUPS [byte r8 + 112], xmm7
	MOVUPS [dword r8 + 128], xmm8
	MOVUPS [dword r8 + 144], xmm9
	ADD rdx, 160
	ADD r8, 160
	SUB rax, 40
	JAE .process_batch_full
	.process_restore:
	ADD rax, 40
	JZ .return_ok
	.process_single:
	MOVSS xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 4 - 4]
	MOVSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	MULSS xmm5, xmm4
	ADDSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	MOVSS [r8], xmm5
	ADD rdx, 4
	ADD r8, 4
	SUB rax, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm6, [rsp]
	MOVAPS xmm7, [byte rsp + 16]
	MOVAPS xmm8, [byte rsp + 32]
	MOVAPS xmm9, [byte rsp + 48]
	MOVAPS xmm10, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm12, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm14, [dword rsp + 128]
	MOVAPS xmm15, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$i code align=16
global _yepMath_EvaluatePolynomial_V32fV32f_V32f_Bonnell
_yepMath_EvaluatePolynomial_V32fV32f_V32f_Bonnell:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm6
	MOVAPS [byte rsp + 16], xmm7
	MOVAPS [byte rsp + 32], xmm8
	MOVAPS [byte rsp + 48], xmm9
	MOVAPS [byte rsp + 64], xmm10
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm12
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm14
	MOVAPS [dword rsp + 144], xmm15
	MOV rax, [dword rsp + 208]
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST rax, rax
	JZ .return_ok
	TEST rdx, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSS xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 4 - 4]
	MOVSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	MULSS xmm5, xmm4
	ADDSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	MOVSS [r8], xmm5
	ADD rdx, 4
	ADD r8, 4
	SUB rax, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB rax, 56
	JB .process_restore
	align 32
	.process_batch_full:
	LEA r10, [byte rcx + r9 * 4 - 4]
	MOVSS xmm4, [r10]
	SHUFPS xmm4, xmm4, 0
	MOVAPS xmm5, [rdx]
	MOVAPS xmm3, xmm4
	MOVAPS xmm2, xmm4
	MOVAPS xmm1, xmm4
	MOVAPS xmm0, xmm4
	MOVAPS xmm6, xmm4
	MOVAPS xmm7, xmm4
	MOVAPS xmm8, xmm4
	MOVAPS xmm9, xmm4
	MOVAPS xmm10, xmm4
	MOVAPS xmm11, xmm4
	MOVAPS xmm12, xmm4
	MOVAPS xmm13, xmm4
	MOVAPS xmm14, xmm4
	SUB r10, 4
	CMP r10, rcx
	JB .batch_polevl_finish
	.batch_polevl_next:
	MOVSS xmm15, [r10]
	MULPS xmm4, xmm5
	SHUFPS xmm15, xmm15, 0
	MULPS xmm3, [byte rdx + 16]
	MULPS xmm2, [byte rdx + 32]
	ADDPS xmm4, xmm15
	MULPS xmm1, [byte rdx + 48]
	ADDPS xmm3, xmm15
	MULPS xmm0, [byte rdx + 64]
	ADDPS xmm2, xmm15
	MULPS xmm6, [byte rdx + 80]
	ADDPS xmm1, xmm15
	MULPS xmm7, [byte rdx + 96]
	ADDPS xmm0, xmm15
	MULPS xmm8, [byte rdx + 112]
	ADDPS xmm6, xmm15
	MULPS xmm9, [dword rdx + 128]
	ADDPS xmm7, xmm15
	MULPS xmm10, [dword rdx + 144]
	ADDPS xmm8, xmm15
	MULPS xmm11, [dword rdx + 160]
	ADDPS xmm9, xmm15
	MULPS xmm12, [dword rdx + 176]
	ADDPS xmm10, xmm15
	MULPS xmm13, [dword rdx + 192]
	ADDPS xmm11, xmm15
	MULPS xmm14, [dword rdx + 208]
	ADDPS xmm12, xmm15
	SUB r10, 4
	ADDPS xmm13, xmm15
	CMP r10, rcx
	ADDPS xmm14, xmm15
	JAE .batch_polevl_next
	.batch_polevl_finish:
	MOVUPS [r8], xmm4
	MOVUPS [byte r8 + 16], xmm3
	MOVUPS [byte r8 + 32], xmm2
	MOVUPS [byte r8 + 48], xmm1
	MOVUPS [byte r8 + 64], xmm0
	MOVUPS [byte r8 + 80], xmm6
	MOVUPS [byte r8 + 96], xmm7
	MOVUPS [byte r8 + 112], xmm8
	MOVUPS [dword r8 + 128], xmm9
	MOVUPS [dword r8 + 144], xmm10
	MOVUPS [dword r8 + 160], xmm11
	MOVUPS [dword r8 + 176], xmm12
	MOVUPS [dword r8 + 192], xmm13
	MOVUPS [dword r8 + 208], xmm14
	ADD rdx, 224
	ADD r8, 224
	SUB rax, 56
	JAE .process_batch_full
	.process_restore:
	ADD rax, 56
	JZ .return_ok
	.process_single:
	MOVSS xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 4 - 4]
	MOVSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	MULSS xmm5, xmm4
	ADDSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	MOVSS [r8], xmm5
	ADD rdx, 4
	ADD r8, 4
	SUB rax, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm6, [rsp]
	MOVAPS xmm7, [byte rsp + 16]
	MOVAPS xmm8, [byte rsp + 32]
	MOVAPS xmm9, [byte rsp + 48]
	MOVAPS xmm10, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm12, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm14, [dword rsp + 128]
	MOVAPS xmm15, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$n code align=16
global _yepMath_EvaluatePolynomial_V32fV32f_V32f_Bulldozer
_yepMath_EvaluatePolynomial_V32fV32f_V32f_Bulldozer:
	.ENTRY:
	SUB rsp, 120
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	MOV rax, [dword rsp + 160]
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST rax, rax
	JZ .return_ok
	TEST rdx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 4 - 4]
	VMOVSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	VFMADDSS xmm5, xmm5, xmm4, [r10]
	SUB r10, 4
	CMP r10, rcx
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	VMOVSS [r8], xmm5
	ADD rdx, 4
	ADD r8, 4
	SUB rax, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rax, 40
	JB .process_restore
	align 32
	.process_batch_full:
	LEA r10, [byte rcx + r9 * 4 - 4]
	VBROADCASTSS ymm4, [r10]
	VMOVAPS xmm5, xmm4
	VMOVAPS xmm3, xmm4
	VMOVAPS ymm2, ymm4
	VMOVAPS ymm1, ymm4
	VMOVAPS ymm0, ymm4
	SUB r10, 4
	CMP r10, rcx
	JB .batch_polevl_finish
	VBROADCASTSS ymm6, [r10]
	VMOVAPS xmm7, [rdx]
	VFMADDPS xmm5, xmm5, xmm7, xmm6
	VMOVAPS xmm8, [byte rdx + 16]
	VFMADDPS xmm3, xmm3, xmm8, xmm6
	VMOVAPS ymm9, [byte rdx + 32]
	VFMADDPS ymm2, ymm2, ymm9, ymm6
	VMOVAPS ymm10, [byte rdx + 64]
	VFMADDPS ymm1, ymm1, ymm10, ymm6
	VMOVAPS ymm11, [byte rdx + 96]
	VFMADDPS ymm0, ymm0, ymm11, ymm6
	VMOVAPS ymm12, [dword rdx + 128]
	VFMADDPS ymm4, ymm4, ymm12, ymm6
	SUB r10, 4
	CMP r10, rcx
	JB .batch_polevl_finish
	align 16
	.batch_polevl_next:
	VBROADCASTSS ymm6, [r10]
	VFMADDPS xmm5, xmm5, xmm7, xmm6
	VFMADDPS xmm3, xmm3, xmm8, xmm6
	VFMADDPS ymm2, ymm2, ymm9, ymm6
	VFMADDPS ymm1, ymm1, ymm10, ymm6
	VFMADDPS ymm0, ymm0, ymm11, ymm6
	VFMADDPS ymm4, ymm4, ymm12, ymm6
	SUB r10, 4
	CMP r10, rcx
	JAE .batch_polevl_next
	.batch_polevl_finish:
	VMOVUPS [r8], xmm5
	VMOVUPS [byte r8 + 16], xmm3
	VMOVUPS [byte r8 + 32], xmm2
	VEXTRACTF128 [byte r8 + 48], ymm2, 1
	VMOVUPS [byte r8 + 64], xmm1
	VEXTRACTF128 [byte r8 + 80], ymm1, 1
	VMOVUPS [byte r8 + 96], xmm0
	VEXTRACTF128 [byte r8 + 112], ymm0, 1
	VMOVUPS [dword r8 + 128], xmm4
	VEXTRACTF128 [dword r8 + 144], ymm4, 1
	ADD rdx, 160
	ADD r8, 160
	SUB rax, 40
	JAE .process_batch_full
	.process_restore:
	ADD rax, 40
	JZ .return_ok
	.process_single:
	VMOVSS xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 4 - 4]
	VMOVSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	VFMADDSS xmm5, xmm5, xmm4, [r10]
	SUB r10, 4
	CMP r10, rcx
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	VMOVSS [r8], xmm5
	ADD rdx, 4
	ADD r8, 4
	SUB rax, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	ADD rsp, 120
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepMath_EvaluatePolynomial_V32fV32f_V32f_SandyBridge
_yepMath_EvaluatePolynomial_V32fV32f_V32f_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm14
	VMOVAPS [dword rsp + 144], xmm15
	MOV rax, [dword rsp + 208]
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST rax, rax
	JZ .return_ok
	TEST rdx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 4 - 4]
	VMOVSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	VMULSS xmm5, xmm5, xmm4
	VADDSS xmm5, xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	VMOVSS [r8], xmm5
	ADD rdx, 4
	ADD r8, 4
	SUB rax, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rax, 64
	JB .process_restore
	align 32
	.process_batch_full:
	LEA r10, [byte rcx + r9 * 4 - 4]
	VBROADCASTSS ymm4, [r10]
	VMOVAPS ymm5, ymm4
	VMOVAPS ymm3, ymm4
	VMOVAPS ymm2, ymm4
	VMOVAPS ymm1, ymm4
	VMOVAPS ymm0, ymm4
	VMOVAPS ymm6, ymm4
	VMOVAPS ymm7, ymm4
	SUB r10, 4
	CMP r10, rcx
	JB .batch_polevl_finish
	VBROADCASTSS ymm8, [r10]
	VMOVAPS ymm9, [rdx]
	VMULPS ymm4, ymm4, ymm9
	VADDPS ymm4, ymm4, ymm8
	VMOVAPS ymm10, [byte rdx + 32]
	VMULPS ymm5, ymm5, ymm10
	VADDPS ymm5, ymm5, ymm8
	VMOVAPS ymm11, [byte rdx + 64]
	VMULPS ymm3, ymm3, ymm11
	VADDPS ymm3, ymm3, ymm8
	VMULPS ymm2, ymm2, [byte rdx + 96]
	VADDPS ymm2, ymm2, ymm8
	VMOVAPS ymm12, [dword rdx + 128]
	VMULPS ymm1, ymm1, ymm12
	VADDPS ymm1, ymm1, ymm8
	VMOVAPS ymm13, [dword rdx + 160]
	VMULPS ymm0, ymm0, ymm13
	VADDPS ymm0, ymm0, ymm8
	VMOVAPS ymm14, [dword rdx + 192]
	VMULPS ymm6, ymm6, ymm14
	VADDPS ymm6, ymm6, ymm8
	VMOVAPS ymm15, [dword rdx + 224]
	VMULPS ymm7, ymm7, ymm15
	VADDPS ymm7, ymm7, ymm8
	SUB r10, 4
	CMP r10, rcx
	JB .batch_polevl_finish
	.batch_polevl_next:
	VBROADCASTSS ymm8, [r10]
	VMULPS ymm4, ymm4, ymm9
	VADDPS ymm4, ymm4, ymm8
	VMULPS ymm5, ymm5, ymm10
	VADDPS ymm5, ymm5, ymm8
	VMULPS ymm3, ymm3, ymm11
	VADDPS ymm3, ymm3, ymm8
	VMULPS ymm2, ymm2, [byte rdx + 96]
	VADDPS ymm2, ymm2, ymm8
	VMULPS ymm1, ymm1, ymm12
	VADDPS ymm1, ymm1, ymm8
	VMULPS ymm0, ymm0, ymm13
	VADDPS ymm0, ymm0, ymm8
	VMULPS ymm6, ymm6, ymm14
	VADDPS ymm6, ymm6, ymm8
	VMULPS ymm7, ymm7, ymm15
	VADDPS ymm7, ymm7, ymm8
	SUB r10, 4
	CMP r10, rcx
	JAE .batch_polevl_next
	.batch_polevl_finish:
	VMOVUPS [r8], ymm4
	VMOVUPS [byte r8 + 32], ymm5
	VMOVUPS [byte r8 + 64], ymm3
	VMOVUPS [byte r8 + 96], ymm2
	VMOVUPS [dword r8 + 128], ymm1
	VMOVUPS [dword r8 + 160], ymm0
	VMOVUPS [dword r8 + 192], ymm6
	VMOVUPS [dword r8 + 224], ymm7
	ADD rdx, 256
	ADD r8, 256
	SUB rax, 64
	JAE .process_batch_full
	.process_restore:
	ADD rax, 64
	JZ .return_ok
	.process_single:
	VMOVSS xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 4 - 4]
	VMOVSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	VMULSS xmm5, xmm5, xmm4
	VADDSS xmm5, xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	VMOVSS [r8], xmm5
	ADD rdx, 4
	ADD r8, 4
	SUB rax, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm14, [dword rsp + 128]
	VMOVAPS xmm15, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepMath_EvaluatePolynomial_V32fV32f_V32f_Haswell
_yepMath_EvaluatePolynomial_V32fV32f_V32f_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm14
	VMOVAPS [dword rsp + 144], xmm15
	MOV rax, [dword rsp + 208]
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 3
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 3
	JNZ .return_misaligned_pointer
	TEST rax, rax
	JZ .return_ok
	TEST rdx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSS xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 4 - 4]
	VMOVSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	VFMADD213SS xmm5, xmm4, [r10]
	SUB r10, 4
	CMP r10, rcx
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	VMOVSS [r8], xmm5
	ADD rdx, 4
	ADD r8, 4
	SUB rax, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rax, 80
	JB .process_restore
	align 32
	.process_batch_full:
	LEA r10, [byte rcx + r9 * 4 - 4]
	VBROADCASTSS ymm4, [r10]
	VMOVAPS ymm5, ymm4
	VMOVAPS ymm3, ymm4
	VMOVAPS ymm2, ymm4
	VMOVAPS ymm1, ymm4
	VMOVAPS ymm0, ymm4
	VMOVAPS ymm6, ymm4
	VMOVAPS ymm7, ymm4
	VMOVAPS ymm8, ymm4
	VMOVAPS ymm9, ymm4
	PREFETCHNTA [dword rdx + 768]
	PREFETCHNTA [dword rdx + 800]
	PREFETCHNTA [dword rdx + 832]
	SUB r10, 4
	CMP r10, rcx
	JB .batch_polevl_finish
	VBROADCASTSS ymm10, [r10]
	VMOVAPS ymm11, [rdx]
	VFMADD132PS ymm4, ymm10, ymm11
	VFMADD132PS ymm5, ymm10, [byte rdx + 32]
	VMOVAPS ymm12, [byte rdx + 64]
	VFMADD132PS ymm3, ymm10, ymm12
	VFMADD132PS ymm2, ymm10, [byte rdx + 96]
	VMOVAPS ymm13, [dword rdx + 128]
	VFMADD132PS ymm1, ymm10, ymm13
	VFMADD132PS ymm0, ymm10, [dword rdx + 160]
	VMOVAPS ymm14, [dword rdx + 192]
	VFMADD132PS ymm6, ymm10, ymm14
	VFMADD132PS ymm7, ymm10, [dword rdx + 224]
	VMOVAPS ymm15, [dword rdx + 256]
	VFMADD132PS ymm8, ymm10, ymm15
	VFMADD132PS ymm9, ymm10, [dword rdx + 288]
	SUB r10, 4
	CMP r10, rcx
	JB .batch_polevl_finish
	align 16
	.batch_polevl_next:
	VBROADCASTSS ymm10, [r10]
	VFMADD132PS ymm4, ymm10, ymm11
	VFMADD132PS ymm5, ymm10, [byte rdx + 32]
	VFMADD132PS ymm3, ymm10, ymm12
	VFMADD132PS ymm2, ymm10, [byte rdx + 96]
	VFMADD132PS ymm1, ymm10, ymm13
	VFMADD132PS ymm0, ymm10, [dword rdx + 160]
	VFMADD132PS ymm6, ymm10, ymm14
	VFMADD132PS ymm7, ymm10, [dword rdx + 224]
	VFMADD132PS ymm8, ymm10, ymm15
	VFMADD132PS ymm9, ymm10, [dword rdx + 288]
	SUB r10, 4
	CMP r10, rcx
	JAE .batch_polevl_next
	.batch_polevl_finish:
	VMOVUPS [r8], ymm4
	VMOVUPS [byte r8 + 32], ymm5
	VMOVUPS [byte r8 + 64], ymm3
	VMOVUPS [byte r8 + 96], ymm2
	VMOVUPS [dword r8 + 128], ymm1
	VMOVUPS [dword r8 + 160], ymm0
	VMOVUPS [dword r8 + 192], ymm6
	VMOVUPS [dword r8 + 224], ymm7
	VMOVUPS [dword r8 + 256], ymm8
	VMOVUPS [dword r8 + 288], ymm9
	ADD rdx, 320
	ADD r8, 320
	SUB rax, 80
	JAE .process_batch_full
	.process_restore:
	ADD rax, 80
	JZ .return_ok
	.process_single:
	VMOVSS xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 4 - 4]
	VMOVSS xmm5, [r10]
	SUB r10, 4
	CMP r10, rcx
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	VFMADD213SS xmm5, xmm4, [r10]
	SUB r10, 4
	CMP r10, rcx
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	VMOVSS [r8], xmm5
	ADD rdx, 4
	ADD r8, 4
	SUB rax, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm14, [dword rsp + 128]
	VMOVAPS xmm15, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$a code align=16
global _V64fV64f_V64f_Unknown
_V64fV64f_V64f_Unknown:
	.ENTRY:
	SUB rsp, 120
	MOVAPS [rsp], xmm6
	MOVAPS [byte rsp + 16], xmm7
	MOVAPS [byte rsp + 32], xmm8
	MOVAPS [byte rsp + 48], xmm9
	MOVAPS [byte rsp + 64], xmm10
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm12
	MOV rax, [dword rsp + 160]
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST rax, rax
	JZ .return_ok
	TEST rdx, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSD xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 8 - 8]
	MOVSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	MULSD xmm5, xmm4
	ADDSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	MOVSD [r8], xmm5
	ADD rdx, 8
	ADD r8, 8
	SUB rax, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB rax, 12
	JB .process_restore
	align 32
	.process_batch_full:
	LEA r10, [byte rcx + r9 * 8 - 8]
	MOVDDUP xmm4, [r10]
	MOVAPD xmm5, xmm4
	MOVAPD xmm3, xmm4
	MOVAPD xmm2, xmm4
	MOVAPD xmm1, xmm4
	MOVAPD xmm0, xmm4
	SUB r10, 8
	CMP r10, rcx
	JB .batch_polevl_finish
	MOVDDUP xmm6, [r10]
	MOVAPD xmm7, [rdx]
	MULPD xmm4, xmm7
	ADDPD xmm4, xmm6
	MOVAPD xmm8, [byte rdx + 16]
	MULPD xmm5, xmm8
	ADDPD xmm5, xmm6
	MOVAPD xmm9, [byte rdx + 32]
	MULPD xmm3, xmm9
	ADDPD xmm3, xmm6
	MOVAPD xmm10, [byte rdx + 48]
	MULPD xmm2, xmm10
	ADDPD xmm2, xmm6
	MOVAPD xmm11, [byte rdx + 64]
	MULPD xmm1, xmm11
	ADDPD xmm1, xmm6
	MOVAPD xmm12, [byte rdx + 80]
	MULPD xmm0, xmm12
	ADDPD xmm0, xmm6
	SUB r10, 8
	CMP r10, rcx
	JB .batch_polevl_finish
	.batch_polevl_next:
	MOVDDUP xmm6, [r10]
	MULPD xmm4, xmm7
	ADDPD xmm4, xmm6
	MULPD xmm5, xmm8
	ADDPD xmm5, xmm6
	MULPD xmm3, xmm9
	ADDPD xmm3, xmm6
	MULPD xmm2, xmm10
	ADDPD xmm2, xmm6
	MULPD xmm1, xmm11
	ADDPD xmm1, xmm6
	MULPD xmm0, xmm12
	ADDPD xmm0, xmm6
	SUB r10, 8
	CMP r10, rcx
	JAE .batch_polevl_next
	.batch_polevl_finish:
	MOVUPD [r8], xmm4
	MOVUPD [byte r8 + 16], xmm5
	MOVUPD [byte r8 + 32], xmm3
	MOVUPD [byte r8 + 48], xmm2
	MOVUPD [byte r8 + 64], xmm1
	MOVUPD [byte r8 + 80], xmm0
	ADD rdx, 96
	ADD r8, 96
	SUB rax, 12
	JAE .process_batch_full
	.process_restore:
	ADD rax, 12
	JZ .return_ok
	.process_single:
	MOVSD xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 8 - 8]
	MOVSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	MULSD xmm5, xmm4
	ADDSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	MOVSD [r8], xmm5
	ADD rdx, 8
	ADD r8, 8
	SUB rax, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm6, [rsp]
	MOVAPS xmm7, [byte rsp + 16]
	MOVAPS xmm8, [byte rsp + 32]
	MOVAPS xmm9, [byte rsp + 48]
	MOVAPS xmm10, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm12, [byte rsp + 96]
	ADD rsp, 120
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$e code align=16
global _yepMath_EvaluatePolynomial_V64fV64f_V64f_Nehalem
_yepMath_EvaluatePolynomial_V64fV64f_V64f_Nehalem:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm6
	MOVAPS [byte rsp + 16], xmm7
	MOVAPS [byte rsp + 32], xmm8
	MOVAPS [byte rsp + 48], xmm9
	MOVAPS [byte rsp + 64], xmm10
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm12
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm14
	MOVAPS [dword rsp + 144], xmm15
	MOV rax, [dword rsp + 208]
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST rax, rax
	JZ .return_ok
	TEST rdx, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSD xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 8 - 8]
	MOVSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	MULSD xmm5, xmm4
	ADDSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	MOVSD [r8], xmm5
	ADD rdx, 8
	ADD r8, 8
	SUB rax, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB rax, 20
	JB .process_restore
	align 32
	.process_batch_full:
	LEA r10, [byte rcx + r9 * 8 - 8]
	MOVDDUP xmm4, [r10]
	MOVAPD xmm5, xmm4
	MOVAPD xmm3, xmm4
	MOVAPD xmm2, xmm4
	MOVAPD xmm1, xmm4
	MOVAPD xmm0, xmm4
	MOVAPD xmm6, xmm4
	MOVAPD xmm7, xmm4
	MOVAPD xmm8, xmm4
	MOVAPD xmm9, xmm4
	SUB r10, 8
	CMP r10, rcx
	JB .batch_polevl_finish
	MOVDDUP xmm10, [r10]
	MOVAPD xmm11, [rdx]
	MULPD xmm4, xmm11
	ADDPD xmm4, xmm10
	MULPD xmm5, [byte rdx + 16]
	ADDPD xmm5, xmm10
	MOVAPD xmm12, [byte rdx + 32]
	MULPD xmm3, xmm12
	ADDPD xmm3, xmm10
	MULPD xmm2, [byte rdx + 48]
	ADDPD xmm2, xmm10
	MOVAPD xmm13, [byte rdx + 64]
	MULPD xmm1, xmm13
	ADDPD xmm1, xmm10
	MULPD xmm0, [byte rdx + 80]
	ADDPD xmm0, xmm10
	MOVAPD xmm14, [byte rdx + 96]
	MULPD xmm6, xmm14
	ADDPD xmm6, xmm10
	MULPD xmm7, [byte rdx + 112]
	ADDPD xmm7, xmm10
	MOVAPD xmm15, [dword rdx + 128]
	MULPD xmm8, xmm15
	ADDPD xmm8, xmm10
	MULPD xmm9, [dword rdx + 144]
	ADDPD xmm9, xmm10
	SUB r10, 8
	CMP r10, rcx
	JB .batch_polevl_finish
	.batch_polevl_next:
	MOVDDUP xmm10, [r10]
	MULPD xmm4, xmm11
	ADDPD xmm4, xmm10
	MULPD xmm5, [byte rdx + 16]
	ADDPD xmm5, xmm10
	MULPD xmm3, xmm12
	ADDPD xmm3, xmm10
	MULPD xmm2, [byte rdx + 48]
	ADDPD xmm2, xmm10
	MULPD xmm1, xmm13
	ADDPD xmm1, xmm10
	MULPD xmm0, [byte rdx + 80]
	ADDPD xmm0, xmm10
	MULPD xmm6, xmm14
	ADDPD xmm6, xmm10
	MULPD xmm7, [byte rdx + 112]
	ADDPD xmm7, xmm10
	MULPD xmm8, xmm15
	ADDPD xmm8, xmm10
	MULPD xmm9, [dword rdx + 144]
	ADDPD xmm9, xmm10
	SUB r10, 8
	CMP r10, rcx
	JAE .batch_polevl_next
	.batch_polevl_finish:
	MOVUPD [r8], xmm4
	MOVUPD [byte r8 + 16], xmm5
	MOVUPD [byte r8 + 32], xmm3
	MOVUPD [byte r8 + 48], xmm2
	MOVUPD [byte r8 + 64], xmm1
	MOVUPD [byte r8 + 80], xmm0
	MOVUPD [byte r8 + 96], xmm6
	MOVUPD [byte r8 + 112], xmm7
	MOVUPD [dword r8 + 128], xmm8
	MOVUPD [dword r8 + 144], xmm9
	ADD rdx, 160
	ADD r8, 160
	SUB rax, 20
	JAE .process_batch_full
	.process_restore:
	ADD rax, 20
	JZ .return_ok
	.process_single:
	MOVSD xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 8 - 8]
	MOVSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	MULSD xmm5, xmm4
	ADDSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	MOVSD [r8], xmm5
	ADD rdx, 8
	ADD r8, 8
	SUB rax, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm6, [rsp]
	MOVAPS xmm7, [byte rsp + 16]
	MOVAPS xmm8, [byte rsp + 32]
	MOVAPS xmm9, [byte rsp + 48]
	MOVAPS xmm10, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm12, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm14, [dword rsp + 128]
	MOVAPS xmm15, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$i code align=16
global _yepMath_EvaluatePolynomial_V64fV64f_V64f_Bonnell
_yepMath_EvaluatePolynomial_V64fV64f_V64f_Bonnell:
	.ENTRY:
	SUB rsp, 168
	MOVAPS [rsp], xmm6
	MOVAPS [byte rsp + 16], xmm7
	MOVAPS [byte rsp + 32], xmm8
	MOVAPS [byte rsp + 48], xmm9
	MOVAPS [byte rsp + 64], xmm10
	MOVAPS [byte rsp + 80], xmm11
	MOVAPS [byte rsp + 96], xmm12
	MOVAPS [byte rsp + 112], xmm13
	MOVAPS [dword rsp + 128], xmm14
	MOVAPS [dword rsp + 144], xmm15
	MOV rax, [dword rsp + 208]
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST rax, rax
	JZ .return_ok
	TEST rdx, 15
	JZ .source_16b_aligned
	.source_16b_misaligned:
	MOVSD xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 8 - 8]
	MOVSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	MULSD xmm5, xmm4
	ADDSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	MOVSD [r8], xmm5
	ADD rdx, 8
	ADD r8, 8
	SUB rax, 1
	JZ .return_ok
	TEST rdx, 15
	JNZ .source_16b_misaligned
	.source_16b_aligned:
	SUB rax, 14
	JB .process_restore
	align 32
	.process_batch_full:
	LEA r10, [byte rcx + r9 * 8 - 8]
	MOVSD xmm4, [r10]
	MOVSD xmm5, [rdx]
	MOVAPS xmm3, xmm4
	MOVAPS xmm2, xmm4
	MOVAPS xmm1, xmm4
	MOVAPS xmm0, xmm4
	MOVAPS xmm6, xmm4
	MOVAPS xmm7, xmm4
	MOVAPS xmm8, xmm4
	MOVAPS xmm9, xmm4
	MOVAPS xmm10, xmm4
	MOVAPS xmm11, xmm4
	MOVAPS xmm12, xmm4
	MOVAPS xmm13, xmm4
	MOVAPS xmm14, xmm4
	SUB r10, 8
	CMP r10, rcx
	JB .batch_polevl_finish
	.batch_polevl_next:
	MOVSD xmm15, [r10]
	MULSD xmm4, xmm5
	MULSD xmm3, [byte rdx + 8]
	MULSD xmm2, [byte rdx + 16]
	ADDSD xmm4, xmm15
	MULSD xmm1, [byte rdx + 24]
	ADDSD xmm3, xmm15
	MULSD xmm0, [byte rdx + 32]
	ADDSD xmm2, xmm15
	MULSD xmm6, [byte rdx + 40]
	ADDSD xmm1, xmm15
	MULSD xmm7, [byte rdx + 48]
	ADDSD xmm0, xmm15
	MULSD xmm8, [byte rdx + 56]
	ADDSD xmm6, xmm15
	MULSD xmm9, [byte rdx + 64]
	ADDSD xmm7, xmm15
	MULSD xmm10, [byte rdx + 72]
	ADDSD xmm8, xmm15
	MULSD xmm11, [byte rdx + 80]
	ADDSD xmm9, xmm15
	MULSD xmm12, [byte rdx + 88]
	ADDSD xmm10, xmm15
	MULSD xmm13, [byte rdx + 96]
	ADDSD xmm11, xmm15
	MULSD xmm14, [byte rdx + 104]
	ADDSD xmm12, xmm15
	SUB r10, 8
	ADDSD xmm13, xmm15
	CMP r10, rcx
	ADDSD xmm14, xmm15
	JAE .batch_polevl_next
	.batch_polevl_finish:
	MOVSD [r8], xmm4
	MOVSD [byte r8 + 8], xmm3
	MOVSD [byte r8 + 16], xmm2
	MOVSD [byte r8 + 24], xmm1
	MOVSD [byte r8 + 32], xmm0
	MOVSD [byte r8 + 40], xmm6
	MOVSD [byte r8 + 48], xmm7
	MOVSD [byte r8 + 56], xmm8
	MOVSD [byte r8 + 64], xmm9
	MOVSD [byte r8 + 72], xmm10
	MOVSD [byte r8 + 80], xmm11
	MOVSD [byte r8 + 88], xmm12
	MOVSD [byte r8 + 96], xmm13
	MOVSD [byte r8 + 104], xmm14
	ADD rdx, 112
	ADD r8, 112
	SUB rax, 14
	JAE .process_batch_full
	.process_restore:
	ADD rax, 14
	JZ .return_ok
	.process_single:
	MOVSD xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 8 - 8]
	MOVSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	MULSD xmm5, xmm4
	ADDSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	MOVSD [r8], xmm5
	ADD rdx, 8
	ADD r8, 8
	SUB rax, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	MOVAPS xmm6, [rsp]
	MOVAPS xmm7, [byte rsp + 16]
	MOVAPS xmm8, [byte rsp + 32]
	MOVAPS xmm9, [byte rsp + 48]
	MOVAPS xmm10, [byte rsp + 64]
	MOVAPS xmm11, [byte rsp + 80]
	MOVAPS xmm12, [byte rsp + 96]
	MOVAPS xmm13, [byte rsp + 112]
	MOVAPS xmm14, [dword rsp + 128]
	MOVAPS xmm15, [dword rsp + 144]
	ADD rsp, 168
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$n code align=16
global _yepMath_EvaluatePolynomial_V64fV64f_V64f_Bulldozer
_yepMath_EvaluatePolynomial_V64fV64f_V64f_Bulldozer:
	.ENTRY:
	SUB rsp, 120
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	MOV rax, [dword rsp + 160]
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST rax, rax
	JZ .return_ok
	TEST rdx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 8 - 8]
	VMOVSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	VFMADDSD xmm5, xmm5, xmm4, [r10]
	SUB r10, 8
	CMP r10, rcx
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	VMOVSD [r8], xmm5
	ADD rdx, 8
	ADD r8, 8
	SUB rax, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rax, 20
	JB .process_restore
	align 32
	.process_batch_full:
	LEA r10, [byte rcx + r9 * 8 - 8]
	VBROADCASTSD ymm4, [r10]
	VMOVAPD xmm5, xmm4
	VMOVAPD xmm3, xmm4
	VMOVAPD ymm2, ymm4
	VMOVAPD ymm1, ymm4
	VMOVAPD ymm0, ymm4
	SUB r10, 8
	CMP r10, rcx
	JB .batch_polevl_finish
	VBROADCASTSD ymm6, [r10]
	VMOVAPD xmm7, [rdx]
	VFMADDPD xmm5, xmm5, xmm7, xmm6
	VMOVAPD xmm8, [byte rdx + 16]
	VFMADDPD xmm3, xmm3, xmm8, xmm6
	VMOVAPD ymm9, [byte rdx + 32]
	VFMADDPD ymm2, ymm2, ymm9, ymm6
	VMOVAPD ymm10, [byte rdx + 64]
	VFMADDPD ymm1, ymm1, ymm10, ymm6
	VMOVAPD ymm11, [byte rdx + 96]
	VFMADDPD ymm0, ymm0, ymm11, ymm6
	VMOVAPD ymm12, [dword rdx + 128]
	VFMADDPD ymm4, ymm4, ymm12, ymm6
	SUB r10, 8
	CMP r10, rcx
	JB .batch_polevl_finish
	align 16
	.batch_polevl_next:
	VBROADCASTSD ymm6, [r10]
	VFMADDPD xmm5, xmm5, xmm7, xmm6
	VFMADDPD xmm3, xmm3, xmm8, xmm6
	VFMADDPD ymm2, ymm2, ymm9, ymm6
	VFMADDPD ymm1, ymm1, ymm10, ymm6
	VFMADDPD ymm0, ymm0, ymm11, ymm6
	VFMADDPD ymm4, ymm4, ymm12, ymm6
	SUB r10, 8
	CMP r10, rcx
	JAE .batch_polevl_next
	.batch_polevl_finish:
	VMOVUPD [r8], xmm5
	VMOVUPD [byte r8 + 16], xmm3
	VMOVUPD [byte r8 + 32], xmm2
	VEXTRACTF128 [byte r8 + 48], ymm2, 1
	VMOVUPD [byte r8 + 64], xmm1
	VEXTRACTF128 [byte r8 + 80], ymm1, 1
	VMOVUPD [byte r8 + 96], xmm0
	VEXTRACTF128 [byte r8 + 112], ymm0, 1
	VMOVUPD [dword r8 + 128], xmm4
	VEXTRACTF128 [dword r8 + 144], ymm4, 1
	ADD rdx, 160
	ADD r8, 160
	SUB rax, 20
	JAE .process_batch_full
	.process_restore:
	ADD rax, 20
	JZ .return_ok
	.process_single:
	VMOVSD xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 8 - 8]
	VMOVSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	VFMADDSD xmm5, xmm5, xmm4, [r10]
	SUB r10, 8
	CMP r10, rcx
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	VMOVSD [r8], xmm5
	ADD rdx, 8
	ADD r8, 8
	SUB rax, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	ADD rsp, 120
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$f code align=16
global _yepMath_EvaluatePolynomial_V64fV64f_V64f_SandyBridge
_yepMath_EvaluatePolynomial_V64fV64f_V64f_SandyBridge:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm14
	VMOVAPS [dword rsp + 144], xmm15
	MOV rax, [dword rsp + 208]
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST rax, rax
	JZ .return_ok
	TEST rdx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 8 - 8]
	VMOVSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	VMULSD xmm5, xmm5, xmm4
	VADDSD xmm5, xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	VMOVSD [r8], xmm5
	ADD rdx, 8
	ADD r8, 8
	SUB rax, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rax, 32
	JB .process_restore
	align 32
	.process_batch_full:
	LEA r10, [byte rcx + r9 * 8 - 8]
	VBROADCASTSD ymm4, [r10]
	VMOVAPD ymm5, ymm4
	VMOVAPD ymm3, ymm4
	VMOVAPD ymm2, ymm4
	VMOVAPD ymm1, ymm4
	VMOVAPD ymm0, ymm4
	VMOVAPD ymm6, ymm4
	VMOVAPD ymm7, ymm4
	SUB r10, 8
	CMP r10, rcx
	JB .batch_polevl_finish
	VBROADCASTSD ymm8, [r10]
	VMOVAPD ymm9, [rdx]
	VMULPD ymm4, ymm4, ymm9
	VADDPD ymm4, ymm4, ymm8
	VMOVAPD ymm10, [byte rdx + 32]
	VMULPD ymm5, ymm5, ymm10
	VADDPD ymm5, ymm5, ymm8
	VMOVAPD ymm11, [byte rdx + 64]
	VMULPD ymm3, ymm3, ymm11
	VADDPD ymm3, ymm3, ymm8
	VMULPD ymm2, ymm2, [byte rdx + 96]
	VADDPD ymm2, ymm2, ymm8
	VMOVAPD ymm12, [dword rdx + 128]
	VMULPD ymm1, ymm1, ymm12
	VADDPD ymm1, ymm1, ymm8
	VMOVAPD ymm13, [dword rdx + 160]
	VMULPD ymm0, ymm0, ymm13
	VADDPD ymm0, ymm0, ymm8
	VMOVAPD ymm14, [dword rdx + 192]
	VMULPD ymm6, ymm6, ymm14
	VADDPD ymm6, ymm6, ymm8
	VMOVAPD ymm15, [dword rdx + 224]
	VMULPD ymm7, ymm7, ymm15
	VADDPD ymm7, ymm7, ymm8
	SUB r10, 8
	CMP r10, rcx
	JB .batch_polevl_finish
	.batch_polevl_next:
	VBROADCASTSD ymm8, [r10]
	VMULPD ymm4, ymm4, ymm9
	VADDPD ymm4, ymm4, ymm8
	VMULPD ymm5, ymm5, ymm10
	VADDPD ymm5, ymm5, ymm8
	VMULPD ymm3, ymm3, ymm11
	VADDPD ymm3, ymm3, ymm8
	VMULPD ymm2, ymm2, [byte rdx + 96]
	VADDPD ymm2, ymm2, ymm8
	VMULPD ymm1, ymm1, ymm12
	VADDPD ymm1, ymm1, ymm8
	VMULPD ymm0, ymm0, ymm13
	VADDPD ymm0, ymm0, ymm8
	VMULPD ymm6, ymm6, ymm14
	VADDPD ymm6, ymm6, ymm8
	VMULPD ymm7, ymm7, ymm15
	VADDPD ymm7, ymm7, ymm8
	SUB r10, 8
	CMP r10, rcx
	JAE .batch_polevl_next
	.batch_polevl_finish:
	VMOVUPD [r8], ymm4
	VMOVUPD [byte r8 + 32], ymm5
	VMOVUPD [byte r8 + 64], ymm3
	VMOVUPD [byte r8 + 96], ymm2
	VMOVUPD [dword r8 + 128], ymm1
	VMOVUPD [dword r8 + 160], ymm0
	VMOVUPD [dword r8 + 192], ymm6
	VMOVUPD [dword r8 + 224], ymm7
	ADD rdx, 256
	ADD r8, 256
	SUB rax, 32
	JAE .process_batch_full
	.process_restore:
	ADD rax, 32
	JZ .return_ok
	.process_single:
	VMOVSD xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 8 - 8]
	VMOVSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	VMULSD xmm5, xmm5, xmm4
	VADDSD xmm5, xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	VMOVSD [r8], xmm5
	ADD rdx, 8
	ADD r8, 8
	SUB rax, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm14, [dword rsp + 128]
	VMOVAPS xmm15, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return

section .text$h code align=16
global _yepMath_EvaluatePolynomial_V64fV64f_V64f_Haswell
_yepMath_EvaluatePolynomial_V64fV64f_V64f_Haswell:
	.ENTRY:
	SUB rsp, 168
	VMOVAPS [rsp], xmm6
	VMOVAPS [byte rsp + 16], xmm7
	VMOVAPS [byte rsp + 32], xmm8
	VMOVAPS [byte rsp + 48], xmm9
	VMOVAPS [byte rsp + 64], xmm10
	VMOVAPS [byte rsp + 80], xmm11
	VMOVAPS [byte rsp + 96], xmm12
	VMOVAPS [byte rsp + 112], xmm13
	VMOVAPS [dword rsp + 128], xmm14
	VMOVAPS [dword rsp + 144], xmm15
	MOV rax, [dword rsp + 208]
	TEST rdx, rdx
	JZ .return_null_pointer
	TEST rdx, 7
	JNZ .return_misaligned_pointer
	TEST r8, r8
	JZ .return_null_pointer
	TEST r8, 7
	JNZ .return_misaligned_pointer
	TEST rax, rax
	JZ .return_ok
	TEST rdx, 31
	JZ .source_32b_aligned
	.source_32b_misaligned:
	VMOVSD xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 8 - 8]
	VMOVSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JB .prologue_scalar_polevl_finish
	.prologue_scalar_polevl_next:
	VFMADD213SD xmm5, xmm4, [r10]
	SUB r10, 8
	CMP r10, rcx
	JAE .prologue_scalar_polevl_next
	.prologue_scalar_polevl_finish:
	VMOVSD [r8], xmm5
	ADD rdx, 8
	ADD r8, 8
	SUB rax, 1
	JZ .return_ok
	TEST rdx, 31
	JNZ .source_32b_misaligned
	.source_32b_aligned:
	SUB rax, 40
	JB .process_restore
	align 32
	.process_batch_full:
	LEA r10, [byte rcx + r9 * 8 - 8]
	VBROADCASTSD ymm4, [r10]
	VMOVAPD ymm5, ymm4
	VMOVAPD ymm3, ymm4
	VMOVAPD ymm2, ymm4
	VMOVAPD ymm1, ymm4
	VMOVAPD ymm0, ymm4
	VMOVAPD ymm6, ymm4
	VMOVAPD ymm7, ymm4
	VMOVAPD ymm8, ymm4
	VMOVAPD ymm9, ymm4
	PREFETCHNTA [dword rdx + 768]
	PREFETCHNTA [dword rdx + 800]
	PREFETCHNTA [dword rdx + 832]
	SUB r10, 8
	CMP r10, rcx
	JB .batch_polevl_finish
	VBROADCASTSD ymm10, [r10]
	VMOVAPD ymm11, [rdx]
	VFMADD132PD ymm4, ymm10, ymm11
	VFMADD132PD ymm5, ymm10, [byte rdx + 32]
	VMOVAPD ymm12, [byte rdx + 64]
	VFMADD132PD ymm3, ymm10, ymm12
	VFMADD132PD ymm2, ymm10, [byte rdx + 96]
	VMOVAPD ymm13, [dword rdx + 128]
	VFMADD132PD ymm1, ymm10, ymm13
	VFMADD132PD ymm0, ymm10, [dword rdx + 160]
	VMOVAPD ymm14, [dword rdx + 192]
	VFMADD132PD ymm6, ymm10, ymm14
	VFMADD132PD ymm7, ymm10, [dword rdx + 224]
	VMOVAPD ymm15, [dword rdx + 256]
	VFMADD132PD ymm8, ymm10, ymm15
	VFMADD132PD ymm9, ymm10, [dword rdx + 288]
	SUB r10, 8
	CMP r10, rcx
	JB .batch_polevl_finish
	align 16
	.batch_polevl_next:
	VBROADCASTSD ymm10, [r10]
	VFMADD132PD ymm4, ymm10, ymm11
	VFMADD132PD ymm5, ymm10, [byte rdx + 32]
	VFMADD132PD ymm3, ymm10, ymm12
	VFMADD132PD ymm2, ymm10, [byte rdx + 96]
	VFMADD132PD ymm1, ymm10, ymm13
	VFMADD132PD ymm0, ymm10, [dword rdx + 160]
	VFMADD132PD ymm6, ymm10, ymm14
	VFMADD132PD ymm7, ymm10, [dword rdx + 224]
	VFMADD132PD ymm8, ymm10, ymm15
	VFMADD132PD ymm9, ymm10, [dword rdx + 288]
	SUB r10, 8
	CMP r10, rcx
	JAE .batch_polevl_next
	.batch_polevl_finish:
	VMOVUPD [r8], ymm4
	VMOVUPD [byte r8 + 32], ymm5
	VMOVUPD [byte r8 + 64], ymm3
	VMOVUPD [byte r8 + 96], ymm2
	VMOVUPD [dword r8 + 128], ymm1
	VMOVUPD [dword r8 + 160], ymm0
	VMOVUPD [dword r8 + 192], ymm6
	VMOVUPD [dword r8 + 224], ymm7
	VMOVUPD [dword r8 + 256], ymm8
	VMOVUPD [dword r8 + 288], ymm9
	ADD rdx, 320
	ADD r8, 320
	SUB rax, 40
	JAE .process_batch_full
	.process_restore:
	ADD rax, 40
	JZ .return_ok
	.process_single:
	VMOVSD xmm4, [rdx]
	LEA r10, [byte rcx + r9 * 8 - 8]
	VMOVSD xmm5, [r10]
	SUB r10, 8
	CMP r10, rcx
	JB .epilogue_scalar_polevl_finish
	.epilogue_scalar_polevl_next:
	VFMADD213SD xmm5, xmm4, [r10]
	SUB r10, 8
	CMP r10, rcx
	JAE .epilogue_scalar_polevl_next
	.epilogue_scalar_polevl_finish:
	VMOVSD [r8], xmm5
	ADD rdx, 8
	ADD r8, 8
	SUB rax, 1
	JNZ .process_single
	.return_ok:
	XOR eax, eax
	.return:
	VMOVAPS xmm6, [rsp]
	VMOVAPS xmm7, [byte rsp + 16]
	VMOVAPS xmm8, [byte rsp + 32]
	VMOVAPS xmm9, [byte rsp + 48]
	VMOVAPS xmm10, [byte rsp + 64]
	VMOVAPS xmm11, [byte rsp + 80]
	VMOVAPS xmm12, [byte rsp + 96]
	VMOVAPS xmm13, [byte rsp + 112]
	VMOVAPS xmm14, [dword rsp + 128]
	VMOVAPS xmm15, [dword rsp + 144]
	ADD rsp, 168
	VZEROUPPER
	RET
	.return_null_pointer:
	MOV eax, 1
	JMP .return
	.return_misaligned_pointer:
	MOV eax, 2
	JMP .return
